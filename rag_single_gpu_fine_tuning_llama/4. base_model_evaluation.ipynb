{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f58308a-0532-46ff-972d-550e53b1fbeb",
   "metadata": {},
   "source": [
    "현재 노트북을 제외한 모든 주피터 노트북에서 상단의 `Kernel > ShudDown Kernel`을 하신 후에 실습을 다시 시작하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344600f5-96fb-4710-a06a-08bad19c484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-16 16:34:56 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ecc567-0a50-4bee-8b05-362d5a063c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-16 16:35:19 [config.py:585] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 04-16 16:35:19 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-16 16:35:21 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='NCSOFT/Llama-VARCO-8B-Instruct', speculative_config=None, tokenizer='NCSOFT/Llama-VARCO-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=NCSOFT/Llama-VARCO-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-16 16:35:21 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c478b865f60>\n",
      "INFO 04-16 16:35:22 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-16 16:35:22 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-16 16:35:22 [gpu_model_runner.py:1174] Starting to load model NCSOFT/Llama-VARCO-8B-Instruct...\n",
      "WARNING 04-16 16:35:22 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-16 16:35:22 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcada636a1794f8b93a30cfbd7e98c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-16 16:35:25 [loader.py:447] Loading weights took 2.57 seconds\n",
      "INFO 04-16 16:35:25 [gpu_model_runner.py:1186] Model loading took 14.9596 GB and 3.344050 seconds\n",
      "INFO 04-16 16:35:32 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/69d5a006b2/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-16 16:35:32 [backends.py:425] Dynamo bytecode transform time: 6.72 s\n",
      "INFO 04-16 16:35:35 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 04-16 16:36:00 [backends.py:144] Compiling a graph for general shape takes 26.92 s\n",
      "INFO 04-16 16:36:12 [monitor.py:33] torch.compile takes 33.63 s in total\n",
      "INFO 04-16 16:36:13 [kv_cache_utils.py:566] GPU KV cache size: 418,800 tokens\n",
      "INFO 04-16 16:36:13 [kv_cache_utils.py:569] Maximum concurrency for 8,192 tokens per request: 51.12x\n",
      "INFO 04-16 16:36:37 [gpu_model_runner.py:1534] Graph capturing finished in 24 secs, took 0.52 GiB\n",
      "INFO 04-16 16:36:38 [core.py:151] init engine (profile, create kv cache, warmup model) took 72.14 seconds\n"
     ]
    }
   ],
   "source": [
    "vllm_model = LLM(\n",
    "    model=\"NCSOFT/Llama-VARCO-8B-Instruct\",\n",
    "    dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07c4549-68ef-4682-87c4-fb3560d3897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('test_dataset')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"iamjoon/llama-3-8b-rag-ko-checkpoint-285\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee67b2d9-0279-4bdb-9ac4-4ab530af3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = [] # LLM의 입력\n",
    "label_lst = [] # LLM의 답변\n",
    "questions = [] # 사용자의 질문\n",
    "contexts = [] # 검색 결과\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    question = text.split('<|start_header_id|>user<|end_header_id|>')[1].split('<|eot_id|>')[0].strip()\n",
    "    context = text.split('검색 결과:\\n-----')[1].split('<|eot_id|>')[0].strip()\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)\n",
    "    questions.append(question)\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d377dfac-8422-4a57-8324-6b5fbedf83ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼성전자가 LCD 공장을 보유한 회사에 출자하는 금액은?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7937c255-255a-4535-ac75-f6156d283803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n삼성전자가 LCD 공장을 보유한 일본 샤프에 출자하는 금액은 104억엔(약 1200억원)입니다. 삼성전자는 샤프에 출자해 지분 3%를 확보하며, 이는 60인치 이상 대형 LCD TV 패널의 안정적 확보를 위한 목적입니다 [[ref1]].'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lst[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235da821-e318-48b4-9e41-ac63a7e2ff6c",
   "metadata": {},
   "source": [
    "테스트 데이터가 너무 많은 관계로 50개만 평가해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae49da6-ae18-4470-b51d-731678a23fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4b92e5-f858-4437-9da5-cfaeb0267b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 50/50 [00:28<00:00,  1.74it/s, est. speed input: 6592.10 toks/s, output: 424.48 toks/s]\n"
     ]
    }
   ],
   "source": [
    "preds = vllm_model.generate(prompt_lst[:50], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7f27cf-38da-4fee-94cc-6a5be9503dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [pred.outputs[0].text for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3018f9-d897-403b-81ea-0f633918e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 초기화\n",
    "client = OpenAI(api_key=\"여러분의 키 값\")  # 본인의 API 키로 교체하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48d63a-c0f2-41ec-9ad0-8ab11a4d1317",
   "metadata": {},
   "source": [
    "## LLM 기반의 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3157445f-02bc-4dba-b4c2-d0e1ba3a6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_with_llm(questions, contexts, predictions, labels):\n",
    "    \"\"\"\n",
    "    LLM 평가 메트릭으로 RAG 시스템 평가\n",
    "    \n",
    "    Args:\n",
    "        questions: 질문 리스트\n",
    "        contexts: 검색 결과 리스트\n",
    "        predictions: 예측 리스트\n",
    "        labels: 레이블 리스트\n",
    "    \n",
    "    Returns:\n",
    "        평가 결과가 포함된 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    results = []\n",
    "    \n",
    "    # 평가 프롬프트\n",
    "    prompt_template = \"\"\"\n",
    "당신은 RAG(Retrieval-Augmented Generation) 시스템 평가 전문가입니다. 아래 정보를 바탕으로 생성된 답변의 품질을 철저히 평가해주세요.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "검색된 컨텍스트:\n",
    "{context}\n",
    "\n",
    "생성된 답변:\n",
    "{prediction}\n",
    "\n",
    "참조 답변(정답):\n",
    "{label}\n",
    "\n",
    "다음 4가지 평가 기준으로 1-5점 척도로 점수를 매겨주세요:\n",
    "\n",
    "1. 응답 정확성 (Answer Correctness) [1-5]:\n",
    "   * 생성된 답변이 참조 답변과 비교하여 정확하고 완전한 정보를 제공하는지 평가\n",
    "   * 1점: 완전히 잘못된 정보\n",
    "   * 2점: 부분적으로 관련된 정보를 담고 있으나 대부분 부정확함\n",
    "   * 3점: 정확한 정보와 부정확한 정보가 혼재되어 있음\n",
    "   * 4점: 대부분 정확하지만 일부 정보가 누락되거나 미미한 오류가 있음\n",
    "   * 5점: 참조 답변과 비교했을 때 완전히 정확하고 포괄적인 정보를 제공함\n",
    "\n",
    "2. 컨텍스트 관련성 (Context Relevance) [1-5]:\n",
    "   * 검색된 컨텍스트가 질문에 대답하기 위해 관련성이 높은지 평가\n",
    "   * 1점: 컨텍스트가 질문과 전혀 관련이 없음\n",
    "   * 2점: 컨텍스트가 질문과 간접적으로만 관련됨\n",
    "   * 3점: 컨텍스트 중 일부만 질문과 직접적으로 관련됨\n",
    "   * 4점: 대부분의 컨텍스트가 질문과 직접적으로 관련됨\n",
    "   * 5점: 모든 컨텍스트가 질문에 완벽하게 관련되어 있고 불필요한 정보가 없음\n",
    "\n",
    "3. 컨텍스트 충실성 (Context Faithfulness) [1-5]:\n",
    "   * 생성된 답변이 주어진 컨텍스트에만 기반하는지, 아니면 없는 정보를 추가했는지 평가\n",
    "   * 1점: 답변이 컨텍스트에 없는 정보로만 구성됨 (심각한 환각)\n",
    "   * 2점: 답변이 주로 컨텍스트에 없는 정보로 구성됨\n",
    "   * 3점: 답변이 컨텍스트 정보와 없는 정보가 혼합되어 있음\n",
    "   * 4점: 답변이 주로 컨텍스트에 기반하지만 약간의 추가 정보가 있음\n",
    "   * 5점: 답변이 전적으로 컨텍스트에 있는 정보만을 사용함\n",
    "\n",
    "4. 컨텍스트 충분성 (Context Recall) [1-5]:\n",
    "   * 검색된 컨텍스트가 질문에 완전히 답변하기에 충분한 정보를 포함하는지 평가\n",
    "   * 1점: 컨텍스트가 답변에 필요한 정보를 전혀 포함하지 않음\n",
    "   * 2점: 컨텍스트가 필요한 정보의 일부만 포함함\n",
    "   * 3점: 컨텍스트가 필요한 정보의 약 절반을 포함함\n",
    "   * 4점: 컨텍스트가 필요한 정보의 대부분을 포함하지만 일부 누락됨\n",
    "   * 5점: 컨텍스트가 질문에 완전히 답변하기 위한 모든 필요한 정보를 포함함\n",
    "\n",
    "반드시 다음 JSON 형식으로만 응답하세요. 마크다운은 사용하지 않습니다.:\n",
    "{\n",
    "  \"answer_correctness\": 정수로 된 점수(1-5),\n",
    "  \"context_relevance\": 정수로 된 점수(1-5),\n",
    "  \"context_faithfulness\": 정수로 된 점수(1-5),\n",
    "  \"context_recall\": 점수(1-5),\n",
    "  \"analysis\": \"종합적인 분석 의견\"\n",
    "}\n",
    "\n",
    "다른 형식의 응답은 하지 마세요. 오직 마크다운이 아닌 JSON만 반환하세요.\n",
    "\"\"\"\n",
    "    \n",
    "    # 각 항목에 대해 평가 수행\n",
    "    for i in tqdm(range(len(questions)), total=len(questions), desc=\"RAG 평가 진행 중\"):\n",
    "        try:\n",
    "            # 프롬프트 생성 - format 대신 replace 사용\n",
    "            prompt = prompt_template\n",
    "            prompt = prompt.replace(\"{question}\", str(questions[i]) if questions[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{context}\", str(contexts[i]) if contexts[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{prediction}\", str(predictions[i]) if predictions[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{label}\", str(labels[i]) if labels[i] is not None else \"\")\n",
    "\n",
    "            # GPT-4 API 호출\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 RAG 평가 도구입니다. 반드시 유효한 JSON 형식으로만 응답하세요.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            # 결과 파싱\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # 개별 메트릭 점수 추출\n",
    "            answer_correctness = result['answer_correctness']\n",
    "            context_relevance = result['context_relevance']\n",
    "            context_faithfulness = result['context_faithfulness']\n",
    "            context_recall = result['context_recall']\n",
    "            \n",
    "            # 총점 직접 계산 (개별 메트릭의 합)\n",
    "            total_score = answer_correctness + context_relevance + context_faithfulness + context_recall\n",
    "            \n",
    "            # 원본 데이터와 평가 결과 결합\n",
    "            row_result = {\n",
    "                'id': i,\n",
    "                'question': questions[i],\n",
    "                'answer_correctness': answer_correctness,\n",
    "                'context_relevance': context_relevance,\n",
    "                'context_faithfulness': context_faithfulness,\n",
    "                'context_recall': context_recall,\n",
    "                'total_score': total_score,\n",
    "                'analysis': result['analysis']\n",
    "            }\n",
    "            \n",
    "            results.append(row_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"항목 {i} 평가 중 오류 발생: {e}\")\n",
    "            results.append({\n",
    "                'id': i,\n",
    "                'question': questions[i],\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # 결과 데이터프레임 생성\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 요약 통계 계산\n",
    "    if 'total_score' in results_df.columns:\n",
    "        metrics_summary = {\n",
    "            '평균 총점': results_df['total_score'].mean(),\n",
    "            '응답 정확성 평균': results_df['answer_correctness'].mean(),\n",
    "            '컨텍스트 관련성 평균': results_df['context_relevance'].mean(),\n",
    "            '컨텍스트 충실성 평균': results_df['context_faithfulness'].mean(),\n",
    "            '컨텍스트 충분성 평균': results_df['context_recall'].mean()\n",
    "        }\n",
    "        print(\"\\n===== 평가 요약 =====\")\n",
    "        for metric, value in metrics_summary.items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "    \n",
    "    return results_df, metrics_summary if 'total_score' in results_df.columns else results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59633d25-cf7b-4d49-82ed-6063ef31766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG 평가 진행 중: 100%|██████████| 50/50 [03:20<00:00,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 평가 요약 =====\n",
      "평균 총점: 18.22\n",
      "응답 정확성 평균: 4.16\n",
      "컨텍스트 관련성 평균: 4.74\n",
      "컨텍스트 충실성 평균: 4.42\n",
      "컨텍스트 충분성 평균: 4.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, metrics_summary = evaluate_rag_with_llm(questions[:50], contexts[:50], preds[:50], label_lst[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e754c6-8754-4976-b72b-05a16b7e8876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_relevance</th>\n",
       "      <th>context_faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>total_score</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>생성된 답변은 참조 답변과 비교했을 때 대부분 정확하고 포괄적인 정보를 제공하고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>지능형 생산자동화 기반기술을 개발중인 스타트업은?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>개막전에서 3안타 2실점을 기록해서 패한 선수는?</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>생성된 답변은 질문에 대한 정확한 정보를 제공하지 못했습니다. 참조 답변에 따르면 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>컵라면 매출에서 불닭볶음면을 이긴 상품은?</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>생성된 답변은 질문에 대한 정확한 정보를 제공하지 못했습니다. 참조 답변에 따르면 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>정부에게 환경과 관련해서 우선적으로 원조 받고 있는 곳은?</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>생성된 답변은 질문에 대한 직접적인 답변을 제공하지 못하고 있으며, 검색된 컨텍스트...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             question  answer_correctness  \\\n",
       "0   0  북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?                   4   \n",
       "1   1          지능형 생산자동화 기반기술을 개발중인 스타트업은?                   5   \n",
       "2   2          개막전에서 3안타 2실점을 기록해서 패한 선수는?                   2   \n",
       "3   3              컵라면 매출에서 불닭볶음면을 이긴 상품은?                   2   \n",
       "4   4     정부에게 환경과 관련해서 우선적으로 원조 받고 있는 곳은?                   2   \n",
       "\n",
       "   context_relevance  context_faithfulness  context_recall  total_score  \\\n",
       "0                  4                     5               5           18   \n",
       "1                  5                     5               5           20   \n",
       "2                  4                     2               5           13   \n",
       "3                  4                     2               5           13   \n",
       "4                  3                     2               4           11   \n",
       "\n",
       "                                            analysis  \n",
       "0  생성된 답변은 참조 답변과 비교했을 때 대부분 정확하고 포괄적인 정보를 제공하고 있...  \n",
       "1  생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....  \n",
       "2  생성된 답변은 질문에 대한 정확한 정보를 제공하지 못했습니다. 참조 답변에 따르면 ...  \n",
       "3  생성된 답변은 질문에 대한 정확한 정보를 제공하지 못했습니다. 참조 답변에 따르면 ...  \n",
       "4  생성된 답변은 질문에 대한 직접적인 답변을 제공하지 못하고 있으며, 검색된 컨텍스트...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327e5780-1d44-4455-a3da-19a5e0a3f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'평균 총점': 18.22, '응답 정확성 평균': 4.16, '컨텍스트 관련성 평균': 4.74, '컨텍스트 충실성 평균': 4.42, '컨텍스트 충분성 평균': 4.9}\n"
     ]
    }
   ],
   "source": [
    "print(metrics_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb9505-ba09-43df-a0d4-63dbbf6810ae",
   "metadata": {},
   "source": [
    "## 정량 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d819b6-7775-4ea5-9979-60d325e97e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_ref_numbers(text: str) -> List[int]:\r\n",
    "    \"\"\"\r\n",
    "    텍스트에서 [[ref숫자]] 패턴의 숫자들을 추출하여 중복 없이 정렬된 리스트로 반환합니다.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        text (str): 분석할 텍스트\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        List[int]: 추출된 숫자들의 정렬된 리스트\r\n",
    "\r\n",
    "    Example:\r\n",
    "        >>> text = \"이것은 [[ref1]]과 [[ref2]], [[ref1]]입니다.\"\r\n",
    "        >>> extract_ref_numbers(text)\r\n",
    "        [1, 2]\r\n",
    "    \"\"\"\r\n",
    "    # [[ref숫자]] 패턴을 찾는 정규표현식\r\n",
    "    pattern = r'\\[\\[ref(\\d+)\\]\\]'\r\n",
    "\r\n",
    "    # 모든 매치를 찾아서 숫자만 추출\r\n",
    "    numbers = [int(match.group(1)) for match in re.finditer(pattern, text)]\r\n",
    "\r\n",
    "    # 중복 제거하고 정렬하여 반환\r\n",
    "    return sorted(list(set(numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25977aae-a13a-49b1-8cba-c0b9e7b6d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = '''제3차 중동 전쟁과 아랍의 겨울의 주요 원인과 결과는 다음과 같습니다.\n",
    "\n",
    "### 제3차 중동 전쟁 (6일 전쟁)\n",
    "**주요 원인:**\n",
    "1. **티란 해협 봉쇄:** 이집트가 이스라엘 선박의 티란 해협 통과를 봉쇄하면서 긴장이 고조되었습니다. 이스라엘은 이를 전쟁의 명분으로 삼았습니다[[ref1]].\n",
    "2. **군사적 준비:** 이집트는 이스라엘과의 국경에 군을 배치하고, 이스라엘은 이에 대응해 예방적 공습을 감행했습니다[[ref1]].\n",
    "\n",
    "**주요 결과:**\n",
    "1. **영토 확장:** 이스라엘은 가자 지구, 시나이 반도, 동예루살렘, 요르단 강 서안 지구, 골란 고원을 점령하여 영토를 3배로 확장했습니다[[ref1]].\n",
    "2. **난민 발생:** 전쟁으로 인해 약 100,000명의 시리아인과 300,000명의 팔레스타인인이 난민이 되었습니다[[ref1]].\n",
    "3. **군사적 자만:** 이스라엘의 승리는 군사적 자만을 초래하여 이후 1973년 욤키푸르 전쟁에서 아랍 연합군의 초기 승리를 가능하게 했습니다[[ref1]].\n",
    "\n",
    "### 아랍의 겨울\n",
    "**주요 원인:**\n",
    "1. **아랍의 봄의 실패:** 2011년 아랍의 봄 이후 민주화와 개혁을 목표로 한 시도들이 실패하면서 권위주의가 복고되고, 사회적 불안정이 증가했습니다[[ref4]].\n",
    "2. **이슬람 극단주의의 부상:** 이슬람 극단주의와 테러 단체들이 활동을 강화하면서 지역 내 폭력과 혼란이 가중되었습니다[[ref4]].\n",
    "\n",
    "**주요 결과:**\n",
    "1. **내전과 사회 불안정:** 시리아 내전, 이라크 내전, 예멘 내전 등 여러 지역에서 내전과 사회 불안정이 발생했습니다[[ref4]].\n",
    "2. **경제적 손실:** 아랍의 겨울로 인해 발생한 사회적 손실비용은 약 8천억 달러에 달하며, 수백만 명의 난민이 발생했습니다[[ref4]].\n",
    "3. **인도적 위기:** 시리아, 이집트, 이라크 등 여러 지역에서 인도적 지원이 필요한 사람들의 수가 급증했습니다[[ref4]].\n",
    "\n",
    "이와 같이 제3차 중동 전쟁과 아랍의 겨울은 각각 중동 지역의 군사적, 정치적, 사회적 변화를 초래한 중요한 사건들입니다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "631ebdc4-6fce-47c2-926e-89958663b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "cited_docs = extract_ref_numbers(label)\n",
    "print(cited_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6496c6c5-44c5-43df-b91c-3276b9e532aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = '''제3차 중동 전쟁과 아랍의 겨울은 각각 다른 시기에 발생한 사건이며, 주요 원인과 결과는 다음과 같습니다.\n",
    "\n",
    "제3차 중동 전쟁은 1967년 6월에 시작된 이스라엘과 주변 국가인 이집트, 요르단, 시리아, 레바논 사이의 전쟁입니다. 이 전쟁의 주요 원인은 이스라엘과 주변 국가 사이의 긴장과 불안정한 관계였습니다. 이스라엘은 1956년 수에즈 위기 때 주요 목표 중 하나였던 티란 해협을 확보하려고 했고, 이 지역은 1950년 이래 이집트가 이스라엘의 항해를 봉쇄하고 있었습니다. 이스라엘은 이 지역의 봉쇄를 계속해서 유지하려고 했지만, 이는 결국 1967년 6월에 이스라엘과 주변 국가 사이의 전쟁으로 이어졌습니다.\n",
    "\n",
    "전쟁의 결과로, 이스라엘은 이집트로부터 가자 지구와 시나이 반도를, 요르단으로부터 동예루살렘과 요르단 강 서안 지구를, 시리아로부터 골란 고원을 획득했습니다. 이스라엘의 영토는 3배나 커졌고, 이스라엘군은 군사적 승리를 거두었습니다. 그러나 이 전쟁으로 인해 군사들은 자만감에 빠졌고, 이는 1973년 욤키푸르 전쟁 당시 아랍 연합군이 초기에 승리를 거두는 계기가 되었습니다. 또한, 이 전쟁으로 인해 100,000명의 시리아인이 골란 고원을 떠났고, 300,000명의 팔레스타인인이 서안 지구를 떠났는데 이들은 모두 난민이 되었습니다.\n",
    "\n",
    "아랍의 겨울은 2011년 아랍의 봄 이후 아랍 일대에서 벌어지고 있는 광범위한 혼란과 폭력과 사회불안정 사태를 가리키는 말입니다. 이 겨울에는 시리아 내전, 이라크 내전, 이집트 위기, 예멘 내전이 모두 포함됩니다. 권위주의가 복고되고 자유 민권이 억압되고 있으며, 이슬람 극단주의와 테러 단체가 곳곳에서 준동하고 있습니다. 아랍의 겨울은 복수의 지역에서 내란과 내전이 발생하고, 사회가 불안정해지며, 아랍 지역의 경제와 인구가 쇠퇴하고 인종주의와 종교적 종파주의가 판을 치는 것으로 특징지어집니다. 아랍의 겨울의 가장 극단적 사례는 시리아입니다. 바샤르 알 아사드 대통령의 독재에 반대하여 일어난 시위에 알누스라 전선 등의 이슬람주의 집단이 개입하고, 자유 시리아군의 부패와 범죄가 밝혀지면서 이슬람주의자들의 입지가 강화되었습니다. 그리하여 시리아는 독재정부와 반정부 반군과 이슬람주의 테러리스트들의 삼파내전에 휩싸이게 되었으며, 그 여파는 이웃나라인 레바논과 이라크까지 번지고 있습니다. [[ref1]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62e06bb2-23fa-4452-8498-04745d0a0394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "model_pred = extract_ref_numbers(model_prediction)\n",
    "print(model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e41bc1-cde8-41df-8699-c984db366268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(labels, predictions):\n",
    "    \"\"\"\n",
    "    F1 score 계산 (multilabel 케이스) - 이중 리스트 입력 지원\n",
    "    \n",
    "    Args:\n",
    "        labels: 샘플별 실제 레이블이 담긴 리스트 또는 이중 리스트\n",
    "        predictions: 샘플별 예측 레이블이 담긴 리스트 또는 이중 리스트\n",
    "    \n",
    "    Returns:\n",
    "        F1 점수\n",
    "    \"\"\"\n",
    "    # 입력 형식 확인\n",
    "    is_nested_list = isinstance(labels, list) and len(labels) > 0 and isinstance(labels[0], list)\n",
    "    \n",
    "    # 이중 리스트가 아니면 이중 리스트로 변환\n",
    "    if not is_nested_list:\n",
    "        labels = [labels]\n",
    "        predictions = [predictions]\n",
    "    \n",
    "    # 길이 확인\n",
    "    if len(labels) != len(predictions):\n",
    "        raise ValueError(\"라벨과 예측의 길이가 일치하지 않습니다.\")\n",
    "    \n",
    "    # 전체 통계 초기화\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # 각 샘플에 대해 계산\n",
    "    for sample_labels, sample_preds in zip(labels, predictions):\n",
    "        # 빈 리스트는 빈 집합으로 변환\n",
    "        if not sample_labels:\n",
    "            sample_labels = set()\n",
    "        else:\n",
    "            sample_labels = set(sample_labels)\n",
    "            \n",
    "        if not sample_preds:\n",
    "            sample_preds = set()\n",
    "        else:\n",
    "            sample_preds = set(sample_preds)\n",
    "        \n",
    "        # 통계 계산\n",
    "        true_positives = len(sample_labels.intersection(sample_preds))\n",
    "        false_positives = len(sample_preds - sample_labels)\n",
    "        false_negatives = len(sample_labels - sample_preds)\n",
    "        \n",
    "        # 전체 통계에 추가\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # 특수 케이스: 둘 다 빈 리스트인 경우는 F1 = 1.0으로 처리\n",
    "    if sum(bool(l) for l in labels) == 0 and sum(bool(p) for p in predictions) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # 정밀도와 재현율 계산\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    \n",
    "    # F1 점수 계산\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d547c3c-3cff-4a40-95e2-849c5599b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1, 4]\n",
      "Predictions: [1]\n",
      "F1 Score: 0.6667\n",
      "------------------------------\n",
      "Labels: [1, 2, 3]\n",
      "Predictions: [1, 2, 3]\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n",
      "Labels: [1, 2, 3]\n",
      "Predictions: [4, 5, 6]\n",
      "F1 Score: 0.0000\n",
      "------------------------------\n",
      "Labels: [1]\n",
      "Predictions: [1, 2, 3]\n",
      "F1 Score: 0.5000\n",
      "------------------------------\n",
      "Labels: []\n",
      "Predictions: []\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ([1, 4], [1]),  # 기본 케이스\n",
    "    ([1, 2, 3], [1, 2, 3]),  # 완벽한 예측\n",
    "    ([1, 2, 3], [4, 5, 6]),  # 완전히 틀린 예측\n",
    "    ([1], [1, 2, 3]),  # 과대 예측\n",
    "    ([], []),  # 빈 리스트\n",
    "]\n",
    "\n",
    "for labels, predictions in test_cases:\n",
    "    f1 = calculate_f1_score([labels], [predictions])\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcb6f010-457f-43a5-89b3-e422114e5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ref_numbers = []\n",
    "pred_ref_numbers = []\n",
    "\n",
    "for label, pred in zip(label_lst[:50], preds[:50]):\n",
    "    label_ref_numbers.append(extract_ref_numbers(label))\n",
    "    pred_ref_numbers.append(extract_ref_numbers(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e069afcd-918a-4aa1-8301-460bc70d6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [1], [2], [2], [1], [4], [5], [2, 4], [5]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ref_numbers[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6d2034b-91c0-4770-bfce-198295383584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], [], [], [], [], [], []]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ref_numbers[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1d845e3-0463-4c10-976d-44528033f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = calculate_f1_score(label_ref_numbers, pred_ref_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e247555d-3680-4998-858a-6ea67970a138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10169491525423728\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
