{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cc90fd-2f37-4c74-b196-dc075cd7c07e",
   "metadata": {},
   "source": [
    "현재 노트북을 제외한 모든 주피터 노트북에서 상단의 `Kernel > ShudDown Kernel`을 하신 후에 실습을 다시 시작하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6448a3b7-a9c3-4e9e-a437-2227648369de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from: NCSOFT/Llama-VARCO-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b55f0c4a2d41a0a2f8c24ee9cf4276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging PEFT from: ./llama-3-8b-rag-ko/checkpoint-285\n",
      "Saving merged model to: ./output_dir\n",
      "✅ 모델과 토크나이저 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 경로 설정\n",
    "base_model_path = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "adapter_path = \"./llama-3-8b-rag-ko/checkpoint-285\"\n",
    "merged_model_path = \"./output_dir\"\n",
    "\n",
    "# 디바이스 설정\n",
    "device_arg = {\"device_map\": \"auto\"}\n",
    "\n",
    "# 베이스 모델 로드\n",
    "print(f\"Loading base model from: {base_model_path}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    **device_arg\n",
    ")\n",
    "\n",
    "# LoRA 어댑터 로드 및 병합\n",
    "print(f\"Loading and merging PEFT from: {adapter_path}\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path, **device_arg)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# 저장\n",
    "print(f\"Saving merged model to: {merged_model_path}\")\n",
    "model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(\"✅ 모델과 토크나이저 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccdf6f3-b1ed-4ff9-8246-a7a08eea63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "\n",
    "test_dataset = load_from_disk('test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623a0773-a9fc-44d8-a112-2e3d700cc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'NCSOFT/Llama-VARCO-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ec39ab-9aca-4b8a-ab59-f92c511dafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dcfd61e-f5ec-40c4-9de7-ebf80794316a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df15a2b2cbd40d09db77e264d681804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "검색 결과에는 \"방콕 셧다운\"이라는 용어를 처음 누가 사용했는지에 대한 정보를 찾을 수 없습니다.\n",
      "    label:\n",
      "\n",
      "검색 결과에는 '방콕 셧다운'이라는 용어를 처음 사용한 사람이 누구인지에 대한 정보가 없습니다.\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "스뮈츠가 크리칭거 장군에게 가기 전 마지막으로 전투를 벌였던 지역은 서부 트란스발입니다. 스뮈츠는 트란스발에서 500명의 게릴라를 지휘하는 쿠스 드 라 레이 장군의 지휘를 받게 되었고, 그와 함께 여러 게릴라전에서 활약하여 영국군을 격퇴했습니다. 그러나 드 라 레이 장군이 서부 트란스발로 가면서 스뮈츠는 300명의 게릴라들을 이끌게 되었습니다. 이후 스뮈츠는 영국군의 청야전술로 인해 보어인 농장들이 파괴되고 식량 부족으로 인해 게릴라 참여를 거절해야 했지만, 결국 스파르타와 아테네 사이의 적대 관계로 인해 펠로폰네소스 전쟁으로 이어지게 되었습니다 [[ref3]].\n",
      "    label:\n",
      "\n",
      "검색 결과에는 스뮈츠가 크리칭거 장군에게 가기 전 마지막으로 전투를 벌였던 지역을 찾을 수 없습니다.\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "검색 결과에는 일연이 삼국유사를 저술하면서 어떤 자료들을 주로 참고했는지에 대한 정보를 찾을 수 없습니다.\n",
      "    label:\n",
      "\n",
      "검색 결과에 따르면, 일연이 《삼국유사》를 저술하는 데 어떤 자료들을 참고했는지에 대한 정보는 확인되지 않습니다.\n",
      "\n",
      "=> 검색 결과에는 일연이 《삼국유사》를 저술하면서 참고한 자료들에 대한 언급이 없습니다.\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "검색 결과에는 서울 행복주택 입주자가 되기 위해 필요한 서류를 찾을 수 없습니다.\n",
      "    label:\n",
      "\n",
      "검색 결과에는 서울 행복주택 입주자가 제출해야 하는 서류에 대한 구체적인 정보가 포함되어 있지 않습니다.\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "검색 결과에는 자급제 휴대폰의 개선법이 통과된 이후 첫해의 자급제 휴대폰 판매량에 대한 정보를 찾을 수 없습니다.\n",
      "    label:\n",
      "\n",
      "검색 결과에는 자급제 휴대폰의 개선법이 통과된 이후 첫해의 자급제 휴대폰 판매량 변화에 대한 정보가 없습니다.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_id = './output_dir'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e508e0-e233-4d56-9a99-ce77dcd06ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "username = \"iamjoon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92776173-b3a0-40bf-a397-2a73f568c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'llama-3-8b-rag-ko-checkpoint-285'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7511453b-0bdb-4f99-952f-aef18b124e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3178d62114fa4fe2970df12e12e1a97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a06b995d47845fda08d2849d8b1069b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227157358d93435fa218683b8d545420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb8287dabb44f15955ba594b6e8671f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5089decb467a4751b87d00340c5f8e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cb671f8c824d208a7cbe73cc7bb562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/iamjoon/llama-3-8b-rag-ko-checkpoint-285/commit/98cefe39bc38e0562a292dac184da497caa88ff7', commit_message='Upload folder using huggingface_hub', commit_description='', oid='98cefe39bc38e0562a292dac184da497caa88ff7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/iamjoon/llama-3-8b-rag-ko-checkpoint-285', endpoint='https://huggingface.co', repo_type='model', repo_id='iamjoon/llama-3-8b-rag-ko-checkpoint-285'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(\n",
    "    token=\"hf_여러분의 Key 값\",\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    token=\"hf_여러분의 Key 값\",\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"output_dir\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
