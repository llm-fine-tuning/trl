{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3745307-e926-4218-a6fc-9c12175805ac",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b81a4-2477-4262-8006-8e6e80dde05f",
   "metadata": {},
   "source": [
    "runpod에서 a100 GPU 1개 sxm 대여해주시고 container disk는 50기가 바이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63702f92-6c10-4a90-9cb9-e3e950bfccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m190.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m181.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m137.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0 typing-extensions-4.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.8.93)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m196.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m148.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m183.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m196.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.0/334.0 kB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typeguard, tqdm, shtab, safetensors, requests, regex, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.30.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 regex-2024.11.6 requests-2.32.3 rich-14.0.0 safetensors-0.5.3 shtab-1.7.2 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.45.1 trl-0.11.1 typeguard-4.4.2 tyro-0.9.18 tzdata-2025.2 xxhash-3.5.0 yarl-1.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.4.0\"\n",
    "%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eaf62e-8aee-4331-803e-20d92e6cf689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e07c5-04d3-48fb-8a30-2113838c4384",
   "metadata": {},
   "source": [
    "빠른 학습을 위해 학습 데이터와 테스트 데이터를 2:8 비율로 분할합니다. 이 값을 변경하고자 하는 분은 test_ratio의 값을 변경하세요.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d9837a-d604-443d-9afb-7e343d0d1011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8472db7b6e134f7288463dc1f699653b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/909 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce44a4fe5314c91995ca46fc60f714f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/13.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d765f885f47f428e8fe8f0d7d9fc0137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터의 type 분포:\n",
      "paraphrased_question: 196\n",
      "mrc_question: 491\n",
      "no_answer: 404\n",
      "synthetic_question: 497\n",
      "mrc_question_with_1_to_4_negative: 296\n",
      "\n",
      "전체 데이터 분할 결과: Train 380개, Test 1504개\n",
      "\n",
      "학습 데이터의 type 분포:\n",
      "paraphrased_question: 40\n",
      "mrc_question: 99\n",
      "no_answer: 81\n",
      "synthetic_question: 100\n",
      "mrc_question_with_1_to_4_negative: 60\n",
      "\n",
      "테스트 데이터의 type 분포:\n",
      "paraphrased_question: 156\n",
      "mrc_question: 392\n",
      "no_answer: 323\n",
      "synthetic_question: 397\n",
      "mrc_question_with_1_to_4_negative: 236\n"
     ]
    }
   ],
   "source": [
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n",
    "\n",
    "# 2. system_message 정의\n",
    "system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
    "\n",
    "다음의 지시사항을 따르십시오.\n",
    "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
    "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
    "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
    "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
    "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
    "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
    "\n",
    "검색 결과:\n",
    "-----\n",
    "{search_result}\"\"\"\n",
    "\n",
    "# 3. 원본 데이터의 type별 분포 출력 \n",
    "print(\"원본 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    print(f\"{type_name}: {dataset['type'].count(type_name)}\")\n",
    "\n",
    "# 4. train/test 분할 비율 설정 (0.5면 5:5로 분할)\n",
    "test_ratio = 0.8\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 5. type별로 순회하면서 train/test 데이터 분할\n",
    "for type_name in set(dataset['type']):\n",
    "    # 현재 type에 해당하는 데이터의 인덱스만 추출\n",
    "    curr_type_data = [i for i in range(len(dataset)) if dataset[i]['type'] == type_name]\n",
    "    \n",
    "    # test_ratio에 따라 test 데이터 개수 계산 \n",
    "    test_size = int(len(curr_type_data) * test_ratio)\n",
    "    \n",
    "    # 현재 type의 데이터를 test_ratio 비율로 분할하여 추가\n",
    "    test_data.extend(curr_type_data[:test_size])\n",
    "    train_data.extend(curr_type_data[test_size:])\n",
    "\n",
    "# 6. OpenAI format으로 데이터 변환을 위한 함수 \n",
    "def format_data(sample):\n",
    "    # 검색 결과를 문서1, 문서2... 형태로 포매팅\n",
    "    search_result = \"\\n-----\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(sample[\"search_result\"])])\n",
    "    \n",
    "    # OpenAI format으로 변환\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message.format(search_result=search_result),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"answer\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 7. 분할된 데이터를 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "# 8. 최종 데이터셋 크기 출력\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")\n",
    "\n",
    "# 9. 분할된 데이터의 type별 분포 출력\n",
    "print(\"\\n학습 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in train_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")\n",
    "\n",
    "print(\"\\n테스트 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in test_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01634b1-25ad-46ae-9d19-8923fcd3c536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: LED(발광다이오드) 조명 등을 만드는 동부그룹 계열사 동부라이텍은 일본 요코하마에 LED 라이트 패널(루미시트) 생산공장을 완공, 본격 양산에 들어갔다고 31일 발표했다. 이 공장은 일본 현지 유통사인 테크타이토와 합작해 세운 공장이다. 루미시트는 얇은 종이판 형태의 LED 조명으로, 이 공장에서는 광고 인테리어용 루미시트 4종을 양산한다.동부라이텍은 2008년 캐나다 토론토에 현지 합작법인 DLC를 세워 북미 고급 매장에서 사용하는 진열대용 루미시트를 생산하고 있다. DLC는 올해 상반기에 약 200억원의 매출을 올렸고, 순이익은 최근 수년간 매년 20%씩 증가하고 있다. 요코하마 공장은 캐나다에서의 성공 모델을 일본으로 옮겨온 것이라는 게 회사 측 설명이다. 동부라이텍은 테크타이토와 합작해 지난해 8월 도쿄에 자본금 1억엔 규모의 합작법인 씨엔디라이텍을 설립한 뒤 현지 공장 가동을 준비해 왔다. 동부라이텍은 테크타이토의 일본 내 유통망을 활용해 일본 루미시트 시장에서 점유율을 늘릴 수 있을 것으로 기대하고 있다.'},\n",
       " {'role': 'user', 'content': '테크타이토와 합작해서 지은 공장은 어느 지역에 있는가?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '테크타이토와 합작해서 지은 공장은 일본 요코하마에 있습니다 [[ref1]].'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038167a1-0a6f-4b9c-8aa9-e4e753d36076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7285dd35-4047-415f-95f7-62abec39bfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: 서울 북촌에 자리잡은 서울교육청 정독도서관은 옛 학교 건물을 그대로 물려받았다. 도서관이 보금자리로 쓰고 있는 옛 경기고 건물은 등록문화재 제2호로 1938년 건립됐다. 경기고가 1976년 서울 삼성동으로 이전하면서 이듬해 도서관으로 새롭게 문을 열었다. 개관 때부터 학교 운동장을 아름다운 정원으로 꾸민 덕에 북촌 주민과 주변 직장인은 물론 관광객도 자주 찾는 명소가 됐다. 많은 사람이 이곳을 추억을 간직한 장소로 기억하는 이유다.아름다운 건물 외관과 더불어 정취로 잘 알려진 정독도서관은 도서관 운영 면에서도 모범사례로 꼽힌다. 도서관을 찾은 사람들은 방대한 자료와 쾌적한 환경에 놀란다. 장서는 50만권이 넘고 바로 옆에 있는 서울교육박물관에는 유물 1만3000여점이 있다. 학교 건물을 도서관으로 만들어 자료실과 열람실이 일반도서관에 비해 훨씬 넓다.정독도서관은 최근 서울교육청이 진행하는 도서관 특성화 사업에서 ‘청소년 중심 도서관’으로 지정됐다. 앞으로 3년 동안 중·고교 교과서에 나오는 소설과 진로지도 관련 자료, 특성화고 학생에게 필요한 취업 관련 자료 등을 집중적으로 구비할 예정이다. 오는 9월 리모델링으로 조성되는 청소년관엔 자료실과 함께 독서토론이나 강의를 할 수 있는 공간도 마련된다. 어린이관은 확장해 가족단위 이용자 방문을 늘릴 계획이다.정독도서관은 지난 5월부터 서울시내 고교에서 모집한 학생을 대상으로 독서논술교육을 하고 있다. 김성갑 관장은 “학생들이 어려워하는 대입논술의 길잡이 역할을 하기 위해 마련한 교육 프로그램”이라며 “아직은 수강생이 30명 정도지만 성공사례로 만들어 다른 도서관으로 확산하고 싶다”고 말했다.\\n-----\\n문서2: 서울도서관은 다음달 4일까지 1층 기획전시실에서 ‘장서표(藏書票)의 세계, 책과 사람들: 남궁산 목판화 장서표전’을 연다. 소장자의 표식이자 책의 장식인 장서표는 예술성을 가미하기 위해 문자와 그림을 조합해 판화로 제작하는 경우가 많다. 이번 전시에는 고은, 안도현, 정호승, 공지영 등 유명 작가들의 도서와 작가의 삶과 이야기를 담은 판화가 남궁산의 장서표 49종이 전시된다.국립중앙도서관과 서울대 중앙도서관은 한글역주 ‘자치통감(資治通鑑)’의 전자책 서비스를 시작했다. 제왕학의 교과서로 불리는 자치통감은 ‘춘추’ ‘사기’와 함께 동양 3대 역사서로 꼽힌다. 송나라 사마광이 전국(戰國) 시대부터 오대(五代)까지 1362년간의 역사를 편년체로 기록한 중국 통사다. 한글판은 권중달 중앙대 명예교수가 2010년 전 32권으로 완간한 종이책을 전자책으로 만든 것이다. 국립중앙도서관에선 도서관 열람회원들이 관내 PC나 태블릿을 통해 열람할 수 있다. 서울대 중앙도서관에선 소속 교수와 교직원, 재학생과 중앙도서관 동문회원들이 이용할 수 있다.충북 제천시립도서관은 12월까지 ‘청풍호 수몰 30주년 사진’을 1층 로비에 전시한다. 수몰 30주년 사진은 청풍호반과 함께 어우러진 산의 형태를 이미지화한 패널 형식으로 제작됐으며 고향, 가족, 친구 이야기로 구성돼 있다. 이번 장기 전시는 수몰된 청풍의 30년 전 모습을 알지 못하는 시민들이 제천의 숨겨진 역사를 새롭게 발견하는 계기를 주기 위해 기획됐다.서대문구립이진아기념도서관은 오는 11월12일까지 매주 목요일 청각장애인을 위한 독서프로그램을 운영한다. 10회에 걸쳐 운영되는 이번 프로그램은 국립중앙도서관에서 주관하는 ‘장애인 독서프로그램 공모’에 선정된 사업으로 분야별 책을 선정하고 수화통역사와 함께 책 읽기를 진행한다. 청각장애인이면 누구나 무료로 이 프로그램에 참여할 수 있으며, 도서관에 방문하거나 이메일(younghwa@sscmc.or.kr)로 신청하면 된다.\\n-----\\n문서3: 서울 사직동 서울교육청어린이도서관은 올 상반기 철거 위기를 겪었다. 문화재청이 복원을 추진하고 있는 사직단(사적 제121호) 권역에 도서관이 자리잡고 있어서였다. 사직단 권역은 국가 소유 땅이기 때문에 임대 연장을 승인하지 않으면 쫓겨날 상황이었다. 다행히 국내 최초의 어린이도서관을 지켜야 한다는 여론이 높아 문화재청은 어린이도서관을 복원 계획에서 제외하는 쪽으로 정리했다. 36년간 ‘어린이와 함께하며 미래를 열어가는 도서관’을 목표로 아이들과 함께 성장하고 발전하며 자리를 지켜온 전통과 상징을 인정받은 것이다.어린이도서관은 1979년 5월 ‘세계 어린이의 해’를 기념해 개관했다. 도서관이 들어선 건물은 이전까지 시립어린이병원으로 쓰여 어린이와 함께한 역사가 깊다.이 도서관은 어린이전문 도서관답게 아동도서에 특화돼 있다. 보유 장서는 27만5000권으로, 90%가량이 아동도서다. 홍순영 관장은 “1년에 구입하는 책만 2만2000여권”이라며 “국내에서 출간되는 아동서는 대부분 소장하고 있다”고 설명했다.이용자의 희망 도서 위주로 새 책이 들어오는 매주 수요일엔 사람들로 북적인다. 오랜 시간 입소문이 퍼지면서 경기 고양, 성남 등 수도권에서도 이용자가 몰린다. 일반 이용자 외에 유아교육 연구자들도 단골손님이다. 하루평균 대출 도서는 2000권을 훌쩍 넘는다. 다문화도서실에는 일본, 중국, 몽골 등 7개국 언어로 제작한 아동도서를 볼 수 있다.사서 18명이 만드는 권장도서 목록은 어린이도서관의 자랑이다. 5월 ‘가정의 달’과 여름·겨울방학 등 세 차례에 걸쳐 미취학 아동 그림책과 학년별 도서 등 40권을 추천한다. 목록을 발표할 때가 되면 서울시내 초등학교는 물론 학부모로부터 “언제쯤 나오느냐”는 질문이 쏟아진다. 학기마다 두 번 열리는 독서증진대회는 어린이도서관과 역사를 같이하는 행사로, 교장 추천을 받은 어린이들이 모인다. 독서감상회, 동화구연, 독서감상문, 글짓기 행사 등이 열린다. 매주 토요일에는 다양한 문화·체험 행사와 프로그램이 운영된다.도서관은 최근 문화관 1층 전시실을 개조해 ‘체험동화마을’을 조성했다. 대형 스크린에 아이들이 동화 배경과 함께 나오게 해 동화 속 주인공이 된 느낌을 준다. 매주 수요일 단체신청자를 대상으로 프로그램을 진행한다. 다음달부터는 개별적으로도 체험할 수 있다. 평일 오후 유아실에는 할머니가 들려주는 동화구연 프로그램이 진행된다.홍 관장은 “예전에 도서관을 찾았던 어린이들이 이제는 부모가 돼 아이 손을 잡고 찾아오는 걸 볼 때면 가슴이 뭉클하다”며 “아이들과 부모가 함께 참여해 공감을 나눌 수 있는 프로그램을 늘릴 계획”이라고 말했다.\\n-----\\n문서4: 420여개의 출판사와 인쇄업체가 입주해 있는 파주출판도시 한가운데에 자리한 아시아출판문화정보센터와 게스트하우스 ‘지지향(紙之鄕)’. 지지향 로비에 들어서면 기둥과 벽을 가득 채운 책장이 먼저 눈에 띈다. 건물이 연결된 통로를 지나 출판문화정보센터로 들어서면 더욱 큰 서가가 방문객을 맞이한다. 1256㎡에 이르는 공간의 벽마다 책장을 설치해 책을 채워 넣었다. 높이 6m가 넘는 책장을 보면 얼마나 많은 책이 꽂혀 있는지 가늠하기 어려울 정도다. 이 공간은 내달 19일 문을 여는 ‘열린 도서관-지혜의 숲’이다. 출판도시문화재단(이사장 김언호·한길사 대표)이 지난해 5월 설립에 착수해 1년 만에 개관을 앞두고 있다. ‘지혜의 숲’은 재단이 여러 출판사와 지식인, 학자를 비롯해 다양한 사람들로부터 기증받은 도서로 꾸민 전면 개가식 도서관이다. 개별 서재들의 거대한 집합인 셈. 중국 일본 대만 등에서도 책을 보내왔다. 파주출판도시를 찾은 사람들이라면 누구나 제한 없이 이용할 수 있다. 100만권을 기증받는다는 목표 아래 사업을 시작해 현재 50만권이 확보된 상태. 1단계로 20만권의 책이 서가에 꽂혀 있다. 책을 관리하는 사서가 없고 보통의 도서관처럼 체계적으로 분류하지 않아 다소 생소하지만 다양한 분야의 책이 한눈에 들어올 만큼 널찍하다. ‘지혜의 숲’ 도서관의 가장 큰 특징은 출판사와 기증자별 서가다. 책을 기증한 출판사 서가를 찾으면 출판사가 그동안 낸 책을 모두 만날 수 있다. 대형 서점을 찾아도 일반 도서관처럼 분야별로 분류된 책만 볼 수 있지만, 이곳에선 출판사가 어떤 철학으로 책을 만들어 왔는지 한눈에 볼 수 있다. 책을 좋아하는 이들이라면 민음사 세계문학전집, 한길 그레이트북스 같은 전집이 모두 꽂힌 서가에 반할 수밖에 없다. 많은 학자들이 선뜻 기증한 책을 보면 한 연구자가 그동안 어떤 책을 읽으며 공부했는지 지식의 이력서를 보는 재미를 느낄 수 있다. 인류의 가장 위대한 정신·문화 유산인 종이책이 함부로 버려지는 걸 안타까워하며 책 리사이클링 운동과 독서운동 활성화를 제창해온 김 이사장은 “수많은 책이 쏟아지면서 독자들에게 채 읽히지도 못한 채 폐기되는 안타까운 상황이 빈번하다”며 “이미 나온 책이라 하더라도 존중하지 않는다면 우리에게 발전은 없다”고 강조했다. 그는 이어 “젊은 세대가 책을 잘 읽지 않는 현상을 항상 우려해왔다”며 “전문서는 물론 교양서를 두루 갖춰 책이 사람들에게 좀 더 친근하게 다가설 수 있도록 만들었다”고 설명했다. 도서관 운영은 사서 대신 30여명의 권독사(勸讀司)들이 맡는다. 책을 소개하고 독서를 권하는 자원봉사자다. 권독사 교육을 담당하는 번역가 박종일 씨는 “대만 고궁박물관에서 허름한 차림의 할아버지가 어린 학생들에게 갑골문 이야기를 들려주는 것을 본 적이 있는데 그 분이 당대 최고의 갑골문 학자였다는 사실을 알고 놀랐다”며 “지식을 가진 사람이 젊은이들에게 독서를 통한 앎의 기쁨을 전달하는 것이 이 도서관의 목적”이라고 설명했다. ‘지혜의 숲’의 지지향 로비 서가는 24시간 열람할 수 있으며 점차 열람 범위를 확대할 방침이다. (031)955-0050\\n-----\\n문서5: 국립중앙도서관은 ‘조선과 청조(淸朝) 문인의 만남’이라는 주제로 오는 12월30일까지 본관 6층 고전운영실에서 고문헌 전시회를 연다. 조선 실학자 홍대용이 항주 선비들과 주고받은 필담 및 편지가 수록된 ‘담헌서’, 이덕무 유득공 박제가 이서구의 시를 모아놓은 ‘한객건연집’, 김정희가 청조 전각가의 인장집 표지에 평을 쓴 ‘일석산방인록’ 등 25종 133책의 고문헌을 전시한다. 국립중앙도서관 관계자는 “조선과 청의 문명 교류사를 조명하는 자리”라고 설명했다. 자세한 전시 목록은 도서관 홈페이지(www.nl.go.kr)의 ‘소통·참여→전시행사’에서 확인할 수 있다.서울교육청 정독도서관이 지난달 30일 공공도서관 중 처음으로 청소년 특화 전문도서관을 개관했다. 도서관 내 378.5㎡ 규모로 조성된 청소년관은 일반 중·고교생과 정규학업을 중단하고 검정고시 등을 준비하는 ‘학교 밖 청소년’, 학부모와 교사 등을 위해 마련됐다. 진로·진학자료실, 독서상담실, 토론실 등으로 구성됐다. 청소년 도서 7000여권, 간행물 28종, 대학별 입학 안내서 150여종, 검인정 교과서 458종 등을 비치하고 데스크톱 컴퓨터 6대를 설치했다.경기 용인 수지도서관(관장 신현국)은 오는 10일 낮 12시부터 ‘러시아문학, 연극과 만나다’ 행사를 연다. ‘길 위의 인문학’ 3차 프로그램인 이번 행사는 강연과 탐방으로 구성된다. 러시아 문학의 황금기인 19세기 대문호들의 삶 및 작품에 대한 강연과 서울 대학로 극장을 찾아 안톤 체호프의 대표 희곡 ‘갈매기’를 감상하는 시간으로 진행된다. ‘로쟈의 인문학 서재’의 저자 이현우 씨가 도서관 2층 시청각실에서 강연을 하고 탐방을 이끈다. 참가 대상은 19세 이상 일반인 40명으로 도서관 홈페이지(www.yonginlib.go.kr)에서 신청하면 된다. 참가비는 무료.',\n",
       "   'role': 'system'},\n",
       "  {'content': '다량의 검인정 교과서를 갖춘 도서관', 'role': 'user'},\n",
       "  {'content': \"서울교육청 정독도서관은 다량의 검인정 교과서를 갖춘 도서관으로, 최근 청소년 특화 전문도서관을 개관했습니다. 이 도서관은 일반 중·고교생뿐만 아니라 정규학업을 중단하고 검정고시 등을 준비하는 '학교 밖 청소년'을 위해 마련되었습니다. 청소년관은 진로·진학자료실, 독서상담실, 토론실 등으로 구성되어 있으며, 청소년 도서 7000여 권, 간행물 28종, 대학별 입학 안내서 150여 종, 검인정 교과서 458종 등을 비치하고 있습니다. 또한, 데스크톱 컴퓨터 6대도 설치되어 있어 다양한 학습 자료를 제공하고 있습니다[[ref5]].\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eef45da-7f4f-4bc0-b322-46390c21ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f13e83b23d4cd4b184991f4f331821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 저장\n",
    "test_dataset.save_to_disk(\"test_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94383df4-9c95-47ab-b686-e32f8ce4f28c",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520b546c-f82f-4a44-951d-ea85dec73a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e7b1c0f1e24b088e4281c57abece92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3850f11011114aea9391cb748841d5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff546693b0b47939da6c12032c79cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dea63b4e1894c349a5cf5b338917f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d86afff0524279afd37d8035db5592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c1c920b3324c53999335e562db3b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52960b3566d748d5951cda96ab86ebf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045c54edae6141bbb78cde7da791decd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5100c839fd452398e0f26b0ba2a6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b7b061f3914221bc50621087e0639f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8056eafba414e5799098fd66ec47d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7aca009-8f0e-4579-b018-6fbcb2f2ad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: 서울 북촌에 자리잡은 서울교육청 정독도서관은 옛 학교 건물을 그대로 물려받았다. 도서관이 보금자리로 쓰고 있는 옛 경기고 건물은 등록문화재 제2호로 1938년 건립됐다. 경기고가 1976년 서울 삼성동으로 이전하면서 이듬해 도서관으로 새롭게 문을 열었다. 개관 때부터 학교 운동장을 아름다운 정원으로 꾸민 덕에 북촌 주민과 주변 직장인은 물론 관광객도 자주 찾는 명소가 됐다. 많은 사람이 이곳을 추억을 간직한 장소로 기억하는 이유다.아름다운 건물 외관과 더불어 정취로 잘 알려진 정독도서관은 도서관 운영 면에서도 모범사례로 꼽힌다. 도서관을 찾은 사람들은 방대한 자료와 쾌적한 환경에 놀란다. 장서는 50만권이 넘고 바로 옆에 있는 서울교육박물관에는 유물 1만3000여점이 있다. 학교 건물을 도서관으로 만들어 자료실과 열람실이 일반도서관에 비해 훨씬 넓다.정독도서관은 최근 서울교육청이 진행하는 도서관 특성화 사업에서 ‘청소년 중심 도서관’으로 지정됐다. 앞으로 3년 동안 중·고교 교과서에 나오는 소설과 진로지도 관련 자료, 특성화고 학생에게 필요한 취업 관련 자료 등을 집중적으로 구비할 예정이다. 오는 9월 리모델링으로 조성되는 청소년관엔 자료실과 함께 독서토론이나 강의를 할 수 있는 공간도 마련된다. 어린이관은 확장해 가족단위 이용자 방문을 늘릴 계획이다.정독도서관은 지난 5월부터 서울시내 고교에서 모집한 학생을 대상으로 독서논술교육을 하고 있다. 김성갑 관장은 “학생들이 어려워하는 대입논술의 길잡이 역할을 하기 위해 마련한 교육 프로그램”이라며 “아직은 수강생이 30명 정도지만 성공사례로 만들어 다른 도서관으로 확산하고 싶다”고 말했다.\n",
      "-----\n",
      "문서2: 서울도서관은 다음달 4일까지 1층 기획전시실에서 ‘장서표(藏書票)의 세계, 책과 사람들: 남궁산 목판화 장서표전’을 연다. 소장자의 표식이자 책의 장식인 장서표는 예술성을 가미하기 위해 문자와 그림을 조합해 판화로 제작하는 경우가 많다. 이번 전시에는 고은, 안도현, 정호승, 공지영 등 유명 작가들의 도서와 작가의 삶과 이야기를 담은 판화가 남궁산의 장서표 49종이 전시된다.국립중앙도서관과 서울대 중앙도서관은 한글역주 ‘자치통감(資治通鑑)’의 전자책 서비스를 시작했다. 제왕학의 교과서로 불리는 자치통감은 ‘춘추’ ‘사기’와 함께 동양 3대 역사서로 꼽힌다. 송나라 사마광이 전국(戰國) 시대부터 오대(五代)까지 1362년간의 역사를 편년체로 기록한 중국 통사다. 한글판은 권중달 중앙대 명예교수가 2010년 전 32권으로 완간한 종이책을 전자책으로 만든 것이다. 국립중앙도서관에선 도서관 열람회원들이 관내 PC나 태블릿을 통해 열람할 수 있다. 서울대 중앙도서관에선 소속 교수와 교직원, 재학생과 중앙도서관 동문회원들이 이용할 수 있다.충북 제천시립도서관은 12월까지 ‘청풍호 수몰 30주년 사진’을 1층 로비에 전시한다. 수몰 30주년 사진은 청풍호반과 함께 어우러진 산의 형태를 이미지화한 패널 형식으로 제작됐으며 고향, 가족, 친구 이야기로 구성돼 있다. 이번 장기 전시는 수몰된 청풍의 30년 전 모습을 알지 못하는 시민들이 제천의 숨겨진 역사를 새롭게 발견하는 계기를 주기 위해 기획됐다.서대문구립이진아기념도서관은 오는 11월12일까지 매주 목요일 청각장애인을 위한 독서프로그램을 운영한다. 10회에 걸쳐 운영되는 이번 프로그램은 국립중앙도서관에서 주관하는 ‘장애인 독서프로그램 공모’에 선정된 사업으로 분야별 책을 선정하고 수화통역사와 함께 책 읽기를 진행한다. 청각장애인이면 누구나 무료로 이 프로그램에 참여할 수 있으며, 도서관에 방문하거나 이메일(younghwa@sscmc.or.kr)로 신청하면 된다.\n",
      "-----\n",
      "문서3: 서울 사직동 서울교육청어린이도서관은 올 상반기 철거 위기를 겪었다. 문화재청이 복원을 추진하고 있는 사직단(사적 제121호) 권역에 도서관이 자리잡고 있어서였다. 사직단 권역은 국가 소유 땅이기 때문에 임대 연장을 승인하지 않으면 쫓겨날 상황이었다. 다행히 국내 최초의 어린이도서관을 지켜야 한다는 여론이 높아 문화재청은 어린이도서관을 복원 계획에서 제외하는 쪽으로 정리했다. 36년간 ‘어린이와 함께하며 미래를 열어가는 도서관’을 목표로 아이들과 함께 성장하고 발전하며 자리를 지켜온 전통과 상징을 인정받은 것이다.어린이도서관은 1979년 5월 ‘세계 어린이의 해’를 기념해 개관했다. 도서관이 들어선 건물은 이전까지 시립어린이병원으로 쓰여 어린이와 함께한 역사가 깊다.이 도서관은 어린이전문 도서관답게 아동도서에 특화돼 있다. 보유 장서는 27만5000권으로, 90%가량이 아동도서다. 홍순영 관장은 “1년에 구입하는 책만 2만2000여권”이라며 “국내에서 출간되는 아동서는 대부분 소장하고 있다”고 설명했다.이용자의 희망 도서 위주로 새 책이 들어오는 매주 수요일엔 사람들로 북적인다. 오랜 시간 입소문이 퍼지면서 경기 고양, 성남 등 수도권에서도 이용자가 몰린다. 일반 이용자 외에 유아교육 연구자들도 단골손님이다. 하루평균 대출 도서는 2000권을 훌쩍 넘는다. 다문화도서실에는 일본, 중국, 몽골 등 7개국 언어로 제작한 아동도서를 볼 수 있다.사서 18명이 만드는 권장도서 목록은 어린이도서관의 자랑이다. 5월 ‘가정의 달’과 여름·겨울방학 등 세 차례에 걸쳐 미취학 아동 그림책과 학년별 도서 등 40권을 추천한다. 목록을 발표할 때가 되면 서울시내 초등학교는 물론 학부모로부터 “언제쯤 나오느냐”는 질문이 쏟아진다. 학기마다 두 번 열리는 독서증진대회는 어린이도서관과 역사를 같이하는 행사로, 교장 추천을 받은 어린이들이 모인다. 독서감상회, 동화구연, 독서감상문, 글짓기 행사 등이 열린다. 매주 토요일에는 다양한 문화·체험 행사와 프로그램이 운영된다.도서관은 최근 문화관 1층 전시실을 개조해 ‘체험동화마을’을 조성했다. 대형 스크린에 아이들이 동화 배경과 함께 나오게 해 동화 속 주인공이 된 느낌을 준다. 매주 수요일 단체신청자를 대상으로 프로그램을 진행한다. 다음달부터는 개별적으로도 체험할 수 있다. 평일 오후 유아실에는 할머니가 들려주는 동화구연 프로그램이 진행된다.홍 관장은 “예전에 도서관을 찾았던 어린이들이 이제는 부모가 돼 아이 손을 잡고 찾아오는 걸 볼 때면 가슴이 뭉클하다”며 “아이들과 부모가 함께 참여해 공감을 나눌 수 있는 프로그램을 늘릴 계획”이라고 말했다.\n",
      "-----\n",
      "문서4: 420여개의 출판사와 인쇄업체가 입주해 있는 파주출판도시 한가운데에 자리한 아시아출판문화정보센터와 게스트하우스 ‘지지향(紙之鄕)’. 지지향 로비에 들어서면 기둥과 벽을 가득 채운 책장이 먼저 눈에 띈다. 건물이 연결된 통로를 지나 출판문화정보센터로 들어서면 더욱 큰 서가가 방문객을 맞이한다. 1256㎡에 이르는 공간의 벽마다 책장을 설치해 책을 채워 넣었다. 높이 6m가 넘는 책장을 보면 얼마나 많은 책이 꽂혀 있는지 가늠하기 어려울 정도다. 이 공간은 내달 19일 문을 여는 ‘열린 도서관-지혜의 숲’이다. 출판도시문화재단(이사장 김언호·한길사 대표)이 지난해 5월 설립에 착수해 1년 만에 개관을 앞두고 있다. ‘지혜의 숲’은 재단이 여러 출판사와 지식인, 학자를 비롯해 다양한 사람들로부터 기증받은 도서로 꾸민 전면 개가식 도서관이다. 개별 서재들의 거대한 집합인 셈. 중국 일본 대만 등에서도 책을 보내왔다. 파주출판도시를 찾은 사람들이라면 누구나 제한 없이 이용할 수 있다. 100만권을 기증받는다는 목표 아래 사업을 시작해 현재 50만권이 확보된 상태. 1단계로 20만권의 책이 서가에 꽂혀 있다. 책을 관리하는 사서가 없고 보통의 도서관처럼 체계적으로 분류하지 않아 다소 생소하지만 다양한 분야의 책이 한눈에 들어올 만큼 널찍하다. ‘지혜의 숲’ 도서관의 가장 큰 특징은 출판사와 기증자별 서가다. 책을 기증한 출판사 서가를 찾으면 출판사가 그동안 낸 책을 모두 만날 수 있다. 대형 서점을 찾아도 일반 도서관처럼 분야별로 분류된 책만 볼 수 있지만, 이곳에선 출판사가 어떤 철학으로 책을 만들어 왔는지 한눈에 볼 수 있다. 책을 좋아하는 이들이라면 민음사 세계문학전집, 한길 그레이트북스 같은 전집이 모두 꽂힌 서가에 반할 수밖에 없다. 많은 학자들이 선뜻 기증한 책을 보면 한 연구자가 그동안 어떤 책을 읽으며 공부했는지 지식의 이력서를 보는 재미를 느낄 수 있다. 인류의 가장 위대한 정신·문화 유산인 종이책이 함부로 버려지는 걸 안타까워하며 책 리사이클링 운동과 독서운동 활성화를 제창해온 김 이사장은 “수많은 책이 쏟아지면서 독자들에게 채 읽히지도 못한 채 폐기되는 안타까운 상황이 빈번하다”며 “이미 나온 책이라 하더라도 존중하지 않는다면 우리에게 발전은 없다”고 강조했다. 그는 이어 “젊은 세대가 책을 잘 읽지 않는 현상을 항상 우려해왔다”며 “전문서는 물론 교양서를 두루 갖춰 책이 사람들에게 좀 더 친근하게 다가설 수 있도록 만들었다”고 설명했다. 도서관 운영은 사서 대신 30여명의 권독사(勸讀司)들이 맡는다. 책을 소개하고 독서를 권하는 자원봉사자다. 권독사 교육을 담당하는 번역가 박종일 씨는 “대만 고궁박물관에서 허름한 차림의 할아버지가 어린 학생들에게 갑골문 이야기를 들려주는 것을 본 적이 있는데 그 분이 당대 최고의 갑골문 학자였다는 사실을 알고 놀랐다”며 “지식을 가진 사람이 젊은이들에게 독서를 통한 앎의 기쁨을 전달하는 것이 이 도서관의 목적”이라고 설명했다. ‘지혜의 숲’의 지지향 로비 서가는 24시간 열람할 수 있으며 점차 열람 범위를 확대할 방침이다. (031)955-0050\n",
      "-----\n",
      "문서5: 국립중앙도서관은 ‘조선과 청조(淸朝) 문인의 만남’이라는 주제로 오는 12월30일까지 본관 6층 고전운영실에서 고문헌 전시회를 연다. 조선 실학자 홍대용이 항주 선비들과 주고받은 필담 및 편지가 수록된 ‘담헌서’, 이덕무 유득공 박제가 이서구의 시를 모아놓은 ‘한객건연집’, 김정희가 청조 전각가의 인장집 표지에 평을 쓴 ‘일석산방인록’ 등 25종 133책의 고문헌을 전시한다. 국립중앙도서관 관계자는 “조선과 청의 문명 교류사를 조명하는 자리”라고 설명했다. 자세한 전시 목록은 도서관 홈페이지(www.nl.go.kr)의 ‘소통·참여→전시행사’에서 확인할 수 있다.서울교육청 정독도서관이 지난달 30일 공공도서관 중 처음으로 청소년 특화 전문도서관을 개관했다. 도서관 내 378.5㎡ 규모로 조성된 청소년관은 일반 중·고교생과 정규학업을 중단하고 검정고시 등을 준비하는 ‘학교 밖 청소년’, 학부모와 교사 등을 위해 마련됐다. 진로·진학자료실, 독서상담실, 토론실 등으로 구성됐다. 청소년 도서 7000여권, 간행물 28종, 대학별 입학 안내서 150여종, 검인정 교과서 458종 등을 비치하고 데스크톱 컴퓨터 6대를 설치했다.경기 용인 수지도서관(관장 신현국)은 오는 10일 낮 12시부터 ‘러시아문학, 연극과 만나다’ 행사를 연다. ‘길 위의 인문학’ 3차 프로그램인 이번 행사는 강연과 탐방으로 구성된다. 러시아 문학의 황금기인 19세기 대문호들의 삶 및 작품에 대한 강연과 서울 대학로 극장을 찾아 안톤 체호프의 대표 희곡 ‘갈매기’를 감상하는 시간으로 진행된다. ‘로쟈의 인문학 서재’의 저자 이현우 씨가 도서관 2층 시청각실에서 강연을 하고 탐방을 이끈다. 참가 대상은 19세 이상 일반인 40명으로 도서관 홈페이지(www.yonginlib.go.kr)에서 신청하면 된다. 참가비는 무료.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "다량의 검인정 교과서를 갖춘 도서관<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "서울교육청 정독도서관은 다량의 검인정 교과서를 갖춘 도서관으로, 최근 청소년 특화 전문도서관을 개관했습니다. 이 도서관은 일반 중·고교생뿐만 아니라 정규학업을 중단하고 검정고시 등을 준비하는 '학교 밖 청소년'을 위해 마련되었습니다. 청소년관은 진로·진학자료실, 독서상담실, 토론실 등으로 구성되어 있으며, 청소년 도서 7000여 권, 간행물 28종, 대학별 입학 안내서 150여 종, 검인정 교과서 458종 등을 비치하고 있습니다. 또한, 데스크톱 컴퓨터 6대도 설치되어 있어 다양한 학습 자료를 제공하고 있습니다[[ref5]].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6edd1-951a-4950-be9a-8d2096d2752d",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad8216d-baaa-4aaa-b172-ca992c9f4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395e400-be31-4e70-afaa-b90a0573cc0a",
   "metadata": {},
   "source": [
    "- lora_alpha: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "- lora_dropout: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. 0.1로 설정하면 학습 중 10%의 뉴런이 비활성화.\n",
    "- r: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "- bias: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. \"none\"으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. \"all\" 또는 \"lora_only\"와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "- target_modules: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 \"q_proj\"와 \"v_proj\"를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "- task_type: LoRA가 적용되는 작업 유형을 지정합니다. \"CAUSAL_LM\"은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 \"SEQ2SEQ_LM\"(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe8e9bc-92e7-41ae-b03b-253357969160",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"llama-3-8b-rag-ko\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ff34d-0256-4a1a-a1d6-728c6a74097f",
   "metadata": {},
   "source": [
    "- `output_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\r\n",
    "- `num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니.\r\n",
    "\r\n",
    "- `per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있니다.\r\n",
    "\r\n",
    "- `gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 `2`로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 용합니다.\r\n",
    "\r\n",
    "- `gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질수 있습니다.\r\n",
    "\r\n",
    "- `optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적기를 사용합니다.\r\n",
    "\r\n",
    "- `logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마 로그를 기록합니다.\r\n",
    "\r\n",
    "- `save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스마다 모델이 저장됩니다.\r\n",
    "\r\n",
    "- `save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50스텝마다 모델을 저장합니다.\r\n",
    "\r\n",
    "- `bf16`: `bfloat16` 정밀도를 사용하도록 설정합니다. `bfloat16`은 FP32와 유사한 범위를 제공하면서 모리와 계산 효율성을 높입니다.\r\n",
    "\r\n",
    "- `learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 용하여 안정적인 학습을 유도합니다.\r\n",
    "\r\n",
    "- `max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 정하여 폭발적 그래디언트를 방지합니다.\r\n",
    "\r\n",
    "- `warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다 학습의 안정성을 높이기 위해 사용됩니다.\r\n",
    "\r\n",
    "- `lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"costant\"`는 학습률을 일정하게 유지합니다.\r\n",
    "\r\n",
    "- `push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니. `False`로 설정하면 업로드하지 않습니다.\r\n",
    "\r\n",
    "- `remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다.`True`로 설정하면 메모리를 절약할 수 있습니다.\r\n",
    "\r\n",
    "- `dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset True`로 설정하여 데이터셋 준비 단계를 건너뜁니다.\r\n",
    "\r\n",
    "- `report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f0a2f-639b-4b89-852a-7a0bbcbe1203",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9526becf-24c6-49b9-b503-75f31f64ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for example in batch:\n",
    "        messages = example[\"messages\"]\n",
    "\n",
    "        # LLaMA 3 채팅 템플릿 적용 (시작 토큰 포함)\n",
    "        prompt = \"<|begin_of_text|>\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"].strip()\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "\n",
    "        # 마지막 assistant 메시지는 응답으로 간주하고 레이블에 포함\n",
    "        text = prompt.strip()\n",
    "\n",
    "        # 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        # assistant 응답의 시작 위치 찾기\n",
    "        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n",
    "\n",
    "        # 레이블 범위 지정\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_tokens):\n",
    "            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n",
    "                start = i + len(assistant_tokens)\n",
    "                end = start\n",
    "                while end <= len(input_ids) - len(eot_tokens):\n",
    "                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start, end):\n",
    "                    labels[j] = input_ids[j]\n",
    "                for j in range(end, end + len(eot_tokens)):\n",
    "                    labels[j] = input_ids[j]  # <|eot_id|> 토큰도 포함\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "\n",
    "    # 패딩 처리\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        pad_len = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n",
    "        new_batch[\"labels\"][i].extend([-100] * pad_len)\n",
    "\n",
    "    for k in new_batch:\n",
    "        new_batch[k] = torch.tensor(new_batch[k])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ea71f-f642-4aed-ba4d-d142535571c4",
   "metadata": {},
   "source": [
    "- 라마 챗 템플릿\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\r\n",
    "\r\n",
    "You are a helpful AI assistant for travel tips and recommendations.<|eot_id|><|start_header_id|>user<|end_header_id|>\r\n",
    "\r\n",
    "What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f337c79-37a9-4f68-8f4d-586a687422e4",
   "metadata": {},
   "source": [
    "collate_fn(batch) 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 attention_mask를 생성합니다. 이후 assistant 답변 부분을 찾아 해당 범위에 레이블을 설정합니다. 이 범위를 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다.\n",
    "\n",
    "최종적으로, 배치 내 모든 샘플의 길이를 동일하게 맞추기 위해 패딩 작업을 수행합니다. 이 과정에서 입력 데이터에는 패딩 토큰 ID를 추가하고, 어텐션 마스크에는 0을 추가하며, 레이블에는 -100을 추가합니다. 모든 데이터는 PyTorch 텐서로 변환되어 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "397d4044-d709-4492-ab4b-63ed3ec11d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 3955])\n",
      "어텐션 마스크 형태: torch.Size([1, 3955])\n",
      "레이블 형태: torch.Size([1, 3955])\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이\n",
    "max_seq_length=8192\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "633a4b4e-6192-4231-a33e-3e1b35cc06fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[128000, 128006, 9125, 128007, 198, 65895, 83628, 34804, 115036, 99901, 18918, 82818, 120378, 43139, 109760, 19954, 111964, 110513, 109670, 382, 13447, 49531, 21028, 67890, 30426, 115790, 18359, 103386, 100968, 119978, 627, 16, 13, 109760, 54780, 115036, 99901, 18918, 82818, 120378, 43139, 111964, 16582, 119978, 627, 17, 13, 115036, 99901, 19954, 108838, 109842, 18359, 111964, 16582, 113348, 117193, 96677, 119978, 627, 18, 13, 109760, 19954, 102597, 108386, 13094, 115036, 99901, 19954, 47782, 115300, 115036, 99901, 102772, 330, 34983, 65895, 109760, 93, 19954, 102597, 109842, 13094, 120078, 1210, 103959, 35495, 111964, 16582, 119978, 627, 19, 13, 111964, 48936, 54718, 103966, 30381, 117294, 18918, 119884, 83290, 54535, 41953, 108520, 54535, 101353, 18359, 114839, 101528, 33390, 107333, 19954, 102722, 102657, 16969, 23955, 101711, 84734, 17835, 95713, 117294, 85721, 48424, 18918, 102484, 21121, 119978, 13, 96717, 18918, 105510, 27796, 103966, 30381, 54535, 41953, 106593, 54535, 101353, 18359, 220, 16, 43144, 117294, 57575, 59777, 27797, 101528, 33390, 107333, 19954, 4416, 1116, 16, 5163, 110917, 55216, 58232, 16582, 119978, 627, 20, 13, 96717, 18918, 105510, 27796, 103966, 30381, 54535, 41953, 106593, 54535, 101353, 18359, 220, 16, 43144, 117294, 81673, 220, 20, 43144, 117294, 57575, 101604, 118472, 59777, 27797, 101528, 33390, 107333, 19954, 4416, 1116, 16, 21128, 4416, 1116, 20, 5163, 110917, 55216, 58232, 16582, 119978, 627, 21, 13, 82273, 113760, 50467, 125546, 117294, 18918, 59777, 27797, 83290, 111964, 16582, 119978, 382, 109070, 78326, 99901, 512, 35864, 52688, 27796, 16, 25, 106010, 108772, 112266, 19954, 65677, 29102, 107198, 34804, 106010, 107434, 102039, 37155, 104841, 49085, 125626, 34804, 39623, 249, 113187, 103521, 123402, 55925, 106687, 103738, 101103, 107094, 103211, 13, 101703, 125626, 13094, 64432, 101136, 26799, 124146, 108533, 35495, 65621, 39623, 249, 107372, 35495, 103521, 101438, 34804, 106917, 113626, 58232, 63171, 17, 48424, 17835, 220, 7285, 23, 100392, 103521, 102365, 33943, 112039, 13, 107372, 35495, 20565, 220, 4468, 21, 100392, 106010, 122851, 58189, 43139, 119444, 116429, 23955, 21819, 105, 34983, 101703, 125626, 43139, 103886, 120591, 58901, 54535, 18359, 105069, 100904, 13, 74623, 101106, 54718, 103551, 113187, 125308, 115096, 49508, 64254, 123366, 37155, 55421, 43139, 8790, 111345, 101607, 101012, 243, 19954, 108772, 112266, 56773, 101607, 54780, 56773, 104449, 105164, 41953, 120861, 103738, 103778, 93851, 104176, 108583, 49085, 65677, 55430, 107364, 16969, 104167, 44690, 20565, 73444, 112039, 13, 110187, 117559, 23955, 109017, 18359, 58935, 108609, 18359, 105131, 102436, 24486, 102027, 44690, 17835, 123020, 44005, 111436, 13447, 13, 54059, 64254, 123366, 103521, 101438, 103807, 101106, 54780, 102519, 106138, 32179, 37155, 114039, 17835, 104670, 116023, 86351, 37155, 104841, 49085, 125626, 34804, 101703, 125626, 107065, 113295, 121048, 55170, 108860, 56154, 109065, 17835, 110606, 121, 101443, 234, 13447, 13, 101703, 125626, 18359, 107364, 34804, 102745, 104210, 75908, 113760, 117313, 81673, 3396, 122, 234, 82068, 24486, 118909, 19954, 115418, 103272, 13447, 13, 102027, 118135, 220, 1135, 73653, 103131, 13094, 112633, 35495, 113531, 122620, 19954, 65621, 106010, 107434, 104706, 101438, 101106, 102772, 101003, 101438, 220, 16, 73653, 3101, 15, 58126, 101838, 13094, 91786, 13, 113187, 103521, 123402, 101703, 125626, 43139, 118667, 117313, 101272, 54780, 105069, 101856, 101272, 13094, 106354, 49085, 125626, 19954, 75086, 34983, 116231, 101, 103897, 105, 66653, 241, 13447, 13, 30381, 104841, 49085, 125626, 34804, 119929, 106010, 107434, 102039, 13094, 111809, 44005, 101703, 125626, 103966, 33931, 57390, 115888, 57575, 3451, 102039, 117546, 122169, 101703, 125626, 529, 43139, 119058, 33943, 112039, 13, 107758, 43139, 220, 18, 100392, 113401, 72043, 14260, 35495, 100848, 101999, 54780, 27796, 19954, 116951, 16969, 101228, 102546, 54780, 102464, 17835, 113037, 106434, 117313, 11, 103966, 33931, 57390, 35495, 112280, 102244, 126168, 107545, 101096, 106434, 117313, 120908, 104441, 101711, 104182, 59877, 71682, 48936, 126088, 101568, 13, 74177, 16969, 220, 24, 100551, 58083, 101555, 112325, 107004, 43139, 66610, 33931, 107205, 105519, 117546, 101106, 108733, 117313, 101272, 54780, 106999, 107712, 27796, 101665, 103778, 106593, 102258, 21028, 18918, 96102, 29833, 65621, 126060, 49085, 96677, 103304, 112931, 13, 101139, 119868, 101106, 34804, 103686, 41953, 34983, 120122, 101353, 82001, 106359, 26799, 75908, 123926, 121056, 109883, 119623, 101568, 13, 30381, 104841, 49085, 125626, 34804, 121066, 220, 20, 123096, 106010, 30426, 96318, 101254, 100848, 57575, 110003, 24486, 112280, 18359, 116464, 43139, 107712, 27796, 119193, 102835, 107434, 18359, 107973, 91786, 13, 102155, 33931, 115072, 114450, 55570, 34804, 1054, 111291, 102823, 123851, 103430, 44005, 62060, 44966, 119193, 102835, 21028, 108523, 107198, 13094, 103135, 48936, 18359, 55000, 21121, 106958, 96677, 103304, 24486, 109194, 113052, 863, 103292, 101203, 1054, 54059, 102436, 34804, 127127, 77535, 13094, 220, 966, 80732, 111297, 102077, 126253, 56154, 109065, 17835, 118667, 105642, 101703, 125626, 43139, 103686, 86157, 101360, 107719, 13447, 863, 35495, 108537, 627, 35864, 52688, 27796, 17, 25, 106010, 49085, 125626, 34804, 106788, 104684, 220, 19, 33177, 102704, 220, 16, 102156, 55216, 107656, 66965, 30426, 101272, 57575, 3451, 41953, 27796, 102260, 7, 51466, 102181, 95544, 112574, 110572, 11, 110080, 54780, 102745, 65950, 25, 102484, 121321, 86157, 103504, 103079, 57390, 102027, 27796, 102260, 66965, 529, 18359, 78453, 13447, 13, 101228, 41953, 110257, 104683, 77437, 126344, 110080, 21028, 102027, 77437, 32428, 102027, 27796, 102260, 16969, 96717, 102835, 111490, 36609, 57139, 67525, 106958, 81021, 81673, 121612, 18359, 66610, 100660, 34983, 106478, 57390, 17835, 114699, 44005, 50152, 20565, 104038, 13447, 13, 117717, 57519, 30426, 102772, 101254, 34804, 11, 96270, 49085, 102335, 11, 37155, 48424, 104303, 11, 114226, 101090, 78102, 101003, 80732, 69332, 20565, 106001, 101703, 27796, 81673, 69332, 20565, 21028, 124883, 54780, 110614, 106647, 110038, 34804, 106478, 57390, 20565, 102484, 121321, 86157, 21028, 102027, 27796, 102260, 220, 2491, 102757, 13094, 57519, 30426, 112931, 13, 100654, 102365, 101711, 110669, 49085, 125626, 54780, 106010, 67945, 125199, 49085, 125626, 34804, 62398, 84391, 101577, 55430, 3451, 26799, 60798, 102233, 103655, 7, 103087, 71005, 33035, 124140, 8, 529, 21028, 57519, 26799, 107011, 110514, 18918, 94821, 101528, 13, 63171, 108668, 100508, 21028, 101999, 54780, 27796, 17835, 102786, 104374, 65677, 60798, 102233, 103655, 34804, 3451, 122432, 103948, 529, 3451, 56154, 21121, 529, 81673, 106999, 101604, 101927, 220, 18, 67945, 119229, 27796, 17835, 110606, 121, 101443, 234, 13447, 13, 112822, 118747, 33229, 100711, 104176, 13094, 123080, 7, 107937, 101257, 8, 45618, 67945, 103551, 74177, 67945, 7, 76208, 31640, 8, 102704, 220, 9795, 17, 100392, 63375, 21028, 103135, 111636, 105613, 100392, 50643, 17835, 112482, 24486, 112780, 102681, 56154, 13447, 13, 62398, 84391, 103079, 34804, 109969, 101711, 104684, 125199, 67945, 104167, 103415, 100848, 123503, 220, 679, 15, 100392, 57519, 220, 843, 103131, 43139, 107123, 63375, 24486, 99458, 13094, 107011, 18359, 57519, 26799, 107011, 43139, 63207, 82776, 108137, 13, 102160, 102365, 101711, 110669, 49085, 125626, 19954, 101151, 101703, 125626, 105069, 101856, 62841, 55421, 102823, 93851, 96318, 6812, 61415, 106891, 105551, 119357, 18359, 110155, 105069, 101856, 48936, 29833, 91786, 13, 106010, 67945, 125199, 49085, 125626, 19954, 101151, 101228, 102130, 116567, 81673, 101999, 102436, 55421, 11, 102888, 111291, 54780, 125199, 49085, 125626, 101604, 52688, 62841, 55421, 102823, 106359, 48936, 29833, 91786, 13, 112037, 103825, 63171, 101584, 30426, 102365, 49085, 125626, 34804, 220, 717, 116678, 3451, 102039, 116942, 48424, 29833, 116194, 220, 966, 55430, 100392, 105429, 529, 18359, 220, 16, 102156, 72115, 71682, 19954, 57519, 30426, 52976, 13, 29833, 116194, 220, 966, 55430, 100392, 105429, 34804, 105519, 116942, 48424, 101738, 54780, 106999, 101139, 41381, 61394, 86351, 105178, 21028, 106612, 87472, 18918, 127501, 57390, 24486, 108158, 109811, 106612, 77437, 43139, 114699, 33943, 238, 104429, 46230, 58260, 244, 98, 11, 120122, 11, 116892, 116844, 17835, 114702, 33308, 120, 91786, 13, 117717, 102027, 21121, 57519, 117977, 29833, 116194, 53400, 105519, 116942, 21028, 220, 966, 100392, 57519, 113952, 18359, 102066, 22035, 104352, 44005, 45618, 101607, 102823, 63171, 101584, 21028, 122041, 108381, 86351, 103135, 111636, 103886, 120591, 58901, 121712, 44005, 95303, 106647, 56773, 21121, 106958, 55216, 107656, 33943, 112039, 13, 27796, 67945, 52688, 89359, 102365, 13094, 86351, 54059, 21121, 111281, 49085, 125626, 34804, 74177, 16969, 220, 806, 100551, 717, 33177, 102704, 102293, 55430, 103504, 114020, 105519, 101930, 41953, 105851, 123486, 107472, 107712, 27796, 101204, 110357, 18359, 107065, 52976, 13, 220, 605, 62841, 19954, 105701, 110218, 107065, 107205, 117717, 113052, 34804, 102160, 102365, 101711, 110669, 49085, 125626, 57575, 56773, 101106, 44005, 3451, 41953, 105851, 32428, 107712, 27796, 101204, 110357, 100994, 101555, 529, 19954, 101585, 30381, 53400, 115888, 43139, 127290, 102517, 110080, 18359, 101585, 30381, 101360, 29833, 57390, 102233, 101577, 56154, 81673, 106999, 110080, 118151, 106647, 111809, 52976, 13, 105519, 101930, 41953, 105851, 112215, 33390, 123621, 61415, 112830, 17835, 23955, 113052, 19954, 116768, 48936, 29833, 117097, 11, 101703, 125626, 19954, 127900, 16582, 109745, 23955, 85767, 33177, 7166, 1656, 876, 10196, 31, 784, 95966, 25268, 68993, 8, 17835, 111529, 108302, 119068, 627, 35864, 52688, 27796, 18, 25, 106010, 33229, 102436, 58189, 106010, 107434, 102039, 32179, 119868, 49085, 125626, 34804, 104350, 59134, 127240, 112521, 93292, 46810, 106647, 24839, 103, 100904, 13, 115762, 58232, 102039, 13094, 107067, 125160, 58935, 86351, 101360, 65621, 33229, 102436, 101353, 7, 56154, 82068, 63171, 7994, 48424, 8, 109969, 101577, 19954, 101703, 125626, 13094, 65677, 29102, 107198, 35495, 127887, 102563, 13, 33229, 102436, 101353, 109969, 101577, 34804, 109916, 101228, 101314, 126276, 13094, 21121, 109644, 105813, 67945, 78453, 115096, 107746, 32428, 88525, 51796, 91040, 3396, 104, 241, 108381, 106223, 116492, 107675, 13, 35243, 45780, 52375, 101709, 120380, 82273, 103719, 21028, 101139, 119868, 49085, 125626, 18359, 67890, 115061, 90759, 62398, 105453, 84618, 103778, 13094, 108499, 54059, 115762, 58232, 102039, 34804, 101139, 119868, 49085, 125626, 18359, 107067, 55421, 119623, 57575, 63171, 104065, 44005, 113866, 43139, 37155, 29102, 101528, 13, 220, 1927, 100392, 63375, 3451, 32179, 119868, 81673, 106999, 108859, 101412, 54542, 18918, 105069, 32179, 107368, 101703, 125626, 529, 18359, 103504, 102260, 17835, 101817, 115467, 106999, 102132, 41953, 101360, 97096, 66965, 108859, 65677, 106064, 67890, 115061, 102837, 57519, 102233, 54780, 59134, 112932, 18359, 127507, 107094, 34804, 108137, 13, 32179, 119868, 49085, 125626, 34804, 220, 4468, 24, 100392, 220, 20, 100551, 3451, 42529, 101015, 101139, 119868, 21028, 61816, 529, 18918, 55216, 111281, 34983, 74623, 101106, 101528, 13, 101703, 125626, 13094, 105510, 101151, 103521, 101438, 34804, 119444, 102704, 45618, 102365, 32179, 119868, 105297, 55421, 43139, 108533, 58126, 101139, 119868, 81673, 106999, 24486, 103135, 114333, 101413, 232, 13447, 13, 13094, 101703, 125626, 34804, 101139, 119868, 66965, 52688, 101703, 125626, 109659, 58901, 49508, 58189, 49085, 27796, 19954, 103966, 57390, 33308, 120, 91786, 13, 64432, 101314, 102027, 118135, 220, 1544, 73653, 2636, 15, 103131, 43139, 11, 220, 1954, 4, 20565, 104690, 13094, 49508, 58189, 49085, 27796, 13447, 13, 109666, 106869, 101090, 114450, 55570, 34804, 1054, 16, 116899, 59877, 44966, 44005, 110080, 73653, 220, 17, 73653, 1049, 15, 58126, 103131, 863, 103292, 101203, 1054, 100654, 96318, 57575, 102722, 63375, 107205, 49508, 58189, 118135, 127002, 101228, 41953, 101360, 91786, 863, 35495, 114942, 101528, 13, 13094, 27797, 110257, 116681, 105115, 101703, 27796, 46810, 55430, 17835, 103886, 110080, 13094, 105510, 125301, 102293, 55430, 29833, 114020, 108733, 102745, 65950, 17835, 108772, 103684, 13447, 13, 74177, 105501, 106243, 39250, 44690, 52688, 13094, 117887, 22035, 104448, 107372, 101254, 101927, 11, 102132, 101963, 78102, 116992, 103131, 121048, 106359, 108181, 113424, 102423, 13447, 13, 106354, 106359, 26799, 103807, 19954, 101003, 54059, 107434, 106024, 26799, 126303, 103123, 112542, 111270, 127466, 13447, 13, 123106, 103126, 103931, 62060, 71023, 101703, 118135, 220, 1049, 15, 103131, 18359, 116231, 234, 116115, 235, 112633, 121969, 13, 50467, 113626, 49085, 27796, 101272, 102772, 107715, 11, 112780, 11, 127385, 112542, 78102, 220, 22, 60861, 100654, 116918, 17835, 114699, 24486, 49508, 58189, 49085, 27796, 18918, 110773, 29833, 91786, 13, 56154, 27796, 220, 972, 80732, 13094, 63207, 118162, 109969, 41953, 49085, 27796, 122805, 34804, 101139, 119868, 49085, 125626, 21028, 65677, 102581, 101568, 13, 220, 20, 100551, 3451, 20565, 30381, 21028, 104685, 529, 54780, 84618, 64254, 14260, 108381, 102275, 101482, 100508, 78102, 101852, 103213, 109065, 19954, 105701, 110218, 101412, 114039, 100508, 49508, 58189, 121612, 107011, 54780, 102533, 100392, 102517, 101703, 27796, 78102, 220, 1272, 103131, 18359, 109336, 52976, 13, 122805, 18359, 117870, 48936, 54718, 20565, 98243, 33390, 106010, 30426, 96318, 84415, 112070, 16969, 103738, 103778, 102533, 64189, 101555, 123103, 1054, 105198, 38187, 168, 107, 97, 116951, 106833, 106888, 863, 16969, 109760, 13094, 125906, 253, 54059, 86351, 13447, 13, 102533, 21121, 119596, 103405, 85721, 105069, 104374, 107712, 27796, 102249, 86351, 116986, 16969, 101139, 119868, 49085, 125626, 54780, 103135, 111636, 109583, 44005, 104652, 56154, 17835, 11, 101999, 41953, 109336, 18359, 84696, 34804, 101139, 119868, 102823, 55170, 32428, 13447, 13, 107712, 27796, 103655, 57002, 62841, 11, 101604, 57390, 89359, 101347, 11, 107712, 27796, 103655, 57002, 52688, 11, 107285, 125879, 21121, 104652, 56154, 78102, 13094, 105069, 102423, 13447, 13, 102293, 55430, 104689, 114020, 102772, 118696, 115762, 14260, 50643, 102005, 104652, 56154, 81673, 113052, 13094, 107065, 112931, 13, 49085, 125626, 34804, 119929, 115762, 101106, 220, 16, 102156, 57519, 30426, 101272, 18359, 74623, 93917, 34983, 3451, 50643, 102005, 58189, 57390, 100711, 18359, 529, 18359, 66610, 33931, 101528, 13, 62060, 102193, 80307, 45780, 223, 105, 102423, 19954, 101817, 102823, 101604, 57390, 74769, 66406, 54780, 106999, 116951, 58901, 61816, 101604, 57390, 105220, 56773, 32428, 79225, 13094, 107382, 122723, 234, 18359, 108683, 13447, 13, 102293, 55430, 29833, 114020, 103123, 50643, 119525, 113798, 116464, 43139, 113052, 18359, 111809, 52976, 13, 106788, 104684, 103551, 16969, 74623, 102517, 104182, 49085, 106906, 102005, 48936, 29833, 91786, 13, 101971, 33177, 124467, 101003, 54059, 101272, 102772, 96102, 111341, 20565, 102407, 101103, 119087, 101604, 57390, 89359, 101347, 113052, 13094, 111809, 112931, 13, 112032, 114450, 55570, 34804, 1054, 103415, 123194, 101703, 125626, 18359, 107364, 101760, 101954, 101139, 119868, 102823, 113857, 16969, 86503, 101555, 20565, 65905, 120, 101817, 125508, 107144, 35495, 115115, 125301, 105701, 110773, 54718, 33390, 36609, 120534, 13094, 5251, 255, 231, 108661, 108907, 863, 101203, 1054, 114714, 115467, 86503, 101555, 20565, 106999, 116768, 34983, 100994, 103655, 18359, 74618, 105940, 234, 29833, 65621, 113052, 18359, 121056, 109883, 119623, 863, 110917, 108537, 627, 35864, 52688, 27796, 19, 25, 220, 12819, 58126, 123590, 102722, 103079, 56154, 81673, 59777, 122075, 226, 127302, 20565, 39250, 55430, 34983, 65621, 56069, 55430, 71023, 103079, 49085, 30426, 62398, 20565, 127230, 19954, 65677, 29102, 24486, 49508, 109324, 71023, 103079, 113626, 105922, 110816, 81673, 100027, 54289, 16582, 114061, 3451, 22035, 22035, 104762, 7, 112810, 55030, 109546, 243, 8, 24535, 67890, 22035, 104762, 72115, 71682, 19954, 105510, 27796, 33390, 55216, 125222, 54780, 127079, 18359, 36609, 108185, 104965, 94772, 110080, 114784, 125921, 105195, 19954, 5251, 251, 20541, 13, 103521, 101438, 13094, 121365, 53400, 102681, 17835, 18918, 118652, 102722, 103079, 113626, 105922, 110816, 17835, 105510, 27796, 33390, 127992, 110670, 90960, 20565, 20565, 127900, 108583, 18359, 107625, 13094, 52976, 13, 220, 6549, 21, 105565, 19954, 23955, 113562, 126060, 21028, 127079, 119596, 110080, 115096, 115426, 34983, 110080, 18359, 104965, 103430, 121942, 100904, 13, 108499, 13094, 220, 21, 76, 20565, 112633, 16969, 110080, 115096, 126515, 118176, 61415, 110187, 110080, 13094, 114417, 224, 113048, 65621, 22035, 36609, 15478, 254, 67525, 123851, 102275, 111297, 13447, 13, 23955, 126060, 34804, 67236, 104684, 220, 777, 33177, 54535, 18359, 84618, 16969, 3451, 55055, 102423, 101703, 125626, 12, 22035, 111138, 21028, 70292, 110, 529, 101568, 13, 102722, 103079, 49085, 30426, 113626, 58232, 101353, 7, 13094, 56154, 41953, 102155, 105198, 48424, 14260, 24486, 106103, 56154, 116865, 125543, 121066, 34983, 220, 20, 100551, 58952, 102365, 19954, 122787, 24140, 34983, 220, 16, 100392, 63207, 19954, 74623, 101106, 18359, 107758, 103097, 35495, 91786, 13, 3451, 22035, 111138, 21028, 70292, 110, 529, 34804, 102888, 101353, 13094, 110714, 102722, 103079, 56154, 81673, 67890, 77437, 32428, 11, 102533, 113798, 75086, 117998, 34983, 118696, 102745, 65950, 123103, 55216, 102249, 107094, 34804, 101703, 27796, 17835, 8790, 111345, 101607, 57519, 33390, 74623, 20565, 77437, 101703, 125626, 101568, 13, 74623, 102517, 90960, 58232, 106001, 101429, 113760, 104441, 100660, 32428, 113375, 230, 13, 112780, 107715, 62060, 73653, 78102, 121048, 110080, 18359, 121599, 114881, 13, 56069, 55430, 71023, 103079, 49085, 30426, 18918, 107364, 34804, 126284, 51440, 33390, 123621, 61415, 63171, 24486, 116469, 106359, 48936, 29833, 91786, 13, 220, 1041, 73653, 103131, 18359, 55216, 102249, 107094, 16969, 105453, 103504, 102260, 116100, 115888, 18359, 94821, 34983, 111530, 220, 1135, 73653, 103131, 13094, 103686, 42771, 53400, 117132, 13, 220, 16, 101353, 101015, 17835, 220, 508, 73653, 103131, 21028, 110080, 13094, 90960, 20565, 19954, 114417, 224, 113048, 91786, 13, 110080, 18359, 104019, 44005, 33229, 27796, 20565, 47782, 35495, 64432, 102233, 21028, 101703, 125626, 107335, 106906, 101015, 104182, 119670, 88525, 51796, 54059, 50467, 44690, 48918, 44690, 118768, 118696, 127290, 21028, 110080, 13094, 62398, 105940, 230, 19954, 105510, 108807, 63207, 118009, 66653, 238, 89641, 235, 108907, 13, 3451, 22035, 111138, 21028, 70292, 110, 529, 101703, 125626, 21028, 107120, 110670, 103966, 112932, 34804, 102722, 103079, 56154, 81673, 55216, 102249, 26799, 102517, 90960, 20565, 13447, 13, 110080, 18359, 55216, 102249, 24486, 102722, 103079, 56154, 90960, 123740, 107364, 91040, 102722, 103079, 114333, 55925, 124663, 38295, 116, 110080, 18359, 109580, 63207, 106223, 29833, 91786, 13, 62060, 102193, 90960, 101838, 18359, 115115, 49085, 106354, 101703, 125626, 107335, 127290, 102517, 17835, 119670, 53400, 110080, 73653, 110773, 29833, 36439, 102077, 11, 23955, 109017, 19954, 101151, 102722, 103079, 114333, 112700, 112521, 100508, 43139, 110080, 18359, 118667, 127441, 120279, 62398, 105940, 230, 19954, 110773, 29833, 91786, 13, 110080, 18359, 117004, 44005, 23955, 102823, 51440, 33390, 107138, 49531, 56154, 110572, 52688, 100508, 66965, 102201, 11, 62398, 106103, 55925, 101164, 101691, 103825, 25941, 105718, 57519, 102201, 13094, 109580, 114417, 224, 101443, 234, 90960, 20565, 19954, 64857, 48936, 29833, 39277, 122226, 118318, 13, 110187, 102533, 26799, 102823, 101585, 112722, 119, 55216, 102249, 24486, 110080, 18359, 126515, 62398, 106024, 108181, 55925, 124663, 112700, 110080, 18359, 118151, 104429, 121213, 102621, 120279, 67890, 77437, 21028, 23955, 29854, 27796, 18918, 64432, 16969, 102888, 57139, 18918, 122723, 226, 29833, 91786, 13, 59777, 99029, 21028, 107120, 46810, 113760, 123323, 14260, 113626, 101003, 86157, 32428, 99458, 13094, 107011, 13094, 52072, 64189, 17835, 87931, 101103, 107054, 105701, 96270, 101109, 101154, 103430, 108859, 110080, 58083, 125166, 108661, 107004, 125308, 54780, 107712, 27796, 122594, 106249, 33931, 117216, 63171, 106646, 34983, 102837, 102155, 23955, 56154, 124788, 1054, 24140, 40011, 236, 34804, 110080, 13094, 125906, 253, 54059, 22035, 104448, 107712, 26799, 114026, 104965, 118151, 101709, 113037, 104352, 24486, 104965, 118562, 21121, 107205, 96270, 101109, 101154, 94772, 116492, 13094, 122292, 43144, 108907, 863, 101203, 1054, 13094, 57139, 74618, 102837, 110080, 103292, 55000, 102913, 114038, 109584, 101711, 88525, 110661, 115300, 105695, 102244, 97096, 66965, 34804, 118318, 863, 35495, 102258, 93917, 101528, 13, 108154, 121856, 1054, 14806, 123143, 101852, 67945, 20565, 110080, 18359, 104670, 118151, 22035, 110661, 103055, 114542, 107744, 57002, 101834, 101103, 34983, 114881, 863, 101203, 1054, 66965, 52688, 118135, 103738, 103778, 101999, 101927, 27796, 18918, 103405, 102268, 116253, 54596, 108, 110080, 13094, 102745, 114026, 110979, 102519, 108280, 104152, 102893, 50467, 20565, 102546, 29833, 123644, 108098, 100904, 863, 35495, 114942, 101528, 13, 101703, 125626, 107065, 34804, 33229, 27796, 62060, 83628, 220, 966, 58126, 123286, 109969, 104841, 56154, 7, 53508, 116, 121103, 65743, 8, 102823, 127088, 121969, 13, 110080, 18359, 124827, 101360, 107712, 27796, 18918, 109969, 44005, 65677, 55421, 107801, 56154, 26799, 13447, 13, 109969, 104841, 56154, 109194, 18359, 110038, 65895, 44005, 85721, 101577, 20565, 104293, 102757, 33177, 117264, 16969, 1054, 67945, 73653, 101254, 121321, 104706, 101438, 101106, 57575, 108785, 64254, 24486, 103213, 103338, 21028, 96102, 54059, 117732, 20565, 101139, 102423, 112280, 114026, 117403, 112542, 52688, 110614, 106647, 102407, 101103, 119087, 107387, 104414, 103607, 13094, 125226, 55925, 101968, 13094, 103153, 67945, 106287, 21028, 117403, 112542, 52688, 102533, 26799, 102563, 16969, 112024, 18359, 115877, 115418, 39519, 112039, 863, 101203, 1054, 22035, 118156, 126653, 117559, 19097, 123143, 13094, 114026, 107712, 27796, 18918, 102681, 24486, 24814, 236, 21028, 55216, 113222, 101, 18359, 57519, 104684, 44005, 105512, 23955, 101703, 125626, 21028, 103504, 82068, 863, 110917, 114942, 101528, 13, 3451, 22035, 111138, 21028, 70292, 110, 529, 21028, 67890, 22035, 104762, 72115, 71682, 90960, 107368, 220, 1187, 108076, 105069, 101856, 48936, 29833, 117097, 106313, 101532, 105069, 101856, 115483, 121255, 103686, 67945, 48936, 75908, 108308, 101568, 13, 320, 18887, 8, 25875, 12, 8504, 15, 198, 35864, 52688, 27796, 20, 25, 102160, 102365, 101711, 110669, 49085, 125626, 34804, 3451, 93917, 101151, 54780, 105519, 93917, 7, 162, 34026, 103293, 8, 54535, 115666, 63207, 101963, 529, 113781, 56773, 38187, 17835, 74177, 16969, 220, 717, 100551, 966, 33177, 102704, 104414, 101106, 220, 21, 102156, 101254, 66965, 94772, 101090, 101272, 57575, 101254, 52688, 116135, 57519, 30426, 62841, 18918, 78453, 13447, 13, 120244, 102326, 100508, 26799, 109666, 67945, 27797, 13094, 107744, 55430, 101585, 71682, 115467, 56773, 35495, 107094, 34804, 76628, 102997, 101824, 105613, 122877, 29833, 50764, 53400, 3451, 102997, 116135, 27796, 20182, 23955, 109814, 100981, 101003, 108185, 79225, 104293, 38187, 20565, 23955, 27796, 89359, 21028, 45618, 18918, 55170, 54059, 117964, 34804, 3451, 24486, 108583, 101868, 101347, 102201, 20182, 102155, 30381, 105204, 20565, 105519, 93917, 57519, 101930, 20565, 21028, 59777, 41953, 102201, 104683, 22035, 19954, 101971, 18359, 120657, 112, 3451, 33177, 102080, 86157, 101482, 32428, 50764, 529, 78102, 220, 914, 102757, 220, 9423, 107011, 21028, 101254, 52688, 116135, 18359, 57519, 30426, 52976, 13, 102160, 102365, 101711, 110669, 49085, 125626, 116680, 112953, 1054, 93917, 101151, 54780, 105519, 21028, 54535, 80732, 101999, 99029, 111636, 66610, 80732, 44005, 65677, 29102, 863, 105771, 114942, 101528, 13, 123154, 24486, 57519, 30426, 122805, 34804, 101703, 125626, 125180, 7, 2185, 31607, 18487, 68993, 112574, 3451, 44690, 102233, 14260, 113825, 58126, 52118, 66965, 30426, 101066, 56154, 529, 57575, 74959, 48936, 29833, 91786, 13, 115978, 107434, 102039, 37155, 104841, 49085, 125626, 13094, 121066, 104684, 220, 966, 33177, 100994, 79225, 49085, 125626, 72043, 114489, 43139, 105519, 117546, 103966, 57390, 116425, 49085, 125626, 18359, 74623, 101106, 101528, 13, 101703, 125626, 67236, 220, 19166, 13, 20, 105565, 111850, 101555, 17835, 66610, 33931, 53400, 105519, 117546, 101106, 34804, 106354, 72043, 14260, 35495, 100848, 77535, 54780, 127005, 100508, 101096, 18359, 72043, 101353, 101360, 86422, 30381, 35495, 30426, 120908, 119225, 44005, 3451, 103583, 22817, 244, 105519, 117546, 20182, 102533, 64189, 101555, 81673, 101999, 56154, 120908, 106958, 96677, 103304, 33943, 112039, 13, 102464, 17835, 14260, 86351, 100508, 109581, 101272, 11, 107712, 27796, 126862, 101272, 11, 104689, 103778, 101272, 78102, 43139, 114702, 33943, 112039, 13, 105519, 117546, 101703, 27796, 220, 7007, 15, 58126, 103131, 11, 105131, 101066, 101438, 220, 1591, 102757, 11, 116316, 102517, 39250, 100508, 103603, 27796, 220, 3965, 58126, 102757, 11, 86422, 32428, 30381, 101999, 54780, 27796, 220, 21209, 102757, 120908, 75086, 60798, 101360, 103659, 115777, 100757, 109, 118209, 126692, 220, 21, 124784, 115426, 101528, 13, 116853, 107032, 32428, 29833, 113037, 125626, 7, 117484, 55570, 101327, 102335, 100654, 116504, 74177, 16969, 220, 605, 33177, 120889, 220, 717, 30426, 103551, 3451, 61394, 109324, 52688, 100508, 11, 78453, 110616, 54780, 126372, 13447, 529, 104652, 111636, 78453, 13447, 13, 3451, 106103, 46810, 21028, 59777, 52688, 100508, 529, 220, 18, 101532, 113052, 32428, 117717, 104652, 117396, 102258, 101347, 54780, 127433, 101482, 43139, 114702, 112931, 13, 116849, 109324, 54535, 100508, 21028, 110133, 101136, 21121, 32428, 220, 777, 42529, 21121, 62060, 52688, 48424, 106001, 124883, 101824, 118999, 19954, 102597, 102258, 101347, 54780, 106010, 116316, 17835, 118213, 115096, 115115, 96270, 122752, 106906, 48424, 101204, 21028, 116865, 116681, 108047, 3451, 111282, 101518, 21121, 529, 18918, 103185, 57002, 44005, 106243, 43139, 111809, 112931, 13, 3451, 17835, 168, 253, 230, 21028, 59777, 52688, 100508, 90960, 58232, 529, 21028, 102678, 26799, 23955, 102335, 41381, 117264, 20565, 101703, 125626, 220, 17, 102156, 45618, 102039, 101930, 101272, 57575, 102258, 101347, 18359, 107973, 127433, 101482, 18359, 23955, 104381, 20541, 13, 119904, 116464, 34804, 220, 777, 42529, 106751, 106354, 32428, 220, 1272, 80732, 43139, 101703, 125626, 125180, 7, 2185, 2441, 647, 258, 2808, 18487, 68993, 8, 57575, 111529, 108302, 119068, 13, 119904, 71682, 16969, 112830, 13, 128009, 128006, 882, 128007, 198, 13447, 104690, 21028, 86422, 32428, 30381, 101999, 54780, 27796, 18918, 116253, 122432, 101703, 125626, 128009, 128006, 78191, 128007, 198, 115978, 107434, 102039, 37155, 104841, 49085, 125626, 34804, 50467, 104690, 21028, 86422, 32428, 30381, 101999, 54780, 27796, 18918, 116253, 122432, 101703, 125626, 43139, 11, 119929, 105519, 117546, 103966, 57390, 116425, 49085, 125626, 18359, 74623, 101106, 116304, 13, 23955, 101703, 125626, 34804, 106354, 72043, 14260, 35495, 100848, 77535, 125957, 73653, 114607, 127005, 100508, 101096, 18359, 72043, 101353, 101360, 86422, 30381, 35495, 30426, 120908, 119225, 44005, 364, 103583, 22817, 244, 105519, 117546, 6, 18359, 106958, 96677, 103304, 65219, 13879, 90463, 13, 105519, 117546, 101106, 34804, 102464, 17835, 14260, 86351, 100508, 109581, 101272, 11, 107712, 27796, 126862, 101272, 11, 104689, 103778, 101272, 78102, 43139, 114702, 106910, 117097, 11, 105519, 117546, 101703, 27796, 220, 7007, 15, 58126, 109969, 11, 105131, 101066, 101438, 220, 1591, 102757, 11, 116316, 102517, 39250, 100508, 103603, 27796, 220, 3965, 58126, 99458, 11, 86422, 32428, 30381, 101999, 54780, 27796, 220, 21209, 102757, 120908, 75086, 60798, 101360, 103924, 13, 112887, 11, 103659, 115777, 100757, 109, 118209, 126692, 220, 21, 67945, 49085, 115426, 106910, 112795, 118696, 102533, 36630, 117313, 18918, 108273, 101360, 103924, 15873, 1116, 20, 22877, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377f9f21-5caa-417c-b549-cae18348809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids 디코딩 결과:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: 서울 북촌에 자리잡은 서울교육청 정독도서관은 옛 학교 건물을 그대로 물려받았다. 도서관이 보금자리로 쓰고 있는 옛 경기고 건물은 등록문화재 제2호로 1938년 건립됐다. 경기고가 1976년 서울 삼성동으로 이전하면서 이듬해 도서관으로 새롭게 문을 열었다. 개관 때부터 학교 운동장을 아름다운 정원으로 꾸민 덕에 북촌 주민과 주변 직장인은 물론 관광객도 자주 찾는 명소가 됐다. 많은 사람이 이곳을 추억을 간직한 장소로 기억하는 이유다.아름다운 건물 외관과 더불어 정취로 잘 알려진 정독도서관은 도서관 운영 면에서도 모범사례로 꼽힌다. 도서관을 찾은 사람들은 방대한 자료와 쾌적한 환경에 놀란다. 장서는 50만권이 넘고 바로 옆에 있는 서울교육박물관에는 유물 1만3000여점이 있다. 학교 건물을 도서관으로 만들어 자료실과 열람실이 일반도서관에 비해 훨씬 넓다.정독도서관은 최근 서울교육청이 진행하는 도서관 특성화 사업에서 ‘청소년 중심 도서관’으로 지정됐다. 앞으로 3년 동안 중·고교 교과서에 나오는 소설과 진로지도 관련 자료, 특성화고 학생에게 필요한 취업 관련 자료 등을 집중적으로 구비할 예정이다. 오는 9월 리모델링으로 조성되는 청소년관엔 자료실과 함께 독서토론이나 강의를 할 수 있는 공간도 마련된다. 어린이관은 확장해 가족단위 이용자 방문을 늘릴 계획이다.정독도서관은 지난 5월부터 서울시내 고교에서 모집한 학생을 대상으로 독서논술교육을 하고 있다. 김성갑 관장은 “학생들이 어려워하는 대입논술의 길잡이 역할을 하기 위해 마련한 교육 프로그램”이라며 “아직은 수강생이 30명 정도지만 성공사례로 만들어 다른 도서관으로 확산하고 싶다”고 말했다.\n",
      "-----\n",
      "문서2: 서울도서관은 다음달 4일까지 1층 기획전시실에서 ‘장서표(藏書票)의 세계, 책과 사람들: 남궁산 목판화 장서표전’을 연다. 소장자의 표식이자 책의 장식인 장서표는 예술성을 가미하기 위해 문자와 그림을 조합해 판화로 제작하는 경우가 많다. 이번 전시에는 고은, 안도현, 정호승, 공지영 등 유명 작가들의 도서와 작가의 삶과 이야기를 담은 판화가 남궁산의 장서표 49종이 전시된다.국립중앙도서관과 서울대 중앙도서관은 한글역주 ‘자치통감(資治通鑑)’의 전자책 서비스를 시작했다. 제왕학의 교과서로 불리는 자치통감은 ‘춘추’ ‘사기’와 함께 동양 3대 역사서로 꼽힌다. 송나라 사마광이 전국(戰國) 시대부터 오대(五代)까지 1362년간의 역사를 편년체로 기록한 중국 통사다. 한글판은 권중달 중앙대 명예교수가 2010년 전 32권으로 완간한 종이책을 전자책으로 만든 것이다. 국립중앙도서관에선 도서관 열람회원들이 관내 PC나 태블릿을 통해 열람할 수 있다. 서울대 중앙도서관에선 소속 교수와 교직원, 재학생과 중앙도서관 동문회원들이 이용할 수 있다.충북 제천시립도서관은 12월까지 ‘청풍호 수몰 30주년 사진’을 1층 로비에 전시한다. 수몰 30주년 사진은 청풍호반과 함께 어우러진 산의 형태를 이미지화한 패널 형식으로 제작됐으며 고향, 가족, 친구 이야기로 구성돼 있다. 이번 장기 전시는 수몰된 청풍의 30년 전 모습을 알지 못하는 시민들이 제천의 숨겨진 역사를 새롭게 발견하는 계기를 주기 위해 기획됐다.서대문구립이진아기념도서관은 오는 11월12일까지 매주 목요일 청각장애인을 위한 독서프로그램을 운영한다. 10회에 걸쳐 운영되는 이번 프로그램은 국립중앙도서관에서 주관하는 ‘장애인 독서프로그램 공모’에 선정된 사업으로 분야별 책을 선정하고 수화통역사와 함께 책 읽기를 진행한다. 청각장애인이면 누구나 무료로 이 프로그램에 참여할 수 있으며, 도서관에 방문하거나 이메일(younghwa@sscmc.or.kr)로 신청하면 된다.\n",
      "-----\n",
      "문서3: 서울 사직동 서울교육청어린이도서관은 올 상반기 철거 위기를 겪었다. 문화재청이 복원을 추진하고 있는 사직단(사적 제121호) 권역에 도서관이 자리잡고 있어서였다. 사직단 권역은 국가 소유 땅이기 때문에 임대 연장을 승인하지 않으면 쫓겨날 상황이었다. 다행히 국내 최초의 어린이도서관을 지켜야 한다는 여론이 높아 문화재청은 어린이도서관을 복원 계획에서 제외하는 쪽으로 정리했다. 36년간 ‘어린이와 함께하며 미래를 열어가는 도서관’을 목표로 아이들과 함께 성장하고 발전하며 자리를 지켜온 전통과 상징을 인정받은 것이다.어린이도서관은 1979년 5월 ‘세계 어린이의 해’를 기념해 개관했다. 도서관이 들어선 건물은 이전까지 시립어린이병원으로 쓰여 어린이와 함께한 역사가 깊다.이 도서관은 어린이전문 도서관답게 아동도서에 특화돼 있다. 보유 장서는 27만5000권으로, 90%가량이 아동도서다. 홍순영 관장은 “1년에 구입하는 책만 2만2000여권”이라며 “국내에서 출간되는 아동서는 대부분 소장하고 있다”고 설명했다.이용자의 희망 도서 위주로 새 책이 들어오는 매주 수요일엔 사람들로 북적인다. 오랜 시간 입소문이 퍼지면서 경기 고양, 성남 등 수도권에서도 이용자가 몰린다. 일반 이용자 외에 유아교육 연구자들도 단골손님이다. 하루평균 대출 도서는 2000권을 훌쩍 넘는다. 다문화도서실에는 일본, 중국, 몽골 등 7개국 언어로 제작한 아동도서를 볼 수 있다.사서 18명이 만드는 권장도서 목록은 어린이도서관의 자랑이다. 5월 ‘가정의 달’과 여름·겨울방학 등 세 차례에 걸쳐 미취학 아동 그림책과 학년별 도서 등 40권을 추천한다. 목록을 발표할 때가 되면 서울시내 초등학교는 물론 학부모로부터 “언제쯤 나오느냐”는 질문이 쏟아진다. 학기마다 두 번 열리는 독서증진대회는 어린이도서관과 역사를 같이하는 행사로, 교장 추천을 받은 어린이들이 모인다. 독서감상회, 동화구연, 독서감상문, 글짓기 행사 등이 열린다. 매주 토요일에는 다양한 문화·체험 행사와 프로그램이 운영된다.도서관은 최근 문화관 1층 전시실을 개조해 ‘체험동화마을’을 조성했다. 대형 스크린에 아이들이 동화 배경과 함께 나오게 해 동화 속 주인공이 된 느낌을 준다. 매주 수요일 단체신청자를 대상으로 프로그램을 진행한다. 다음달부터는 개별적으로도 체험할 수 있다. 평일 오후 유아실에는 할머니가 들려주는 동화구연 프로그램이 진행된다.홍 관장은 “예전에 도서관을 찾았던 어린이들이 이제는 부모가 돼 아이 손을 잡고 찾아오는 걸 볼 때면 가슴이 뭉클하다”며 “아이들과 부모가 함께 참여해 공감을 나눌 수 있는 프로그램을 늘릴 계획”이라고 말했다.\n",
      "-----\n",
      "문서4: 420여개의 출판사와 인쇄업체가 입주해 있는 파주출판도시 한가운데에 자리한 아시아출판문화정보센터와 게스트하우스 ‘지지향(紙之鄕)’. 지지향 로비에 들어서면 기둥과 벽을 가득 채운 책장이 먼저 눈에 띈다. 건물이 연결된 통로를 지나 출판문화정보센터로 들어서면 더욱 큰 서가가 방문객을 맞이한다. 1256㎡에 이르는 공간의 벽마다 책장을 설치해 책을 채워 넣었다. 높이 6m가 넘는 책장을 보면 얼마나 많은 책이 꽂혀 있는지 가늠하기 어려울 정도다. 이 공간은 내달 19일 문을 여는 ‘열린 도서관-지혜의 숲’이다. 출판도시문화재단(이사장 김언호·한길사 대표)이 지난해 5월 설립에 착수해 1년 만에 개관을 앞두고 있다. ‘지혜의 숲’은 재단이 여러 출판사와 지식인, 학자를 비롯해 다양한 사람들로부터 기증받은 도서로 꾸민 전면 개가식 도서관이다. 개별 서재들의 거대한 집합인 셈. 중국 일본 대만 등에서도 책을 보내왔다. 파주출판도시를 찾은 사람들이라면 누구나 제한 없이 이용할 수 있다. 100만권을 기증받는다는 목표 아래 사업을 시작해 현재 50만권이 확보된 상태. 1단계로 20만권의 책이 서가에 꽂혀 있다. 책을 관리하는 사서가 없고 보통의 도서관처럼 체계적으로 분류하지 않아 다소 생소하지만 다양한 분야의 책이 한눈에 들어올 만큼 널찍하다. ‘지혜의 숲’ 도서관의 가장 큰 특징은 출판사와 기증자별 서가다. 책을 기증한 출판사 서가를 찾으면 출판사가 그동안 낸 책을 모두 만날 수 있다. 대형 서점을 찾아도 일반 도서관처럼 분야별로 분류된 책만 볼 수 있지만, 이곳에선 출판사가 어떤 철학으로 책을 만들어 왔는지 한눈에 볼 수 있다. 책을 좋아하는 이들이라면 민음사 세계문학전집, 한길 그레이트북스 같은 전집이 모두 꽂힌 서가에 반할 수밖에 없다. 많은 학자들이 선뜻 기증한 책을 보면 한 연구자가 그동안 어떤 책을 읽으며 공부했는지 지식의 이력서를 보는 재미를 느낄 수 있다. 인류의 가장 위대한 정신·문화 유산인 종이책이 함부로 버려지는 걸 안타까워하며 책 리사이클링 운동과 독서운동 활성화를 제창해온 김 이사장은 “수많은 책이 쏟아지면서 독자들에게 채 읽히지도 못한 채 폐기되는 안타까운 상황이 빈번하다”며 “이미 나온 책이라 하더라도 존중하지 않는다면 우리에게 발전은 없다”고 강조했다. 그는 이어 “젊은 세대가 책을 잘 읽지 않는 현상을 항상 우려해왔다”며 “전문서는 물론 교양서를 두루 갖춰 책이 사람들에게 좀 더 친근하게 다가설 수 있도록 만들었다”고 설명했다. 도서관 운영은 사서 대신 30여명의 권독사(勸讀司)들이 맡는다. 책을 소개하고 독서를 권하는 자원봉사자다. 권독사 교육을 담당하는 번역가 박종일 씨는 “대만 고궁박물관에서 허름한 차림의 할아버지가 어린 학생들에게 갑골문 이야기를 들려주는 것을 본 적이 있는데 그 분이 당대 최고의 갑골문 학자였다는 사실을 알고 놀랐다”며 “지식을 가진 사람이 젊은이들에게 독서를 통한 앎의 기쁨을 전달하는 것이 이 도서관의 목적”이라고 설명했다. ‘지혜의 숲’의 지지향 로비 서가는 24시간 열람할 수 있으며 점차 열람 범위를 확대할 방침이다. (031)955-0050\n",
      "-----\n",
      "문서5: 국립중앙도서관은 ‘조선과 청조(淸朝) 문인의 만남’이라는 주제로 오는 12월30일까지 본관 6층 고전운영실에서 고문헌 전시회를 연다. 조선 실학자 홍대용이 항주 선비들과 주고받은 필담 및 편지가 수록된 ‘담헌서’, 이덕무 유득공 박제가 이서구의 시를 모아놓은 ‘한객건연집’, 김정희가 청조 전각가의 인장집 표지에 평을 쓴 ‘일석산방인록’ 등 25종 133책의 고문헌을 전시한다. 국립중앙도서관 관계자는 “조선과 청의 문명 교류사를 조명하는 자리”라고 설명했다. 자세한 전시 목록은 도서관 홈페이지(www.nl.go.kr)의 ‘소통·참여→전시행사’에서 확인할 수 있다.서울교육청 정독도서관이 지난달 30일 공공도서관 중 처음으로 청소년 특화 전문도서관을 개관했다. 도서관 내 378.5㎡ 규모로 조성된 청소년관은 일반 중·고교생과 정규학업을 중단하고 검정고시 등을 준비하는 ‘학교 밖 청소년’, 학부모와 교사 등을 위해 마련됐다. 진로·진학자료실, 독서상담실, 토론실 등으로 구성됐다. 청소년 도서 7000여권, 간행물 28종, 대학별 입학 안내서 150여종, 검인정 교과서 458종 등을 비치하고 데스크톱 컴퓨터 6대를 설치했다.경기 용인 수지도서관(관장 신현국)은 오는 10일 낮 12시부터 ‘러시아문학, 연극과 만나다’ 행사를 연다. ‘길 위의 인문학’ 3차 프로그램인 이번 행사는 강연과 탐방으로 구성된다. 러시아 문학의 황금기인 19세기 대문호들의 삶 및 작품에 대한 강연과 서울 대학로 극장을 찾아 안톤 체호프의 대표 희곡 ‘갈매기’를 감상하는 시간으로 진행된다. ‘로쟈의 인문학 서재’의 저자 이현우 씨가 도서관 2층 시청각실에서 강연을 하고 탐방을 이끈다. 참가 대상은 19세 이상 일반인 40명으로 도서관 홈페이지(www.yonginlib.go.kr)에서 신청하면 된다. 참가비는 무료.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "다량의 검인정 교과서를 갖춘 도서관<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "서울교육청 정독도서관은 다량의 검인정 교과서를 갖춘 도서관으로, 최근 청소년 특화 전문도서관을 개관했습니다. 이 도서관은 일반 중·고교생뿐만 아니라 정규학업을 중단하고 검정고시 등을 준비하는 '학교 밖 청소년'을 위해 마련되었습니다. 청소년관은 진로·진학자료실, 독서상담실, 토론실 등으로 구성되어 있으며, 청소년 도서 7000여 권, 간행물 28종, 대학별 입학 안내서 150여 종, 검인정 교과서 458종 등을 비치하고 있습니다. 또한, 데스크톱 컴퓨터 6대도 설치되어 있어 다양한 학습 자료를 제공하고 있습니다[[ref5]].<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# 디코딩된 input_ids 출력\n",
    "decoded_text = tokenizer.decode(\n",
    "    batch[\"input_ids\"][0].tolist(),\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\ninput_ids 디코딩 결과:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7c3cfef-655b-451d-9fa6-599c0397e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 115978, 107434, 102039, 37155, 104841, 49085, 125626, 34804, 50467, 104690, 21028, 86422, 32428, 30381, 101999, 54780, 27796, 18918, 116253, 122432, 101703, 125626, 43139, 11, 119929, 105519, 117546, 103966, 57390, 116425, 49085, 125626, 18359, 74623, 101106, 116304, 13, 23955, 101703, 125626, 34804, 106354, 72043, 14260, 35495, 100848, 77535, 125957, 73653, 114607, 127005, 100508, 101096, 18359, 72043, 101353, 101360, 86422, 30381, 35495, 30426, 120908, 119225, 44005, 364, 103583, 22817, 244, 105519, 117546, 6, 18359, 106958, 96677, 103304, 65219, 13879, 90463, 13, 105519, 117546, 101106, 34804, 102464, 17835, 14260, 86351, 100508, 109581, 101272, 11, 107712, 27796, 126862, 101272, 11, 104689, 103778, 101272, 78102, 43139, 114702, 106910, 117097, 11, 105519, 117546, 101703, 27796, 220, 7007, 15, 58126, 109969, 11, 105131, 101066, 101438, 220, 1591, 102757, 11, 116316, 102517, 39250, 100508, 103603, 27796, 220, 3965, 58126, 99458, 11, 86422, 32428, 30381, 101999, 54780, 27796, 220, 21209, 102757, 120908, 75086, 60798, 101360, 103924, 13, 112887, 11, 103659, 115777, 100757, 109, 118209, 126692, 220, 21, 67945, 49085, 115426, 106910, 112795, 118696, 102533, 36630, 117313, 18918, 108273, 101360, 103924, 15873, 1116, 20, 22877, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82f7eaa4-3837-42de-a37b-8800da82462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labels 디코딩 결과 (-100 제외):\n",
      "서울교육청 정독도서관은 다량의 검인정 교과서를 갖춘 도서관으로, 최근 청소년 특화 전문도서관을 개관했습니다. 이 도서관은 일반 중·고교생뿐만 아니라 정규학업을 중단하고 검정고시 등을 준비하는 '학교 밖 청소년'을 위해 마련되었습니다. 청소년관은 진로·진학자료실, 독서상담실, 토론실 등으로 구성되어 있으며, 청소년 도서 7000여 권, 간행물 28종, 대학별 입학 안내서 150여 종, 검인정 교과서 458종 등을 비치하고 있습니다. 또한, 데스크톱 컴퓨터 6대도 설치되어 있어 다양한 학습 자료를 제공하고 있습니다[[ref5]].<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# -100이 아닌 부분만 골라 디코딩\n",
    "label_ids = [token_id for token_id in batch[\"labels\"][0].tolist() if token_id != -100]\n",
    "\n",
    "decoded_labels = tokenizer.decode(\n",
    "    label_ids,\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\nlabels 디코딩 결과 (-100 제외):\")\n",
    "print(decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbdb20-95a2-41e1-a63a-14f9f6765d6a",
   "metadata": {},
   "source": [
    "### input_ids와 labels는 어떻게 생성되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd6653-5097-4456-8a08-699687804632",
   "metadata": {},
   "source": [
    "\n",
    "LLM 학습에서 `input_ids`와 `labels`는 모델의 학습 목표에 따라 생성됩니다. 이를 예시 문장과 정수 인코딩을 통해 상세히 설명하겠습니다.\r\n",
    "\r\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다.\r\n",
    "사용자가 `안녕하세요, 오늘 날씨는 어떤가요?`라고 물었고,\r\n",
    "모델은 `안녕하세요! 오늘 날씨는 맑고 화창합니다.`라고 응답해야 한다고 합시다.\r\n",
    "\r\n",
    "LLaMA 3에서는 다음과 같은 템플릿 구조를 사용합니다:\r\n",
    "\r\n",
    "`<|begin_of_text|><|start_header_id|>user<|end_header_id|>\r\n",
    "안녕하세요, 오늘 날씨는 어떤가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\r\n",
    "안녕하세요! 오늘 날씨는 맑고 화창합니다.<|eot_id|>`\r\n",
    "\r\n",
    "이 전체 텍스트는 토크나이저에 의해 정수 시퀀스로 변환됩니다. 예시로 단순화된 정수 시퀀스는 다음과 같다고 가정합니다:\r\n",
    "\r\n",
    "`input_ids = [1001, 2001, 3001, 4001, 5001, 6001, 7001, 1002, 1001, 8001, 9001, 1003, 2002]`\r\n",
    "\r\n",
    "여기서 모델이 예측해야 할 영역은 assistant의 응답 부분인\r\n",
    "`안녕하세요! 오늘 날씨는 맑고 화창합니다.`에 해당하는 토큰들입니다.\r\n",
    "따라서 `labels`는 다음과 같이 설정됩니다:\r\n",
    "\r\n",
    "`labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, 8001, 9001, 1003, 2002]`\r\n",
    "\r\n",
    "이처럼 `labels`는 모델의 출력이 필요한 영역만을 포함하고, 나머지 부분은 `-100`으로 채워져\r\n",
    "모델이 실제로 예측하고 오차를 계산해야 하는 대상(학습 대상)에서 제외됩니다.\r\n",
    "\r\n",
    "이를 통해 모델은 불필요한 입력 부분을 학습하지 않고, assnt 응답 부분에만 집중할 수 있습니다.\r\n",
    "\"\"\"\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42c907-fb98-405a-9b90-5a3413199092",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9ef86a9-8b8a-4aeb-809a-9d7babca9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af87dc-5b07-4a0f-b922-cabf0ca549fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 93/285 09:59 < 21:04, 0.15 it/s, Epoch 0.97/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.460600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.430200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f9d42-574a-4ad8-9327-7315d1a1269b",
   "metadata": {},
   "source": [
    "## 6. 테스트 데이터 준비\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c2138-e4e5-4afb-a37d-ef8dd70b5f4c",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 '<|start_header_id|>assistant<|end_header_id|>\\n'가 부착되어서 넣는 것이 좋습니다. 그래야만 모델이 바로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abb48dbc-646c-4a2d-ae1f-0ef824cae419",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "011e29f0-c6d9-4e90-9ebc-78a4f2ca9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: 세계 3위 유통업체인 테스코가 중국에서 9년간 진행해온 독자사업을 포기했다고 텔레그래프가 11일 보도했다.테스코는 대신 중국 국영 화룬(華潤)기업과 합작법인을 설립하기로 했다. 테스코는 중국 내 121개 매장을 이 합작법인에 넘기고 지분 20%를 받을 계획이다. 이번 합작은 양사 간 윈윈으로 보이지만 사실상 테스코가 중국 시장에서 백기를 든 것이라고 로이터통신이 지적했다. 테스코는 올 들어 중국 내 매출이 4.9% 하락하는 등 중국 시장 공략에 어려움을 겪어왔다. 이 회사는 올해 미국, 지난해에는 일본 시장에서 철수키로 결정했으며 당분간 모국인 영국 투자에 집중할 계획이다.화룬은 “합작법인을 통해 단일 브랜드로 대형마트와 슈퍼마켓, 편의점 등 다양한 사업을 벌일 것”이라고 밝혔다. 화룬그룹은 중국 본토와 홍콩에 매장 3000개를 운영하고 있는 중국 최대 유통업체다.중국에서 고전하고 있는 다국적 유통업체는 테스코만이 아니다. 올해 초에는 독일 유통업체 메트로가 중국 내 가전제품 사업부를 철수시켰고, 사무용품 전문업체 홈디포도 중국 내 7개 대형 매장의 문을 닫았다. 월마트와 까르푸는 아직 공격적인 매장 확장 계획을 발표하고 있지만 수익성 악화로 고전하고 있다. 물류비용 부담이 크고, 저렴한 가격으로 무장한 현지 업체들과의 경쟁이 치열하기 때문이다. 유로모니터에 따르면 현재 중국 소매유통시장에서는 대만·프랑스 합작업체인 선아트가 13.6%의 시장점유율로 1위를 달리고 있다. 화룬그룹과 월마트가 각각 10.9%, 까르푸가 6.9%, 테스코가 2.4%의 점유율을 기록하고 있다.\n",
      "-----\n",
      "문서2: 국무총리 후보자 두 명의 연쇄 낙마에 이어 일부 장관 후보자들마저 자질 논란에 휩싸이면서 야당은 물론 여당에서도 반대 기류가 강해지자 박근혜 대통령이 선택의 기로에 몰리고 있다. 2기 내각 교체 명단을 발표한 지 한 달이 지나도록 내각이 정상 출범하지 못한 데 따른 국정공백을 막기 위해 임명을 강행할 수도 있지만, 그럴 경우 어렵사리 마련된 여의도와의 ‘소통정치’ 무드에 찬물을 끼얹을 수도 있다.국회 교육문화체육관광위원회는 11일 전체회의를 열어 김명수 부총리 겸 교육부 장관 후보자와 정성근 문화체육관광부 장관 후보자에 대한 인사청문보고서 채택을 논의할 예정이었지만, 야당이 회의 참석을 거부해 채택이 불발됐다. 안전행정위원회도 이날 정종섭 안전행정부 장관 후보자 청문보고서를 채택하려 했지만 야당 의원들의 반대로 무산됐다.여권 관계자는 “야당이 회의 자체를 거부해 강제적으로 밀어붙일 수 없는 상황”이라며 “박 대통령이 임명을 강행하든지, 아니면 지명을 철회하든지, 이것도 아니면 후보자가 스스로 사퇴하든지 하는 수밖에 없다”고 말했다.청와대와 여권에 따르면 세 명의 후보 가운데 논문 표절과 부적절한 주식거래 의혹을 받고 있는 김 후보자는 여당 내에서도 ‘부적합’이란 기류가 강해 자진사퇴 쪽으로 가닥이 잡힌 것으로 전해졌다. 여당 지도부는 이미 ‘김명수 불가’ 입장을 청와대에 전달했고, 박 대통령도 전날 여야 원내대표와의 회동에서 야당의 재고 요청에 “참고하겠다”고 한 만큼 김 후보자에 대해선 사실상 마음을 접은 것으로 알려졌다. 때문에 김 후보자는 주말을 전후로 자진사퇴 입장을 밝힐 것이란 게 유력한 관측이다.청문회에서 위증 논란을 일으킨 정성근 후보자에 대해서도 막판에 여권 내 기류가 심상치 않게 흐르고 있다. 여당 교문위 관계자는 “자진사퇴 수순으로 가는 김 후보자보다 정 후보자가 더 문제라는 의견이 여당 내에서도 많아지고 있다”고 했다. 이 때문에 당초 ‘김명수 임명 불가, 정성근 임명 강행’으로 가닥이 잡히던 청와대 내 기류도 바뀔 가능성이 거론된다. 청와대 일각에선 야당의 요구대로 세 명의 장관을 지명 철회할 경우 장기간 국정공백이 불가피하다는 점을 들어 정성근-정종섭 후보자는 임명을 강행하자는 강경론도 제기되는 것으로 알려졌다.이들 장관 후보자의 거취가 불확실해지면서 2기 내각 출범 시기도 더 늦춰질 공산이 커졌다. 청와대는 청문 보고서가 채택된 장관 후보자들에 대해서도 임명장 수여 시기를 미룰 방침이다. 이와 관련, 민경욱 청와대 대변인은 “청문 보고서가 채택된 후보자도 있고, 그렇지 못한 후보자도 있는데 임명장 수여는 절차에 따라 한꺼번에 처리할 예정”이라고 말했다. 청문 보고서가 채택되지 않은 후보자에 대해 10일 이내에 국회에 보고서 채택을 다시 요청하는 절차를 밟은 후 일괄 임명하겠다는 것이다. 이 경우 낙마하는 장관을 제외하고 최경환 부총리 겸 기획재정부 장관 후보자 등 나머지 2기 내각 교체 멤버에 대해선 이르면 다음주 초 임명장을 수여할 것으로 예상된다.\n",
      "-----\n",
      "문서3: 1956년 11월 상순에 12명의 선수와 매니저 겸 스카우트인 아오키 이치조가 '후지무라 감독 퇴진 요구서'를 구단 오너인 노다 세이조에게 제출하였고 이를 스포츠 신문이 보도하는 형태로 표면화되었다. 12월 4일에 구단 측은 후지무라 감독의 유임과 퇴진 요구에 관여된 가네다 마사야스·사나다 주조의 두 선수는 다음 시즌에 재계약을 하지 않을 것이라고 발표했다. 그 후 구단 대표인 도자와시 가즈타카가 관계자와 협상을 계속한 결과 12월 25일에 구단은 가네다와의 재계약을 발표했다. 12월 30일에는 도자와시 구단 대표, 후지무라 감독, 가네다가 각각 성명서를 발표하고 사태가 일단락되었다.\n",
      "\n",
      "한신 구단의 역사인 《한신 타이거스 쇼와의 발자취》(1991년)과 마쓰키 겐지로의 《타이거스의 성장》(고분샤, 1973년)에는 발단 부분을 제외하고는 거의 상기에 가까운 내용이 적혀 있다. 당시 최초의 요구서로부터 스포츠 신문을 중심으로 한 보도가 과열되었으나 그것은 도자와시 구단 대표가 \"현실보다 기사가 훨씬 앞서 있다\"고 평했던 것과 같은 내용이다. 따라서 발단에서 해결에까지 이르는 과정에 대한 자세한 내용은 관계자와 후년의 증언에 의존할 수 밖에 없지만 이것도 증언자나 시기에 따라 반드시 일치하지 않는다. 다음 글에서는 그 차이도 근거로 하여 서술한다.\n",
      "-----\n",
      "문서4: 영국 최대 유통업체이자 홈플러스의 모기업인 테스코가 수익성이 낮은 43개 점포를 폐쇄하기로 하는 등 구조조정 계획을 내놓았다. 관심을 모았던 홈플러스 매각 여부에 대해서는 직접적으로 언급하지 않았다.테스코는 8일 점포 폐쇄와 자산 매각 계획 등을 담은 재무구조 개선 방안을 발표했다. 수익성이 낮은 점포 43곳의 문을 닫고 출점 예정이었던 49개 점포를 내지 않기로 했다. 그러나 홈플러스를 포함한 해외 사업 철수 계획은 밝히지 않았다. 데이브 루이스 테스코 회장은 “다른 결정을 내리기 전까지 해외 사업에 최선을 다할 것”이라고 말했다. 루이스 회장은 그러나 “재무상태가 건전하고 유동성이 있지만 오늘 발표가 끝은 아니다”고 말해 앞으로 해외 자산 매각을 추진할 수 있다는 여지를 남겼다.테스코는 지난해 상반기 분식회계로 영업이익을 2억5000만파운드(약 4120억원) 부풀린 사실이 드러나면서 창사 이래 최대 위기를 겪고 있다. 분식회계를 걷어낸 지난해 상반기 매출은 전년 동기보다 4.5% 줄었고 세후 이익은 92% 급감했다. 이 때문에 업계에서는 테스코가 위기를 극복하기 위해 홈플러스를 매각할 것이라는 관측이 있었다.\n",
      "-----\n",
      "문서5: 정태수 전 한보그룹 회장의 한보철강, 가구업체 라자가구의 송자현 전 대표 등 5억원 이상의 세금을 1년 넘게 체납한 개인과 법인 2401명의 명단이 공개됐다. 조세포탈죄로 기소돼 유죄판결을 받은 표순종 씨 등 2명과 234억원의 해외금융계좌를 신고하지 않은 업체 네오트리(대표 이경민)도 공개 명단에 올랐다.국세청은 26일 고액·상습 체납자 개인 1733명과 법인 665개 업체, 조세포탈범 2명, 해외금융계좌 신고의무 위반자 1명의 명단을 홈페이지(www.nts.go.kr)와 세무서 게시판을 통해 공개했다. 상습 체납자는 5억원 이상의 국세를 1년 이상 체납한 사람·법인이다. 국세청이 조세포탈범과 해외금융계좌 신고의무 위반자 명단을 공개한 것은 이번이 처음이다.개인 체납자 가운데는 도소매업체 에이치에스메탈스크랩의 대표인 이성구 씨가 종합소득세 등 424억원을 내지 않아 체납액 1위에 올랐다. 인터넷 도박사이트 운영자 이대근 씨와 라자의 송자현 전 대표는 부가세 등을 377억원, 233억원 각각 체납해 2, 3위에 올랐다. 법인 중에는 한보철강이 부가세 등 423억원을 납부하지 않아 체납액 1위에 올랐다. 1997년 회사 정리절차에 들어갔을 당시의 세금이지만 그동안 회생절차가 진행 중이어서 대상에서 제외됐다가 공개됐다.조세포탈범으로 명단이 공개된 김경철 씨는 창현금속이라는 회사를 설립해 부가세를 납부하지 않은 채 이 회사를 폐업하는 방식으로 8억7900만원을 포탈했다가 징역 2년, 벌금 23억원 판결을 받았다. 이경민 네오트리 대표는 해외계좌에 234억원을 보유하고도 신고하지 않았다가 국세청에 적발됐다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "테스코가 출점을 취소한 총 개수<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "500cdfe5-8e8a-4ad9-a007-7181852c0997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스코는 수익성이 낮은 43개 점포를 폐쇄하고 출점 예정이었던 49개 점포의 출점을 취소하기로 결정했습니다. 따라서 테스코가 출점을 취소한 총 개수는 49개입니다 [[ref4]].\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[700])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20845912-3ac4-4eaa-aaa5-edebd8cc5998",
   "metadata": {},
   "source": [
    "## 7. 파인튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414b57ea-e413-4421-9235-77c324c7fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04c57144-194c-4c97-883a-b8c601557f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd8134662604d0b8e5da790ef30bac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"llama-3-8b-rag-ko/checkpoint-285\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01c8cc1a-db7a-465e-8745-e8b0b340dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e7517c0-f227-43f8-bf2e-55dfe965866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ca694c28-77a9-48be-8192-e50faba386c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "테스코가 출점을 취소한 총 개수는 49개입니다. 테스코는 수익성이 낮은 점포 43개를 폐쇄하고, 출점 예정이었던 49개 점포의 출점을 내지 않기로 결정했습니다 [[ref4]].\n",
      "    label:\n",
      "\n",
      "테스코는 수익성이 낮은 43개 점포를 폐쇄하고 출점 예정이었던 49개 점포의 출점을 취소하기로 결정했습니다. 따라서 테스코가 출점을 취소한 총 개수는 49개입니다 [[ref4]].\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "검찰 내부에서 더 선호하는 검사 드라마를 방영하는 방송국은 MBC입니다. SBS의 '펀치'보다 MBC의 '오만과 편견'에 대한 선호도가 높다고 언급되었습니다. '펀치'는 부패하고 탐욕이 강한 고위급 검사를 다루는 내용이 많아 부정적인 이미지를 부각시키는 반면, '오만과 편견'은 범죄와 치열하게 싸우는 긍정적인 모습을 보여주기 때문에 검찰 내부에서 더 선호된다고 설명되었습니다 [[ref2]].\n",
      "    label:\n",
      "\n",
      "검찰 내부에서 더 선호하는 검사 드라마를 방영하는 방송국은 MBC입니다. MBC의 드라마 '오만과 편견'이 검찰 내부에서 더 긍정적인 평가를 받고 있습니다. 이는 '펀치'가 검찰을 부정적으로 그리는 대목이 많은 반면, '오만과 편견'은 범죄와 치열하게 싸우는 비교적 긍정적인 모습을 보여주기 때문입니다. 검사들 사이에서는 '펀치'가 검찰에 대한 부정적인 사건을 짜깁기해 만든 것 같다는 의견이 있으며, '오만과 편견'은 현실과 동떨어진 느낌이 덜하다는 평가를 받고 있습니다 [[ref2]].\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "정부와 FTA 논의 계획 중인 국가들을 포함하는 대륙은 중미와 남미입니다. 정부는 중미 7개국(과테말라, 엘살바도르, 온두라스, 니카라과, 코스타리카, 파나마, 에콰도르)과 FTA 협상을 개시할 계획이며, 중미경제통합상설사무국(SIECA) 소속 국가들과 에콰도르를 대상으로 협상을 시작할 예정입니다. 또한, 중미통합체제(SICA)와 메르코수르(MERCOSUR) 등 여러 경제 블록이 있는 중남미 지역에서 FTA 협상을 진행할 계획입니다. [[ref1]], [[ref4]]\n",
      "    label:\n",
      "\n",
      "6월에 정부와 FTA 논의를 계획 중인 국가들이 포함된 대륙은 주로 중미와 남미입니다. 구체적으로, 한국 정부는 중미경제통합상설사무국(SIECA) 소속 6개국(과테말라, 엘살바도르, 온두라스, 니카라과, 코스타리카, 파나마)과 에콰도르와의 FTA 협상을 6월 중 공식 선언할 계획입니다. 이들 국가들은 모두 중미와 남미에 위치해 있습니다 [[ref1]].\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[700:703], label_lst[700:703]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe0d81-3c28-4106-bdc0-843be96cde9d",
   "metadata": {},
   "source": [
    "## 8. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffe02a36-c6fb-44fc-b26d-1b2259be5f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7c561a020c4be78805dedbefba4cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4116dc56-45c8-475e-b125-e0692c469de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "테스코가 출점을 취소한 총 개수는 49개입니다.\n",
      "\n",
      "문서4에 따르면 \"수익성이 낮은 점포 43곳의 문을 닫고 출점 예정이었던 49개 점포를 내지 않기로 했다\"라고 명시되어 있습니다.\n",
      "\n",
      "따라서 출점을 취소한 총 개수는 43(폐쇄된 점포 수) + 49(출점 예정이었으나 취소된 점포 수) = 92개가 아니라, 오류로 인해 43과 49를 더한 대신 49만을 언급하면 정확한 답이 됩니다. \n",
      "\n",
      "정확한 답변: 49개\n",
      "    label:\n",
      "\n",
      "테스코는 수익성이 낮은 43개 점포를 폐쇄하고 출점 예정이었던 49개 점포의 출점을 취소하기로 결정했습니다. 따라서 테스코가 출점을 취소한 총 개수는 49개입니다 [[ref4]].\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "검찰 내부에서 더 선호하는 드라마에 대해 직접적으로 언급된 부분은 없습니다. \n",
      "\n",
      "하지만 문서2에 따르면 \"검사 A씨는 ‘TV에 검사가 자주 등장하고 가족들이 ‘진짜로 저래’라고 물어보는 터라 관심을 안 가질 수가 없다’며 ‘검사나 수사관끼리 모이면 종종 가십성으로 검찰 드라마를 화제에 올린다’고 말했다.\"라는 내용이 있습니다. \n",
      "\n",
      "이는 검사들이 자신들의 직업에 대해 더 깊이 이해하고 공감할 수 있는 드라마를 선호할 수 있음을 시사합니다. \n",
      "\n",
      "하지만 특정 방송국이나 드라마를 선호하는 이유에 대해서는 명확히 언급되지 않았습니다. \n",
      "\n",
      "따라서 검찰 내부에서 가장 선호하는 방송국이나 드라마를 정확히 말하기는 어렵습니다. \n",
      "\n",
      "검사들이 선호하는 드라마의 선택 기준에 대해서는 개인의 취향, 가치관, 드라마의 내용 등 다양한 요인이 복합적으로 작용할 것으로 추측할 수 있겠습니다. \n",
      "\n",
      "더 구체적인 정보가 필요하다면 직접 검찰 관계자나 관련 전문가에게 조사를 해볼 필요가 있을 것 같습니다.\n",
      "    label:\n",
      "\n",
      "검찰 내부에서 더 선호하는 검사 드라마를 방영하는 방송국은 MBC입니다. MBC의 드라마 '오만과 편견'이 검찰 내부에서 더 긍정적인 평가를 받고 있습니다. 이는 '펀치'가 검찰을 부정적으로 그리는 대목이 많은 반면, '오만과 편견'은 범죄와 치열하게 싸우는 비교적 긍정적인 모습을 보여주기 때문입니다. 검사들 사이에서는 '펀치'가 검찰에 대한 부정적인 사건을 짜깁기해 만든 것 같다는 의견이 있으며, '오만과 편견'은 현실과 동떨어진 느낌이 덜하다는 평가를 받고 있습니다 [[ref2]].\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "중미 6개국(파나마, 코스타리카, 과테말라, 온두라스, 엘살바도르, 도미니카공화국)과 에콰도르, 멕시코, 메르코수르(아르헨티나, 브라질, 파라과이, 우루과이, 베네수엘라) 총 9개국이 포함됩니다.\n",
      "\n",
      "참고문서:\n",
      "- \"정부가 차기 FTA 대상으로 상정한 곳은 중미 6개국 경제공동체인 SIECA 소속 국가들과 에콰도르... 등이다.\"\n",
      "- \"현재 FTA 체결과 관련해 한국과 공동 연구를 마친 국가는 파나마 코스타리카 과테말라 온두라스 엘살바도르 도미니카공화국과 메르코수르 회원국이다.\"\n",
      "- \"SICA는 파나마 코스타리카 엘살바도르 과테말라 니카라과 도미니카공화국 벨리즈 온두라스 등 중앙아메리카 8개 국가가 참여하는 경제기구이며, SIECA는 SICA 참여국 가운데 도미니카공화국과 벨리즈 등 2개를 제외한 6개 국가가 회원이다.\"\n",
      "- \"메르코수르는 아르헨티나 브라질 파라과이 우루과이 베네수엘라 등 4개국이 포함된 경제 공동체다.\" \n",
      "\n",
      "따라서 FTA 협상 계획에 포함된 국가들을 대륙별로 정리하면 다음과 같습니다:\n",
      "- 북아메리카: 멕시코\n",
      "- 중앙아메리카: 파나마, 코스타리카, 과테말라, 온두라스, 엘살바도르, 도미니카공화국\n",
      "- 남아메리카: 아르헨티나, 브라질, 파라과이, 우루과이, 베네수엘라\n",
      "- 에콰도르 (남아메리카와 북아메리카 경계 지역에 위치)\n",
      "    label:\n",
      "\n",
      "6월에 정부와 FTA 논의를 계획 중인 국가들이 포함된 대륙은 주로 중미와 남미입니다. 구체적으로, 한국 정부는 중미경제통합상설사무국(SIECA) 소속 6개국(과테말라, 엘살바도르, 온두라스, 니카라과, 코스타리카, 파나마)과 에콰도르와의 FTA 협상을 6월 중 공식 선언할 계획입니다. 이들 국가들은 모두 중미와 남미에 위치해 있습니다 [[ref1]].\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "작년 3분기에 삼성전자는 사상 최대인 10조1600억원의 영업이익을 기록했습니다. 이는 회사의 분기 영업이익 기록상 최고치였습니다.\n",
      "\n",
      "출처:\n",
      "1. 문서1: \"삼성전자는 2013년 3분기에 사상 최대인 10조2000억원의 영업이익을 냈다.\"\n",
      "2. 문서4: \"작년 3분기에 분기 기준 사상 최대인 10조1600억원의 영업이익을 올린 점에 비춰...\"\n",
      "\n",
      "따라서 질문에 대한 답변은 다음과 같습니다:\n",
      "\"삼성전자의 작년 3분기 영업이익은 10조1600억원입니다.\"\n",
      "    label:\n",
      "\n",
      "삼성전자의 작년 3분기 영업이익은 10조1600억원이었습니다. 이는 삼성전자가 사상 처음으로 10조원을 넘는 영업이익을 기록한 분기였습니다 [[ref2]].\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "검색 결과에 따르면 임영록 KB금융지주 회장과 임종룡 농협금융지주 회장의 나이에 대한 정보는 명시되어 있지 않습니다. \n",
      "\n",
      "임영록 회장의 출신과 경력에 대해 언급된 문서에서는 그의 나이를 직접적으로 언급하지 않았습니다.\n",
      " \n",
      "임종룡 회장 역시 그의 이력과 경력에 대해 설명된 문서에서는 나이에 대한 정보가 제공되지 않았습니다.\n",
      "\n",
      "따라서 현재 제공된 정보만으로는 두 회장의 나이를 정확히 알 수 없습니다. 추가적인 검색이나 직접적인 언급이 필요한 것 같습니다.\n",
      "    label:\n",
      "\n",
      "KB금융지주와 농협금융지주의 회장 중 행정고시에 먼저 합격한 사람은 임영록 KB금융지주 회장입니다. 임영록 회장은 제20회 행정고시에 합격했으며, 임종룡 농협금융지주 회장은 제24회 행정고시에 합격했습니다. 따라서 임영록 회장이 행정고시에 먼저 합격했습니다. \n",
      "\n",
      "임영록 회장은 58세, 임종룡 회장은 54세로, 임영록 회장이 더 나이가 많습니다 [[ref2]].\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[700:705], label_lst[700:705]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
