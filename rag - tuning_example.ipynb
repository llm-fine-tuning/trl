{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7956796",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc338ac8-931c-4b20-9057-e987e8227b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.45.1 in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: datasets==3.0.1 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: trl==0.11.1 in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
      "Requirement already satisfied: peft==0.13.0 in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (3.11.11)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.1) (0.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.6.85)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (4.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.4.0\"\n",
    "%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab0492f-1159-4562-bc34-9c0967b950a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec3494",
   "metadata": {},
   "source": [
    "빠른 학습을 위해 학습 데이터와 테스트 데이터를 2:8 비율로 분할합니다. 이 값을 변경하고자 하는 분은 test_ratio의 값을 변경하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5183067f-0a44-4c1d-b5a0-b574c1238771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f163a8eb372e473d89f0300d8929dac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/909 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f93db035ad94c978bfb50b2d8565172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/13.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925a7b762bf842e3b73036013b33ef99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터의 type 분포:\n",
      "synthetic_question: 497\n",
      "mrc_question: 491\n",
      "mrc_question_with_1_to_4_negative: 296\n",
      "paraphrased_question: 196\n",
      "no_answer: 404\n",
      "\n",
      "전체 데이터 분할 결과: Train 380개, Test 1504개\n",
      "\n",
      "학습 데이터의 type 분포:\n",
      "synthetic_question: 100\n",
      "mrc_question: 99\n",
      "mrc_question_with_1_to_4_negative: 60\n",
      "paraphrased_question: 40\n",
      "no_answer: 81\n",
      "\n",
      "테스트 데이터의 type 분포:\n",
      "synthetic_question: 397\n",
      "mrc_question: 392\n",
      "mrc_question_with_1_to_4_negative: 236\n",
      "paraphrased_question: 156\n",
      "no_answer: 323\n"
     ]
    }
   ],
   "source": [
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/klue-mrc-ko-rag-dataset\", split=\"train\")\n",
    "\n",
    "# 2. system_message 정의\n",
    "system_message = \"\"\"당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
    "\n",
    "다음의 지시사항을 따르십시오.\n",
    "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
    "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
    "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
    "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
    "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
    "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
    "\n",
    "검색 결과:\n",
    "-----\n",
    "{search_result}\"\"\"\n",
    "\n",
    "# 3. 원본 데이터의 type별 분포 출력 \n",
    "print(\"원본 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    print(f\"{type_name}: {dataset['type'].count(type_name)}\")\n",
    "\n",
    "# 4. train/test 분할 비율 설정 (0.5면 5:5로 분할)\n",
    "test_ratio = 0.8\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 5. type별로 순회하면서 train/test 데이터 분할\n",
    "for type_name in set(dataset['type']):\n",
    "    # 현재 type에 해당하는 데이터의 인덱스만 추출\n",
    "    curr_type_data = [i for i in range(len(dataset)) if dataset[i]['type'] == type_name]\n",
    "    \n",
    "    # test_ratio에 따라 test 데이터 개수 계산 \n",
    "    test_size = int(len(curr_type_data) * test_ratio)\n",
    "    \n",
    "    # 현재 type의 데이터를 test_ratio 비율로 분할하여 추가\n",
    "    test_data.extend(curr_type_data[:test_size])\n",
    "    train_data.extend(curr_type_data[test_size:])\n",
    "\n",
    "# 6. OpenAI format으로 데이터 변환을 위한 함수 \n",
    "def format_data(sample):\n",
    "    # 검색 결과를 문서1, 문서2... 형태로 포매팅\n",
    "    search_result = \"\\n-----\\n\".join([f\"문서{idx + 1}: {result}\" for idx, result in enumerate(sample[\"search_result\"])])\n",
    "    \n",
    "    # OpenAI format으로 변환\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message.format(search_result=search_result),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"question\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"answer\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 7. 분할된 데이터를 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "# 8. 최종 데이터셋 크기 출력\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")\n",
    "\n",
    "# 9. 분할된 데이터의 type별 분포 출력\n",
    "print(\"\\n학습 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in train_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")\n",
    "\n",
    "print(\"\\n테스트 데이터의 type 분포:\")\n",
    "for type_name in set(dataset['type']):\n",
    "    count = sum(1 for i in test_data if dataset[i]['type'] == type_name)\n",
    "    print(f\"{type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e17067-01c0-45e9-9aa0-b352e82a4901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: “우버(UBER)는 사람들이 이동하는 수단을 발전시켰습니다. 승객과 기사를 실시간으로 연결해 승객에게는 편리함을, 기사에게는 더 많은 효율성과 수익을 가져다 주었습니다. 2010년 6월 미국 샌프란시스코에서 처음 서비스를 시작한 뒤 현재 세계 140여개 도시에 진출했습니다. 우버는 앞으로도 계속 사람과 도시를 가깝게 이어줄 것입니다.”샌프란시스코 우버 본사에서 최근 만난 나이리 후다지안 우버 글로벌커뮤니케이션 부문장은 이렇게 강조했다. 우버는 일종의 ‘차량 예약 서비스’를 제공하는 글로벌 정보기술(IT) 기업이다. 스마트폰 애플리케이션(앱·응용프로그램·사진)을 통해 승객과 차량을 연결해 주는 사업을 하고 있다.승객을 일반택시와 연결해 주는 ‘우버택시’, 일반인이 자신의 차량으로 운송 서비스를 할 수 있도록 도와주는 ‘우버엑스’, 일종의 고급 콜택시인 ‘우버블랙’ 등의 서비스를 갖췄다. 한국에서는 지난해 8월 우버코리아가 설립돼 우버블랙 사업을 펼치고 있다.후다지안 부문장은 “우리는 더 나은 교통 서비스 위해 플랫폼을 제공하는 회사”라고 설명했다. 그는 “택시를 잡기 어려운 지역에서도 우버를 통해 사람들에게 편리한 운송 서비스를 제공할 수 있다”며 “우리는 사람들이 좀 더 신뢰할 수 있고 빠른 교통 서비스를 원한다는 사실을 확인할 수 있었다”고 말했다. 우버는 기본적으로 스마트폰 앱을 통해 차량을 부르고 요금을 내는 구조이기 때문에 매우 빠르고 편리하다는 게 그의 설명이다.그러나 우버는 세계 여러 도시에서 법적 논란도 일으키고 있다. 이달 초에는 런던 파리 베를린 로마 등 유럽 주요 대도시에서 택시기사들이 ‘우버 반대’ 시위를 잇달아 벌이기도 했다. 한국에서도 불법 논란이 확산되는 중이다. 정식 택시회사로 등록돼 있지 않고 렌터카(고급 외제차 등) 등을 이용해 우버블랙 서비스가 이뤄지고 있기 때문이다.하지만 우버 측은 차량과 승객만 연결해 주기 때문에 문제가 없다는 입장이다. 후다지안 부문장은 “새로운 서비스가 나올 때마다 논란은 있게 마련”이라며 “분명한 것은 택시기사들도 우버 서비스를 통할 때 더 많은 수익을 내고 있다는 사실”이라고 강조했다. 샌프란시스코=안정락 기자\\n-----\\n문서2: 차량 공유업체 우버를 통해 승객을 태우는 운전자는 자영업자가 아니라 우버의 피고용인으로 봐야 한다는 심결이 미국 캘리포니아주 노동위원회에서 나왔다. 강제성은 없지만 이를 계기로 우버에 대한 집단소송이 캘리포니아에서 일어날 수 있다. 기름값, 통행료 등 각종 비용은 운전자에게 부담시키고 운전자와 승객을 연결해주며 수수료만 받는 우버의 사업 모델이 흔들릴 수 있다는 전망이다.17일(현지시간) 뉴욕타임스 등에 따르면 캘리포니아주 노동위는 지난해 8주 동안 우버 운전기사로 일한 바버라 앤 버윅에게 우버는 4152달러20센트를 지급하라고 명령했다. 버윅은 지난해 7~9월 1주일에 60~80시간을 일하고 1만1000달러를 벌었다. 하지만 비용과 세금을 제하면 최저임금도 받지 못한 셈이라며 지난해 9월 노동위에 문제를 제기했다. 노동위는 “우버는 단순히 승객과 운전자를 연결하는 중립적인 기술 플랫폼일 뿐이라고 항변했지만, 실제론 서비스가 이뤄지는 모든 부분에 개입했다”고 밝혔다.지난 3일 결정된 이번 심결은 16일 우버가 법원에 소송을 제기하면서 알려졌다. 심결은 강제성이 없으며 버윅 개인에게만 한정된다. 우버는 조지아, 펜실베이니아, 텍사스 등 5개 주는 이미 우버 운전자를 독립 계약자로 분류하고 있다며 이번 심결의 부당함을 호소했다. 반면 플로리다주는 종업원으로 분류해야 한다고 결정해 주마다 의견이 엇갈린다. 다만 캘리포니아주는 우버 본사가 있는 지역인 데다 세계 정보기술(IT)의 중심지여서 각 지역 규제 당국이 이번 심결을 참고할 것으로 전망된다.\\n-----\\n문서3: “우버 사태는 시작일 뿐이에요. 새로운 기술에 바탕을 둔 서비스의 등장으로 기존 법·제도와 마찰을 겪는 일은 앞으로 계속 일어날 것입니다.”자가용 콜택시 알선 서비스인 우버(Uber)에 대해 서울시가 차단 방침을 밝힌 것을 계기로 새로운 혁신 서비스와 기술이 현행 법 등과 충돌하는 문제가 도마에 올랐다. 국내 정보기술(IT) 전문가들은 우버와 같은 혁신 서비스를 기존 제도와 법규 틀 안에서 규제해서는 안 된다고 지적한다. 무인 자동차와 무인 비행체, 공유 숙박, 온라인 부동산 중개 서비스 등 현행 법으로는 수용하기 힘든 서비스들이 계속해서 나타나고 있기 때문이다.이를 무조건 불법으로 규정해 막거나 서울시처럼 공공서비스로 대체해 시장을 빼앗는 것은 민간의 창의와 혁신을 짓밟는 행위라는 주장이다. 오히려 우버 사태를 계기로 혁신 기술과 서비스를 탄력적으로 수용할 수 있도록 법과 제도를 바꿔나가는 논의를 시작해야 한다고 강조했다.○법이 기술 변화 못 따라가최근 우버 논란에 대해 김진형 소프트웨어정책연구소장은 “지금 당장은 우버를 금지할 수 있지만 언제까지 계속 금지할 수는 없을 것”이라고 말했다. 자동차가 처음 등장했을 때도 우마차 업자들이 반대해 자동차도 빨간 깃대를 꽂고 다니도록 한 법을 1865년 제정한 적이 있다는 것이다.윤종수 법무법인 세종 변호사는 “우버는 기존 운송 체계와 다른 비즈니스가 등장한 것인데, 합법이냐 불법이냐보다 그것이 사회적으로 효용이나 가치를 더 많이 주는지 아니면 피해나 문제를 더 많이 발생시키는지 먼저 따져야 한다”고 말했다. 그는 “인터넷이 처음 등장했을 때나 인터넷으로 음악을 들을 수 있게 됐을 때도 마찬가지였다”며 “새로운 가치를 제공해 많은 사람이 받아들이면 결국 규제가 바뀌게 된다”고 덧붙였다.조신 연세대 미래융합기술연구원장은 “어느 나라나 규제가 기술의 변화를 못 좇아가는 게 일반적이지만, 한국은 패러다임 변화에 맞춰 법과 제도가 바뀌는 속도가 너무 느린 감이 있다”며 “특히 제조업에 비해 발전이 늦은 서비스산업을 키우기 위해서라도 새로운 기술에 맞춰 다양한 서비스가 활성화될 수 있는 터를 마련해줄 필요가 있다”고 지적했다. 그는 미국 캘리포니아와 네바다주가 무인 자동차 출현에 대비해 이를 합법화한 게 좋은 사례라고 소개했다.○택시기사에게도 선택권 줘야서울시의 우버 앱 서비스 차단 방침에 대해서는 승객은 물론 택시기사에게 선택권을 줘야 한다는 의견도 있다. 임정욱 스타트업얼라이언스 센터장은 “우버는 택시기사가 아니라 택시회사를 위협하는 서비스”라며 “오히려 택시기사들이 우버를 비롯해 다양한 교통 서비스 회사를 통해 일할 수 있도록 선택권을 보장해야 한다”고 말했다.한상기 소셜컴퓨팅연구소 대표는 “많은 택시운전사의 피해가 예상되는 만큼 우버에 반대하는 의견도 이해가 간다”면서 “우버와 택시 업계, 정책 결정자들이 머리를 맞대고 논의해 모두에게 혜택이 돌아갈 수 있도록 조율하는 과정이 필요하다”고 말했다.김진형 소장은 “산업혁명기에 기계를 부수는 러다이트 운동이 일어났지만 결국 기술의 발전을 막을 수 없었다”며 “국가의 역할은 새로운 기술에 빠르게 적응하도록 직업교육을 강화하는 것이지 무조건 막는 게 아니다”고 강조했다.\\n-----\\n문서4: 우버는 금일부터 우버택시 서비스에 앱 결제 시스템을 적용한다고 발표했다. 기존에 우버블랙에 탑재되어 시행되고 있는 앱 결제 시스템이 모든 우버택시에 확대 적용되는 것으로, 우버는 앱 결제 시스템을 기반으로 더욱 편리한 서비스를 제공해 이용객 편의를 강화한다는 방침이다. 신규 결제 시스템 적용으로 우버택시 이용객은 탑승에 앞서 예상 요금을 확인할 수 있으며, 앱에 등록된 카드로만 우버택시 이용 요금을 결제할 수 있다. 특히, 코로나로 안전에 대한 경각심이 높아진 가운데, 앱 결제 시스템 상에서 비대면으로 결제가 진행되므로 승객들은 안심하고 우버택시를 이용할 수 있다. 드라이버와 승객 간에 현금을 주고받고, 거스름돈을 받는 등 결제를 위한 대기 시간이 사라지고, 통행요금은 자동으로 합산되며, 앱을 통해 탑승 및 운임 내역 등을 언제 어디서나 손쉽게 확인 가능하다. 앱 결제 시스템을 이용하기 위해서는 우버 앱 상에 국내 발급 카드 등록이 필요하다. 카드 등록 절차는 간단하다. 먼저 우버 앱에 접속해 화면 좌측 상단의 메뉴 버튼을 눌러 결제[Wallet]를 선택한다. 결제 수단 아래 [결제 수단 추가] 버튼을 거쳐 [신용카드 또는 체크카드] 버튼을 선택, 카드 정보를 입력하면 등록 절차가 완료된다. 앱 결제 시스템은 우버블랙을 포함, 국내 시장에서 이미 널리 사용되는 결제 방법이지만, 해당 시스템이 익숙하지 않은 이들도 원활하게 서비스를 이용할 수 있도록 우버는 앱 내 ‘고객 지원’ 문의 기능을 적극 활용해 도움이 필요한 이들을 신속하게 지원할 예정이다. 우버는 신규 시스템으로 이용객 편의를 제고하는 동시에 다양한 안전 기능을 통해 서비스 안전 강화 정책을 펼치고 있다. 우버는 지난 4월, 스마트폰에 탑재된 GPS 및 센서 기술을 바탕으로 국내 서비스에 ‘운행 상황 확인’ 기능을 도입했다. 해당 기능은 우버 차량에 예기치 못한 정차가 발생했을 경우 혹은 예정 경로에서 벗어나거나 사고가 의심되는 상황 등을 감지해 앱 상에서 드라이버와 승객 모두에게 안전 여부를 확인하는 메시지를 전송한다. 문제가 발생했을 경우, 우버 이용객들은 긴급 버튼을 눌러 도움을 요청할 수 있으며, 우버 안전 관리부(Safety Line)에 문제가 생겼다고 신고할 수 있다. 또한, 운행 상황 확인 기능 도입과 함께 기존의 피드백 기능을 크게 개선해 우버 이용객들이 단순히 별점으로 서비스 평가로 매기는 것이 아니라, 소음, 운전, 운행 경로 등에 대해 상세한 의견을 남길 수 있도록 설계했다. 이 외에도 우버는 최대 장점인 기술력을 적극 활용해 안전에 중점을 둔 다수의 기능을 선보이며 안전 강화 행보를 이어나가고 있다. 눈여겨볼 주요 안전 기능으로는 호출 차량이 맞는지 확인을 도와주는 ‘당신의 여정이 맞는지 확인하세요(Check Your Ride)’ 인앱 기능, 4자리 핀(PIN) 인증 시스템인 ‘요청 차량 확인’ 기능, ‘색상으로 승객 찾기(Spotlight)’ 기능 등이 있다. 우버코리아 톰 화이트(Tom White) 한국 총괄은 “우버는 안전을 최우선 과제로 삼고, 지속되는 글로벌 위기와 변화에 빠르게 대응하기 위해 전염병 전문학자를 비롯 최고 전문가들에게 지속적으로 자문을 구하고 공조하고 있다”라며, “앞서 선보인 우버만의 기술력을 활용한 다양한 기능과 함께 한국에서도 더욱 편리하게 앱을 이용할 수 있도록 앞으로도 최선을 다하겠다”라고 덧붙였다.\\n-----\\n문서5: 우버는 연말연시 안전한 우버 모빌리티 서비스 이용을 위해 꼭 알아두어야 할 여러가지 안전 기능을 소개했다. 우선 우버는 작년 처음 선보인 112로 전화가 연결되는 \\'112 지원 버튼\\' 옵션에 문자 기능을 새롭게 추가한다고 밝혔다. 위급상황 발생시 문자 기능을 탑재한 ‘112 지원 버튼’을 누르면 우버 탑승자 휴대폰 내에 차량의 종류, 모델, 번호판 등 주요 정보를 담은 문자가 자동 생성되며, 전송을 누르면 112 상황실로 문자가 전송된다. 이번 추가 기능을 토대로 우버는 보다 더 안전성을 강화해 나갈 예정이다. 우버는 2018년 앱 상에서 드라이버와 탑승자 모두가 이용할 수 있는 안전 기능으로 ‘112 지원 버튼’을 도입한 바 있으며, 이용자 안전성 및 편의성을 강화하기 위한 노력에 맞춰 앱 상에 지속적으로 다양한 기능들을 추가하고 있다. 그간 우버가 선보인바 있거나 도입하기 위해 추진하고 있는 기능으로는 ‘안심 연락처’, ‘안전 도구 툴킷(Safety Toolkit)’, ‘색상으로 승객 찾기(Spotlight)’, ‘PIN 인증 시스템’, ‘드라이버 신원 확인 시스템’ 강화 등이 있다. 먼저, 작년 ‘112 지원 버튼’의 도입과 함께 적용된 ‘안심 연락처’ 기능은 우버 이용자가 최대 5명까지 지인의 연락처를 미리 앱에 등록해 차량 탑승시 등록되어 있는 연락처로 예상 도착 시간을 포함, 실시간 위치 정보를 공유할 수 있도록 지원한다. 아울러 해당 기능은 이용자 의사에 따라 야간에만 활성화할 수 있는 별도 옵션과 함께 제공된다. 나아가, 탑승자들은 안전에 관한 다양한 정보를 한눈에 보여주는 안전 도구 툴킷 메뉴를 이용해 경찰 당국의 지원으로 제공되는 안전 팁 외에도 드라이버 경력 조회, 보험 보장 정보 등 각종 커뮤니티 가이드라인을 확인할 수 있다. 그 밖에 우버 탑승자들은 ‘색상으로 승객 찾기’ 기능을 통해서 앱에서 차량을 호출한 뒤 해당 버튼을 눌러 핸드폰 전체 화면을 지정된 밝은 색으로 변경할 수 있다. 드라이버에게는 색상 정보를 제공하는 인앱 메시지가 전송되어 탑승자들이 핸드폰 화면을 도로 쪽으로 보이게 들면 드라이버가 탑승자를 쉽게 찾을 수 있다. 이에 멈추지 않고 우버는 4자리 PIN 인증 시스템 도입을 추진하고 있다. 이는 탑승자가 잘못된 차량에 탑승하는 일이 없도록 설계되었다. 탑승자들은 원할시 차량 탑승 전 4자리 고유 핀 번호를 부여받을 수 있다. 드라이버는 탑승자에게 번호를 전달받아 앱 상에 입력할 경우에만 출발이 가능하다. 이와 더불어 우버는 초음파를 사용해 PIN 인증 없이도 올바른 차량 확인이 가능한 기술을 개발 중에 있다. 우버 운전기사 신원확인 시스템 또한 개선되었다. 기존 방식에서는 운전자가 주기적으로 셀카를 찍어 등록된 사진과 동일 여부를 검증했지만 앞으로는 셀카를 찍으면서도 머리를 움직이고 미소까지 지어야 인증이 완료된다. 한층 강화된 보안 기능으로 승객의 안전성을 최대한 보장하겠다는 의도가 담긴 발전이다. 마지막으로, 우버는 휴일에 공항, 기차역 등으로 몰려드는 많은 여행객들로 인해 교통이 혼잡할 것을 대비해 우버 이용객들에게 ‘당신의 여정이 맞는지 확인하세요(Check Your Ride)’ 인앱 기능 사용을 권장하고 있다. 해당 기능을 통해 우버 이용객들은 배정된 차량 및 드라이버가 맞는지 확인하기 위해 필요한 드라이버 사진, 차량 번호판, 차량 종류 및 모델을 비롯한 세부 사항을 안내받을 수 있다.'},\n",
       " {'role': 'user',\n",
       "  'content': '캘리포니아주 외에 독립 계약자로 우버 운전자를 분류하는 다른 3개 주의 구체적인 이유는 무엇인가요?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '검색 결과에는 캘리포니아주 외에 우버 운전자를 독립 계약자로 분류하는 다른 3개 주와 그 이유에 대한 구체적인 정보를 찾을 수 없습니다.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e561d8f-7a5c-4d50-a030-a31ecbe7853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205c38e8-e863-446d-b1f6-e9c587138276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\\n\\n다음의 지시사항을 따르십시오.\\n1. 질문과 검색 결과를 바탕으로 답변하십시오.\\n2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\\n3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\\n4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\\n5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\\n6. 최대한 다수의 문서를 인용하여 답변하십시오.\\n\\n검색 결과:\\n-----\\n문서1: 도시 농업의 경우, 세계 각지에서 지구 온난화 문제, 대기의 건조 및 미중 무역 분쟁, 한일 경제 전쟁 등의 여파에 따라 계속 발생되는 아마존 우림 산불 등과 같은 각종 자연재해 등이 지속적으로 일으키게 되면서, 기후 변화로 인한 생태계 파괴 등을 막을 수 있는 조건도 있고 자발적인 생활 패턴이 생기게 되는 대안을 제시할 수 있게 되는 이점이 있다. 그러나 도시 농업을 활성화하게 되면 숲과 같은 삼림 보호 정책은 물론 바다나 개펄 등 해양 생태계 보호 정책까지 대폭적으로 강화할 수 있는 등 다양한 니즈를 충족시킬 수 있게 된다. 기존에는 삼림 벌채 후 농경지 조성을 하였지만, 건물 최상층, 중간 피난층, 주상복합아파트 같은 주거 및 상업시설 복합 건축물의 상업 지역 부분 옥상 등지에 농지를 가꾸어져서 건물에 상주하고 있는 분들과 주거 시설 거주민들이 1년 동안 안정적으로 식량 자원을 확보할 수 있게 되는 좋은 점이 있다. 이미 EBS TV의 하나뿐인 지구라는 교양 프로그램에도 이와 같은 배경을 소재로 둔 이야기를 토대로 보면, 기후 변화 대응 체계 해소 취지를 앞장서는 여건이 마련된다. 그렇게 되면 훌륭한 조건을 가질 수 있어, 물 부족 국가 또는 강수량이 낮은 지역에게는 새로운 희망이자 기회를 얻게 되는 셈이다.\\n-----\\n문서2: 지속가능성과 안보 관점에서 봤을 때, 여러 장비나 기기에 사용되는 에너지원은 일원화될수록 좋다. 따라서 난방, 운송 부문의 에너지원을 전기로 통합한다면 재생에너지로 생산된 전력을 여러 부문에 효율적으로 사용함으로써 전체 에너지 사용량을 줄일 수 있을 것이다. 이러한 ‘섹터간 융합’은 다양한 기기의 에너지원을 전력으로 바꾸는 ‘전기화’를 통해 달성할 수 있다. 이에 더해 전기를 사용하는 기기는 작동 과정에서 온실가스를 배출하지 않으므로 발전 섹터만 탈탄소화할 수 있다면 에너지 시스템 전체의 탄소배출량을 크게 줄일 수 있다.\\n\\n열펌프가 대표적인 사례다. 열펌프는 전기를 이용해 온도가 낮은 곳에서 높은 곳으로 열을 이동시키는 장치로, 현재 상용화된 기술 중 단일 기술로는 온실가스 배출에 가장 크게 기여할 것으로 기대된다. 냉온장고나 냉난방기를 건물 단위로 적용하는 개념의 기술이라고 볼 수 있다. 열펌프 기술이 적용된 국제에너지기구(IEA)는 화석연료를 사용하는 건물 난방시스템 중 30%만 열펌프로 교체해도 연간 8% 가량의 탄소배출량을 줄일 수 있을 것으로 내다봤다. 화석연료를 이용한 보일러처럼 배출가스가 생성되지도 않으므로 대기오염물질을 배출하지 않는다는 것도 장점이다. 열펌프와 마찬가지로 전기자동차 역시 에너지 이용 효율을 높이는 동시에 대기오염물질 배출을 줄이는 데 유용하다.\\n\\n한편, 에너지를 소모하는 과정에서는 필연적으로 열에너지 형태의 손실이 발생한다. 손실열은 별도의 일을 하지 않고 버려지는 에너지다. 따라서 이러한 에너지를 회수해 다시 사용할 수 있다면 에너지시스템 전반의 효율을 크게 높일 수 있다. 완전한 탈탄소 에너지체계로 이행하는 데 과도기적인 형태인 복합화력이 대표적인 사례다. 화력발전소는 최대 40% 남짓한 효율을 낸다. 투입한 연료의 에너지 중 40% 정도만 전력으로 변환할 수 있다는 뜻이다. 나머지 60%의 에너지는 열에너지 형태로 손실된다. 이렇게 손실되는 열에너지를 도시의 블록 단위로 온수를 공급하는 지역난방에 활용하면 효율을 70%대까지 끌어올릴 수 있다. 복합화력의 연료로는 폐기물을 소각하는 과정에서 발생하는 열을 주로 사용하고 있지만 태양열이나 지열을 이용한 방식을 지역난방에 적용하는 방안도 활발하게 연구되고 있다.\\n\\n운송 부문의 전기화 또한 중요한 요소다. 이미 지하철을 비롯한 철도망이 상당 부분 전기화됐으며 도로 교통에서도 전기차가 빠르게 보급되고 있다. 다만 전기차의 핵심 부품인 배터리를 생산하는 과정에서 온실가스가 적지 않게 배출된다는 점을 고려해야 한다. 전기차에 사용되는 전기를 화력발전으로 생산할 경우, 차량의 전체 생애주기에서 전기차가 내연기관차보다 더 많은 온실가스를 생성한다. 이는 차량의 전기화가 재생에너지로의 전환과 함께 추진돼야 함을 뜻한다. 2009년 기준 유럽의 전력 포트폴리오를 바탕으로 계산해보면 전기차의 탄소배출량이 내연기관에 비해 탄소 배출량이 최대 56% 적은 것으로 나타난다.\\n\\n다만 에너지전환의 다른 요소에 비해 운송 부문의 전기화에는 오랜 시간이 걸릴 것으로 보인다. 현재 시점에서 비행기와 대형 화물 트럭, 선박의 전기화는 어려운 편이다. 이들 운송수단은 대출력이 필요해서 장착해야 하는 배터리의 용량도 큰데, 배터리의 무게로 인해 기체 중량이 지나치게 증가하므로 효율성이 낮아지기 때문이다. 대출력이 필요한 운송수단에는 대안으로 전기로 생산한 바이오연료나 수소를 이용하는 방안이 연구 중이다.\\n-----\\n문서3: 미래 청정에너지 개발에 공동 협력하기 위한 ‘2014 월드그린에너지포럼’이 22~24일 경북 경주시 힐튼호텔에서 열린다.한국경제신문과 경상북도·경주시가 주최하는 이번 포럼에는 ‘그린에너지 패러다임의 대변환’이란 주제로 30여개국 정·관·학·산업계 관계자 3000여명이 참석한다. 첫째날과 둘째날에는 참석자들이 태양광 원자력 정보통신기술 에너지저장 연료전지 등 5개 분과로 나눠 세미나를 연다.2010년 노벨경제학상 수상자인 크리스토퍼 피사리데스 런던정치경제대 교수와 이스마일 엘지줄리 기후 변화에 관한 정부 간 협의체 부의장이 연사로 참석한다. 경상북도는 지방자치단체 가운데 처음으로 개발도상국 고위급 인사 및 유엔산업개발기구 등 국제기구와 함께 개도국의 에너지 협력을 증진하고 기후 변화 대응 방안을 논의한다. 마지막날에는 한국원자력환경공단 등 산업현장을 방문하고 야간에는 경주지역 관광지를 둘러보는 나이트 투어도 계획돼 있다.포럼은 폐회식에서 지구촌의 에너지 빈곤 퇴치와 미래 청정에너지 개발 노력, 세계의 공동 번영 및 지속 가능한 개발을 추구하자는 내용을 담은 경주선언문을 채택한다.\\n-----\\n문서4: 국립종자원은 이달 초 제주도에 지원을 개설했다. 아열대 작물을 개량해 특허권을 확보하는 게 설립 목적이다. 한반도 기후가 아열대성으로 변하면서 중남미에서 나는 국화과 식용열매 아티초크나 남아프리카의 채소 오크라 등의 산지가 한국으로 바뀌었다. 전남·경남 해안까지 아열대 작물의 노지재배가 가능해지면서 매년 100종이 넘는 아열대 작물 특허가 출원되고 있기도 하다. 한반도의 아열대화는 녹차밭을 전남 보성에서 강원 고성까지 북상시키는 등 농산물 지도를 바꿔 놓고 있다.○사철 수확하는 아열대 작물열대 작물인 망고, 용과 등은 제주도 비닐하우스에서 재배된다. 겨울철에만 난방을 하면 무리없이 키울 수 있다. 브로콜리같이 생긴 열매를 먹는 아티초크는 겨울철에도 수확이 가능하다.제주도뿐 아니라 전남 일대도 아열대 작물 재배 면적이 늘어나고 있다. 전남농업기술원에 따르면 2009년 38만㎡이던 전남의 블루베리 농장 규모는 지난해 131만㎡로 커졌다. 수입에만 의존하던 아열대 채소류 오크라, 인디언시금치 등은 해남·강진·장흥 일대에 작년부터 12만㎡ 규모로 재배되기 시작했다.이성주 국립종자원 제주지원장은 “2007년 이후 아열대 작물 신품종 특허출원 신청 건수가 꾸준히 늘어 최근에는 연간 30여개 작물 100여개 품종으로 증가했다”고 말했다. 김천환 농촌진흥청 온난화대응센터 연구원은 “제주 전남 경남 해안가는 이미 아열대 기후로 분류하고 있다”고 설명했다. 국내에서 재배되는 아열대 작물은 주로 고급 레스토랑에 납품되거나 인터넷몰 ‘아시아마트’ 등을 통해 외국인들에게 판매된다. 최근에는 국내 소비자들의 수요도 증가해 백화점에서도 국내산 아열대 작물을 팔기 시작했다. 신세계백화점은 올 들어 제주산 ‘패션프루트’ 판매를 시작했다. 애플망고와 용과 판매량은 전년 대비 20% 늘었다. 갤러리아백화점의 식품관 고메이494에서는 이달 들어 제주산 ‘아테모야(슈거애플)’를 선보였다. 한 박스(3㎏)에 20만원으로 가격은 좀 비싸지만 마니아층이 두터워지고 있다는 설명이다.○청주까지 올라온 한라봉사과 등 주요 작물의 산지는 북상 중이다. 제주 특산물로 유명한 한라봉과 감귤은 이미 충북 청주에서도 생산된다. 대구 등이 주 산지인 사과는 경기 포천, 강원 영월 등에서도 많이 재배되고 있다. ‘보성녹차’로 유명한 전남 보성은 녹차의 주 산지를 강원 고성에 내줄 판이다.수산물도 심상치 않다. 10월이 제철인 찬물에 사는 낙지는 바다수온이 오르면서 올 들어 수확량이 대폭 감소했다. 반대로 따뜻한 물을 좋아하는 꽃게는 올해 풍년이다. 연평도의 올해 꽃게 어획량(900t)은 작년보다 67% 늘었다. 해양수산부에 따르면 2000년대 남해안의 평균 수온은 19.2도로 1970년대에 비해 0.8도 올랐다. 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있다. 반면 수온이 낮아야 잘 자라는 김 미역 다시마 등은 남해안에서 수확하기가 어려워졌다는 분석이다.\\n-----\\n문서5: 서울 강서구 마곡지구에 여의도공원 2배 규모의 아시아 최대 생태공원(조감도)이 조성된다. 서울시는 마곡지구에 5000여종의 식물을 갖춘 도시형 식물원인 ‘서울 화목원(花木園)’(가칭)을 2016년 12월까지 준공할 예정이라고 21일 발표했다. 서울 서남권의 첫 대형 공원이자 서울의 ‘녹색 허브’ 역할을 할 것으로 기대된다. 산업단지(190만㎡)에 입주하는 기업 연구시설에 이어 주거단지(106만㎡) 내 아파트 일반분양이 본격화되는 마곡지구에 호재로 작용할 전망이다.서울시에 따르면 기존의 대규모 택지개발지구에 조성되는 공원이 거주자들의 여가·휴양 기능에 초점을 맞췄다면 서울화목원은 인근 마곡첨단산업지구(산업)와 경계를 허물고 옛 마곡의 풍광을 재현하는 데 치중한다. 겸재 정선의 ‘종해청조(宗海廳潮)’에 나타난 옛 양천 지역의 모습을 재현하기 위해 인공 시설물을 최소화하고 물재생센터의 재생수와 빗물을 공원 호수로 활용하는 것이 특징이다. 전체 면적은 50만3431㎡로 여의도 공원(23만㎡)의 2배가 넘고 어린이대공원(56만㎡)보다 약간 작다. 화목원에는 6만㎡ 규모 식물원과 식물문화센터가 들어선다. 식물문화센터는 3개 건물(연면적 1만㎡)에 전시온실, 식물도서관, 가드닝센터 등이 갖춰진다. 시는 다음달 중 관계기관 협의를 거쳐 기본계획안을 확정하고 설계에 들어갈 예정이다.',\n",
       "   'role': 'system'},\n",
       "  {'content': '기후 변화가 농업에 미치는 영향은 무엇인가요?', 'role': 'user'},\n",
       "  {'content': '기후 변화는 농업에 다양한 영향을 미치고 있습니다. 기후 변화로 인해 농업 환경이 변화하면서 작물 재배 지역과 방식이 달라지고 있습니다.\\n\\n첫째, 기후 변화로 인해 한반도와 같은 지역에서는 아열대 작물의 재배가 가능해지고 있습니다. 예를 들어, 한반도의 기후가 아열대성으로 변하면서 중남미에서 나는 아티초크나 남아프리카의 오크라 등의 작물이 한국에서도 재배되고 있습니다. 이는 농산물 지도를 바꾸고 있으며, 제주도와 전남 일대에서 아열대 작물 재배 면적이 늘어나고 있습니다. 또한, 제주도에서 재배되던 한라봉과 감귤이 충북 청주에서도 생산되고 있으며, 사과의 주 산지도 북상하고 있습니다[[ref4]].\\n\\n둘째, 기후 변화는 수산물에도 영향을 미치고 있습니다. 바다 수온이 오르면서 찬물에 사는 낙지의 수확량은 감소하고, 따뜻한 물을 좋아하는 꽃게의 어획량은 증가하고 있습니다. 또한, 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있습니다. 반면, 수온이 낮아야 잘 자라는 김, 미역, 다시마 등의 수확은 어려워지고 있습니다[[ref4]].\\n\\n이와 같이 기후 변화는 농업에 큰 영향을 미치며, 작물 재배 지역의 변화와 새로운 작물의 도입, 수산물의 어획량 변화 등을 초래하고 있습니다. 이는 농업 생산성과 식량 안보에 중요한 영향을 미칠 수 있습니다.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33536c0",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb5f299a-e0ca-407f-ae21-0f0e5fadeac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59339012f85048ffb0f14ea0334d7d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68df8d307ee49158bf69300834c41bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c9ca6afea1485c8a249ec17ef7a242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7787747d75496595b72cf1bb4b40cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea270641346405db5aa955031567028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31e3d5212ba446181118db4588d4ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b07b0372e9c486cb69ece031e138798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68229a3e4be348afa40460424e0daf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f80f770c824f87a252e99f30052b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1624f35b3b42478b806e3983de2d42dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73ce9076601499886f115cee9d22aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f5220e78f24e69825374a68e56e4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13034641787b4becb110042e029dde74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-7B-Instruct\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8df8462-72af-4ad4-b1ed-06a84dd6086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: 도시 농업의 경우, 세계 각지에서 지구 온난화 문제, 대기의 건조 및 미중 무역 분쟁, 한일 경제 전쟁 등의 여파에 따라 계속 발생되는 아마존 우림 산불 등과 같은 각종 자연재해 등이 지속적으로 일으키게 되면서, 기후 변화로 인한 생태계 파괴 등을 막을 수 있는 조건도 있고 자발적인 생활 패턴이 생기게 되는 대안을 제시할 수 있게 되는 이점이 있다. 그러나 도시 농업을 활성화하게 되면 숲과 같은 삼림 보호 정책은 물론 바다나 개펄 등 해양 생태계 보호 정책까지 대폭적으로 강화할 수 있는 등 다양한 니즈를 충족시킬 수 있게 된다. 기존에는 삼림 벌채 후 농경지 조성을 하였지만, 건물 최상층, 중간 피난층, 주상복합아파트 같은 주거 및 상업시설 복합 건축물의 상업 지역 부분 옥상 등지에 농지를 가꾸어져서 건물에 상주하고 있는 분들과 주거 시설 거주민들이 1년 동안 안정적으로 식량 자원을 확보할 수 있게 되는 좋은 점이 있다. 이미 EBS TV의 하나뿐인 지구라는 교양 프로그램에도 이와 같은 배경을 소재로 둔 이야기를 토대로 보면, 기후 변화 대응 체계 해소 취지를 앞장서는 여건이 마련된다. 그렇게 되면 훌륭한 조건을 가질 수 있어, 물 부족 국가 또는 강수량이 낮은 지역에게는 새로운 희망이자 기회를 얻게 되는 셈이다.\n",
      "-----\n",
      "문서2: 지속가능성과 안보 관점에서 봤을 때, 여러 장비나 기기에 사용되는 에너지원은 일원화될수록 좋다. 따라서 난방, 운송 부문의 에너지원을 전기로 통합한다면 재생에너지로 생산된 전력을 여러 부문에 효율적으로 사용함으로써 전체 에너지 사용량을 줄일 수 있을 것이다. 이러한 ‘섹터간 융합’은 다양한 기기의 에너지원을 전력으로 바꾸는 ‘전기화’를 통해 달성할 수 있다. 이에 더해 전기를 사용하는 기기는 작동 과정에서 온실가스를 배출하지 않으므로 발전 섹터만 탈탄소화할 수 있다면 에너지 시스템 전체의 탄소배출량을 크게 줄일 수 있다.\n",
      "\n",
      "열펌프가 대표적인 사례다. 열펌프는 전기를 이용해 온도가 낮은 곳에서 높은 곳으로 열을 이동시키는 장치로, 현재 상용화된 기술 중 단일 기술로는 온실가스 배출에 가장 크게 기여할 것으로 기대된다. 냉온장고나 냉난방기를 건물 단위로 적용하는 개념의 기술이라고 볼 수 있다. 열펌프 기술이 적용된 국제에너지기구(IEA)는 화석연료를 사용하는 건물 난방시스템 중 30%만 열펌프로 교체해도 연간 8% 가량의 탄소배출량을 줄일 수 있을 것으로 내다봤다. 화석연료를 이용한 보일러처럼 배출가스가 생성되지도 않으므로 대기오염물질을 배출하지 않는다는 것도 장점이다. 열펌프와 마찬가지로 전기자동차 역시 에너지 이용 효율을 높이는 동시에 대기오염물질 배출을 줄이는 데 유용하다.\n",
      "\n",
      "한편, 에너지를 소모하는 과정에서는 필연적으로 열에너지 형태의 손실이 발생한다. 손실열은 별도의 일을 하지 않고 버려지는 에너지다. 따라서 이러한 에너지를 회수해 다시 사용할 수 있다면 에너지시스템 전반의 효율을 크게 높일 수 있다. 완전한 탈탄소 에너지체계로 이행하는 데 과도기적인 형태인 복합화력이 대표적인 사례다. 화력발전소는 최대 40% 남짓한 효율을 낸다. 투입한 연료의 에너지 중 40% 정도만 전력으로 변환할 수 있다는 뜻이다. 나머지 60%의 에너지는 열에너지 형태로 손실된다. 이렇게 손실되는 열에너지를 도시의 블록 단위로 온수를 공급하는 지역난방에 활용하면 효율을 70%대까지 끌어올릴 수 있다. 복합화력의 연료로는 폐기물을 소각하는 과정에서 발생하는 열을 주로 사용하고 있지만 태양열이나 지열을 이용한 방식을 지역난방에 적용하는 방안도 활발하게 연구되고 있다.\n",
      "\n",
      "운송 부문의 전기화 또한 중요한 요소다. 이미 지하철을 비롯한 철도망이 상당 부분 전기화됐으며 도로 교통에서도 전기차가 빠르게 보급되고 있다. 다만 전기차의 핵심 부품인 배터리를 생산하는 과정에서 온실가스가 적지 않게 배출된다는 점을 고려해야 한다. 전기차에 사용되는 전기를 화력발전으로 생산할 경우, 차량의 전체 생애주기에서 전기차가 내연기관차보다 더 많은 온실가스를 생성한다. 이는 차량의 전기화가 재생에너지로의 전환과 함께 추진돼야 함을 뜻한다. 2009년 기준 유럽의 전력 포트폴리오를 바탕으로 계산해보면 전기차의 탄소배출량이 내연기관에 비해 탄소 배출량이 최대 56% 적은 것으로 나타난다.\n",
      "\n",
      "다만 에너지전환의 다른 요소에 비해 운송 부문의 전기화에는 오랜 시간이 걸릴 것으로 보인다. 현재 시점에서 비행기와 대형 화물 트럭, 선박의 전기화는 어려운 편이다. 이들 운송수단은 대출력이 필요해서 장착해야 하는 배터리의 용량도 큰데, 배터리의 무게로 인해 기체 중량이 지나치게 증가하므로 효율성이 낮아지기 때문이다. 대출력이 필요한 운송수단에는 대안으로 전기로 생산한 바이오연료나 수소를 이용하는 방안이 연구 중이다.\n",
      "-----\n",
      "문서3: 미래 청정에너지 개발에 공동 협력하기 위한 ‘2014 월드그린에너지포럼’이 22~24일 경북 경주시 힐튼호텔에서 열린다.한국경제신문과 경상북도·경주시가 주최하는 이번 포럼에는 ‘그린에너지 패러다임의 대변환’이란 주제로 30여개국 정·관·학·산업계 관계자 3000여명이 참석한다. 첫째날과 둘째날에는 참석자들이 태양광 원자력 정보통신기술 에너지저장 연료전지 등 5개 분과로 나눠 세미나를 연다.2010년 노벨경제학상 수상자인 크리스토퍼 피사리데스 런던정치경제대 교수와 이스마일 엘지줄리 기후 변화에 관한 정부 간 협의체 부의장이 연사로 참석한다. 경상북도는 지방자치단체 가운데 처음으로 개발도상국 고위급 인사 및 유엔산업개발기구 등 국제기구와 함께 개도국의 에너지 협력을 증진하고 기후 변화 대응 방안을 논의한다. 마지막날에는 한국원자력환경공단 등 산업현장을 방문하고 야간에는 경주지역 관광지를 둘러보는 나이트 투어도 계획돼 있다.포럼은 폐회식에서 지구촌의 에너지 빈곤 퇴치와 미래 청정에너지 개발 노력, 세계의 공동 번영 및 지속 가능한 개발을 추구하자는 내용을 담은 경주선언문을 채택한다.\n",
      "-----\n",
      "문서4: 국립종자원은 이달 초 제주도에 지원을 개설했다. 아열대 작물을 개량해 특허권을 확보하는 게 설립 목적이다. 한반도 기후가 아열대성으로 변하면서 중남미에서 나는 국화과 식용열매 아티초크나 남아프리카의 채소 오크라 등의 산지가 한국으로 바뀌었다. 전남·경남 해안까지 아열대 작물의 노지재배가 가능해지면서 매년 100종이 넘는 아열대 작물 특허가 출원되고 있기도 하다. 한반도의 아열대화는 녹차밭을 전남 보성에서 강원 고성까지 북상시키는 등 농산물 지도를 바꿔 놓고 있다.○사철 수확하는 아열대 작물열대 작물인 망고, 용과 등은 제주도 비닐하우스에서 재배된다. 겨울철에만 난방을 하면 무리없이 키울 수 있다. 브로콜리같이 생긴 열매를 먹는 아티초크는 겨울철에도 수확이 가능하다.제주도뿐 아니라 전남 일대도 아열대 작물 재배 면적이 늘어나고 있다. 전남농업기술원에 따르면 2009년 38만㎡이던 전남의 블루베리 농장 규모는 지난해 131만㎡로 커졌다. 수입에만 의존하던 아열대 채소류 오크라, 인디언시금치 등은 해남·강진·장흥 일대에 작년부터 12만㎡ 규모로 재배되기 시작했다.이성주 국립종자원 제주지원장은 “2007년 이후 아열대 작물 신품종 특허출원 신청 건수가 꾸준히 늘어 최근에는 연간 30여개 작물 100여개 품종으로 증가했다”고 말했다. 김천환 농촌진흥청 온난화대응센터 연구원은 “제주 전남 경남 해안가는 이미 아열대 기후로 분류하고 있다”고 설명했다. 국내에서 재배되는 아열대 작물은 주로 고급 레스토랑에 납품되거나 인터넷몰 ‘아시아마트’ 등을 통해 외국인들에게 판매된다. 최근에는 국내 소비자들의 수요도 증가해 백화점에서도 국내산 아열대 작물을 팔기 시작했다. 신세계백화점은 올 들어 제주산 ‘패션프루트’ 판매를 시작했다. 애플망고와 용과 판매량은 전년 대비 20% 늘었다. 갤러리아백화점의 식품관 고메이494에서는 이달 들어 제주산 ‘아테모야(슈거애플)’를 선보였다. 한 박스(3㎏)에 20만원으로 가격은 좀 비싸지만 마니아층이 두터워지고 있다는 설명이다.○청주까지 올라온 한라봉사과 등 주요 작물의 산지는 북상 중이다. 제주 특산물로 유명한 한라봉과 감귤은 이미 충북 청주에서도 생산된다. 대구 등이 주 산지인 사과는 경기 포천, 강원 영월 등에서도 많이 재배되고 있다. ‘보성녹차’로 유명한 전남 보성은 녹차의 주 산지를 강원 고성에 내줄 판이다.수산물도 심상치 않다. 10월이 제철인 찬물에 사는 낙지는 바다수온이 오르면서 올 들어 수확량이 대폭 감소했다. 반대로 따뜻한 물을 좋아하는 꽃게는 올해 풍년이다. 연평도의 올해 꽃게 어획량(900t)은 작년보다 67% 늘었다. 해양수산부에 따르면 2000년대 남해안의 평균 수온은 19.2도로 1970년대에 비해 0.8도 올랐다. 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있다. 반면 수온이 낮아야 잘 자라는 김 미역 다시마 등은 남해안에서 수확하기가 어려워졌다는 분석이다.\n",
      "-----\n",
      "문서5: 서울 강서구 마곡지구에 여의도공원 2배 규모의 아시아 최대 생태공원(조감도)이 조성된다. 서울시는 마곡지구에 5000여종의 식물을 갖춘 도시형 식물원인 ‘서울 화목원(花木園)’(가칭)을 2016년 12월까지 준공할 예정이라고 21일 발표했다. 서울 서남권의 첫 대형 공원이자 서울의 ‘녹색 허브’ 역할을 할 것으로 기대된다. 산업단지(190만㎡)에 입주하는 기업 연구시설에 이어 주거단지(106만㎡) 내 아파트 일반분양이 본격화되는 마곡지구에 호재로 작용할 전망이다.서울시에 따르면 기존의 대규모 택지개발지구에 조성되는 공원이 거주자들의 여가·휴양 기능에 초점을 맞췄다면 서울화목원은 인근 마곡첨단산업지구(산업)와 경계를 허물고 옛 마곡의 풍광을 재현하는 데 치중한다. 겸재 정선의 ‘종해청조(宗海廳潮)’에 나타난 옛 양천 지역의 모습을 재현하기 위해 인공 시설물을 최소화하고 물재생센터의 재생수와 빗물을 공원 호수로 활용하는 것이 특징이다. 전체 면적은 50만3431㎡로 여의도 공원(23만㎡)의 2배가 넘고 어린이대공원(56만㎡)보다 약간 작다. 화목원에는 6만㎡ 규모 식물원과 식물문화센터가 들어선다. 식물문화센터는 3개 건물(연면적 1만㎡)에 전시온실, 식물도서관, 가드닝센터 등이 갖춰진다. 시는 다음달 중 관계기관 협의를 거쳐 기본계획안을 확정하고 설계에 들어갈 예정이다.<|im_end|>\n",
      "<|im_start|>user\n",
      "기후 변화가 농업에 미치는 영향은 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "기후 변화는 농업에 다양한 영향을 미치고 있습니다. 기후 변화로 인해 농업 환경이 변화하면서 작물 재배 지역과 방식이 달라지고 있습니다.\n",
      "\n",
      "첫째, 기후 변화로 인해 한반도와 같은 지역에서는 아열대 작물의 재배가 가능해지고 있습니다. 예를 들어, 한반도의 기후가 아열대성으로 변하면서 중남미에서 나는 아티초크나 남아프리카의 오크라 등의 작물이 한국에서도 재배되고 있습니다. 이는 농산물 지도를 바꾸고 있으며, 제주도와 전남 일대에서 아열대 작물 재배 면적이 늘어나고 있습니다. 또한, 제주도에서 재배되던 한라봉과 감귤이 충북 청주에서도 생산되고 있으며, 사과의 주 산지도 북상하고 있습니다[[ref4]].\n",
      "\n",
      "둘째, 기후 변화는 수산물에도 영향을 미치고 있습니다. 바다 수온이 오르면서 찬물에 사는 낙지의 수확량은 감소하고, 따뜻한 물을 좋아하는 꽃게의 어획량은 증가하고 있습니다. 또한, 제주도 일부 지역에서만 서식하던 갯가재, 홍다리얼룩새우 등 아열대 생물이 남해안 전역에서 발견되고 있습니다. 반면, 수온이 낮아야 잘 자라는 김, 미역, 다시마 등의 수확은 어려워지고 있습니다[[ref4]].\n",
      "\n",
      "이와 같이 기후 변화는 농업에 큰 영향을 미치며, 작물 재배 지역의 변화와 새로운 작물의 도입, 수산물의 어획량 변화 등을 초래하고 있습니다. 이는 농업 생산성과 식량 안보에 중요한 영향을 미칠 수 있습니다.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72437ff3",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb39bf90-042a-470f-8ded-63dba005466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880250c",
   "metadata": {},
   "source": [
    "`lora_alpha`: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "\n",
    "`lora_dropout`: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. `0.1`로 설정하면 학습 중 10%의 뉴런이 비활성화됩니다.\n",
    "\n",
    "`r`: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "\n",
    "`bias`: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. `\"none\"`으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. `\"all\"` 또는 `\"lora_only\"`와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "\n",
    "`target_modules`: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 `\"q_proj\"`와 `\"v_proj\"`를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "\n",
    "`task_type`: LoRA가 적용되는 작업 유형을 지정합니다. `\"CAUSAL_LM\"`은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 `\"SEQ2SEQ_LM\"`(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57675fd-4374-4cd6-b8bb-950bf216cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-rag-ko\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b457fc",
   "metadata": {},
   "source": [
    "`output_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\n",
    "`num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니다.\n",
    "\n",
    "`per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있습니다.\n",
    "\n",
    "`gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 2로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 유용합니다.\n",
    "\n",
    "`gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질 수 있습니다.\n",
    "\n",
    "`optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적화기를 사용합니다.\n",
    "\n",
    "`logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마다 로그를 기록합니다.\n",
    "\n",
    "`save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스텝마다 모델이 저장됩니다.\n",
    "\n",
    "`save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50 스텝마다 모델을 저장합니다.\n",
    "\n",
    "`bf16`: bfloat16 정밀도를 사용하도록 설정합니다. bfloat16은 FP32와 유사한 범위를 제공하면서 메모리와 계산 효율성을 높입니다.\n",
    "\n",
    "`learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 사용하여 안정적인 학습을 유도합니다.\n",
    "\n",
    "`max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 조정하여 폭발적 그래디언트를 방지합니다.\n",
    "\n",
    "`warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다. 학습의 안정성을 높이기 위해 사용됩니다.\n",
    "\n",
    "`lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"constant\"`는 학습률을 일정하게 유지합니다.\n",
    "\n",
    "`push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "\n",
    "`remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다. True로 설정하면 메모리를 절약할 수 있습니다.\n",
    "\n",
    "`dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset: True`로 설정하여 데이터셋 준비 단계를 건너뜹니다.\n",
    "\n",
    "`report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11e4e1",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbcf4b1-ab29-451d-b628-21466d16519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eccfa92",
   "metadata": {},
   "source": [
    "`collate_fn(batch)` 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 attention_mask를 생성합니다. 이후 레이블 데이터를 초기화한 다음, 특정 토큰 패턴(<|im_start|>assistant 이후부터 <|im_end|>까지)을 찾아 해당 범위에 레이블을 설정합니다. 이 범위를 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다.\n",
    "\n",
    "최종적으로, 배치 내 모든 샘플의 길이를 동일하게 맞추기 위해 패딩 작업을 수행합니다. 이 과정에서 입력 데이터에는 패딩 토큰 ID를 추가하고, 어텐션 마스크에는 0을 추가하며, 레이블에는 -100을 추가합니다. 모든 데이터는 PyTorch 텐서로 변환되어 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c8810f-2613-4d6b-8dcb-985ba50991f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 4328])\n",
      "어텐션 마스크 형태: torch.Size([1, 4328])\n",
      "레이블 형태: torch.Size([1, 4328])\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이\n",
    "max_seq_length=8192\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50fbb8d-1265-4c9a-b56e-2803f09bd810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 138520, 19391, 143604, 129264, 130650, 382, 13146, 48431, 20401, 66790, 29326, 131193, 17877, 125686, 125548, 139713, 624, 16, 13, 138520, 53680, 85322, 77226, 98801, 18411, 81718, 144059, 42039, 143604, 16186, 139713, 624, 17, 13, 85322, 77226, 98801, 19391, 130768, 130213, 17877, 143604, 16186, 125476, 34395, 53900, 21329, 95577, 139713, 624, 18, 13, 138520, 19391, 128605, 143603, 12802, 85322, 77226, 98801, 19391, 130671, 32290, 85322, 77226, 98801, 126377, 330, 33883, 64795, 138520, 93, 19391, 128605, 130213, 12802, 136673, 1189, 5140, 45881, 34395, 143604, 16186, 139713, 624, 19, 13, 143604, 47836, 53618, 142976, 139236, 18411, 142616, 82190, 53435, 40853, 129549, 53435, 125068, 17877, 140174, 128836, 32290, 5140, 240, 97, 19391, 36330, 250, 125746, 16560, 23084, 126402, 83634, 17380, 94613, 139236, 84621, 47324, 18411, 129624, 20487, 139713, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 56475, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 5053, 130939, 54116, 57132, 16186, 139713, 624, 20, 13, 95617, 18411, 129901, 26698, 142976, 53435, 40853, 129835, 53435, 125068, 17877, 220, 16, 42044, 139236, 80573, 220, 20, 42044, 139236, 56475, 143409, 58677, 26699, 128836, 32290, 5140, 240, 97, 19391, 4318, 1097, 16, 20492, 4318, 1097, 20, 5053, 130939, 54116, 57132, 16186, 139713, 624, 21, 13, 81173, 66845, 23573, 49367, 23259, 20401, 139236, 18411, 58677, 26699, 82190, 143604, 16186, 139713, 382, 129845, 77226, 98801, 510, 34764, 51588, 26698, 16, 25, 129392, 29326, 5140, 228, 235, 124517, 20401, 49052, 11, 133196, 126804, 21329, 56475, 66790, 88259, 38523, 101, 126588, 56290, 126674, 11, 60960, 20487, 20401, 130270, 92817, 128355, 125714, 126402, 125149, 126346, 128618, 131833, 11, 61298, 32077, 43115, 37087, 56419, 131833, 136357, 83518, 126793, 19391, 126629, 133788, 133396, 128841, 140749, 130999, 124657, 131293, 127165, 108, 134836, 77002, 53680, 129381, 142684, 140577, 57132, 33883, 77002, 12802, 66790, 126299, 128552, 83556, 33509, 126896, 57801, 97143, 131611, 11, 54116, 127033, 137525, 17380, 58677, 23573, 47818, 86372, 124781, 54969, 142194, 134454, 32985, 231, 17877, 28733, 64521, 65510, 124873, 47985, 131891, 64577, 126835, 128533, 47818, 125991, 45104, 101, 94152, 12802, 47818, 20487, 57801, 97143, 16560, 60960, 126246, 17877, 62071, 29326, 47836, 28733, 142161, 97143, 16560, 23084, 126333, 12802, 90686, 13, 130549, 129392, 29326, 5140, 228, 235, 124517, 17877, 140968, 32831, 56290, 128555, 97143, 32290, 69192, 110, 53680, 129381, 127165, 120, 131293, 63332, 47324, 36055, 126712, 33704, 133554, 81718, 13146, 60315, 73523, 144950, 77002, 60716, 126345, 47818, 86372, 124781, 63332, 47324, 36055, 126712, 128878, 60960, 135257, 128552, 129413, 56290, 47836, 28733, 64521, 77002, 135392, 34143, 230, 134133, 18411, 36330, 102, 129704, 29326, 143651, 28733, 142161, 130723, 13, 54116, 130999, 126377, 127165, 120, 131293, 47665, 234, 130752, 94315, 5140, 228, 235, 65306, 21329, 65510, 132818, 53900, 124982, 125590, 11, 130270, 126251, 81173, 55902, 137812, 11, 70943, 62275, 142510, 126588, 137812, 11, 55673, 55902, 97834, 128747, 52959, 143083, 129381, 55673, 92192, 128355, 58034, 124517, 132555, 30520, 113, 128747, 130270, 126970, 126251, 20401, 58034, 124517, 131937, 129881, 38523, 98, 55902, 77002, 21329, 19391, 5140, 228, 235, 133120, 35509, 136892, 31079, 83277, 26698, 130270, 126251, 19391, 58034, 54330, 126204, 64521, 128618, 134771, 55673, 92192, 44518, 125624, 126352, 138533, 126253, 220, 16, 126216, 126322, 126246, 95170, 29281, 128552, 28927, 251, 131837, 64577, 54321, 17877, 130729, 41671, 47836, 28733, 142161, 97143, 16560, 132968, 18585, 238, 12802, 90686, 13, 90667, 468, 7347, 5883, 20401, 125703, 127871, 31328, 66790, 88259, 129615, 127048, 126345, 84255, 81650, 127483, 130612, 23084, 80573, 129381, 73669, 65306, 17877, 126291, 57132, 17380, 5140, 239, 242, 130861, 18411, 10764, 228, 254, 129923, 132513, 11, 54116, 127033, 137525, 60960, 131518, 48364, 112, 124781, 60716, 43590, 131565, 133120, 139274, 40853, 26698, 16560, 83518, 124873, 12802, 140175, 132306, 13, 132330, 97143, 32290, 10764, 249, 234, 144295, 23573, 65510, 124873, 17877, 35509, 128732, 28733, 127353, 11, 133553, 85403, 129704, 133152, 129549, 129413, 23259, 131837, 12802, 37195, 106, 33704, 131937, 126327, 16560, 134585, 10764, 251, 105, 130472, 12802, 25715, 54116, 61741, 18411, 79302, 119, 57801, 97143, 16560, 3315, 227, 230, 125489, 624, 34764, 51588, 26698, 17, 25, 66790, 126299, 19969, 66019, 32831, 53680, 95170, 41671, 92751, 126333, 56475, 5140, 112, 97, 17877, 53618, 11, 127296, 129359, 70582, 60315, 54116, 132264, 40720, 128841, 90486, 127085, 137709, 33704, 83556, 54321, 56290, 131396, 23259, 49664, 125953, 13146, 13, 134084, 37195, 250, 126321, 11, 132028, 127105, 85403, 51588, 20401, 90486, 127085, 137709, 17877, 56419, 20487, 17380, 125206, 128747, 140295, 129242, 76435, 142653, 17380, 141091, 52300, 56419, 135818, 127296, 85403, 51588, 19391, 141540, 132841, 128552, 40720, 77953, 42039, 135660, 137138, 90486, 127085, 21329, 40720, 131837, 17877, 53989, 226, 32077, 28733, 130689, 128900, 13, 131367, 3369, 144272, 33861, 62275, 37064, 113, 128747, 527, 33704, 135392, 54116, 20487, 20401, 90486, 127085, 137709, 17877, 56419, 28754, 42039, 81718, 136892, 16560, 3369, 65865, 20487, 56290, 527, 18411, 131582, 34143, 105, 32831, 47836, 28733, 90686, 13, 23084, 19391, 126366, 33883, 56419, 131777, 40720, 42905, 54116, 135854, 68232, 57089, 45130, 120, 29281, 56475, 38523, 101, 125086, 19969, 24897, 18411, 73669, 69923, 87425, 50696, 33509, 137621, 95996, 65865, 27767, 117, 33861, 72553, 74361, 230, 135356, 43590, 56290, 47836, 28733, 90686, 32290, 90486, 127085, 21329, 44518, 24897, 127649, 137138, 20401, 74361, 226, 43590, 130609, 69923, 131837, 17877, 143231, 53989, 226, 32077, 28733, 90686, 382, 53955, 144732, 126445, 19969, 60960, 126414, 128533, 32129, 131000, 13146, 13, 130847, 144732, 126445, 16560, 56419, 131777, 126563, 33883, 38523, 101, 47985, 19969, 37195, 106, 33704, 45130, 111, 56475, 5140, 228, 240, 33704, 45130, 111, 42039, 130847, 17877, 23084, 57089, 135298, 16560, 129359, 59698, 17380, 11, 132270, 58034, 26699, 56290, 52300, 54116, 125880, 70943, 129400, 32077, 54116, 125880, 17380, 16560, 38523, 101, 125086, 19969, 24897, 73669, 69923, 19391, 130887, 143231, 54116, 57026, 47836, 132091, 54116, 66845, 132306, 13, 5140, 225, 231, 130000, 40853, 34395, 60315, 5140, 225, 231, 126588, 126321, 131777, 130270, 126251, 129400, 80901, 17380, 135969, 42905, 142497, 20401, 54116, 125880, 130939, 30520, 120, 28733, 90686, 13, 130847, 144732, 126445, 54116, 125880, 12802, 135969, 52300, 138249, 142653, 20487, 88259, 7, 5371, 32, 8, 16560, 46832, 242, 129150, 125568, 63256, 18411, 40720, 42905, 130270, 126251, 37195, 250, 126321, 134620, 70943, 220, 18, 15, 4, 72553, 130847, 144732, 126445, 17380, 127048, 49543, 33883, 47985, 77353, 62275, 220, 23, 4, 35509, 131837, 20401, 74361, 226, 43590, 130609, 69923, 131837, 17877, 53989, 226, 32077, 28733, 130689, 132091, 66136, 13146, 141551, 13146, 13, 46832, 242, 129150, 125568, 63256, 18411, 126563, 23573, 63332, 32077, 60294, 131137, 73669, 69923, 19969, 24897, 19969, 51917, 132553, 47985, 50696, 33509, 137621, 60960, 20487, 57268, 137284, 126251, 128732, 17877, 73669, 69923, 87425, 127728, 130822, 137638, 129359, 126333, 125489, 13, 130847, 144732, 126445, 80573, 95577, 138143, 19969, 21329, 17380, 56419, 20487, 143995, 139512, 90486, 127085, 21329, 126563, 141540, 132841, 17877, 5140, 228, 240, 132044, 143409, 60960, 20487, 57268, 137284, 126251, 128732, 73669, 69923, 17877, 53989, 226, 132044, 5140, 41902, 126310, 26699, 129330, 382, 23573, 129027, 11, 90486, 127085, 133120, 126291, 129439, 42905, 45130, 120, 29281, 129889, 75528, 125568, 128552, 130847, 142653, 141966, 20401, 77596, 238, 125086, 12802, 133396, 51876, 13, 77596, 238, 125086, 53955, 33704, 30520, 226, 47985, 20401, 138507, 53900, 21329, 135201, 86831, 125476, 130974, 90486, 127085, 21329, 13146, 13, 134084, 131367, 90486, 127085, 133120, 98005, 23259, 33883, 131170, 40720, 47836, 28733, 90686, 32290, 90486, 127085, 21329, 134620, 56419, 126641, 20401, 141540, 132841, 17877, 143231, 5140, 228, 240, 32077, 28733, 90686, 13, 74884, 226, 65865, 23573, 74361, 230, 135356, 43590, 90486, 127085, 21329, 49543, 124781, 17380, 23084, 124528, 42905, 5140, 41902, 45130, 120, 47985, 20487, 128533, 141966, 31328, 30520, 113, 128747, 56290, 28754, 12802, 60960, 126414, 128533, 32129, 131000, 13146, 13, 46832, 242, 28754, 126835, 65865, 43590, 16560, 81173, 66845, 220, 19, 15, 4, 129624, 140847, 23573, 141540, 132841, 17877, 37195, 116, 13146, 13, 10764, 230, 105, 43866, 23573, 77353, 63256, 20401, 90486, 127085, 21329, 70943, 220, 19, 15, 4, 131219, 72553, 56419, 28754, 42039, 46319, 65238, 47836, 28733, 135511, 5140, 250, 119, 125489, 13, 73518, 132125, 21329, 220, 21, 15, 4, 20401, 90486, 127085, 130974, 130847, 142653, 141966, 17380, 77596, 238, 125086, 132306, 13, 130653, 77596, 238, 125086, 128841, 130847, 142652, 133120, 129392, 29326, 20401, 5140, 116, 242, 49664, 129400, 80901, 17380, 38523, 101, 23259, 18411, 125466, 128911, 42905, 131937, 126588, 126321, 19391, 140969, 126559, 141540, 132841, 17877, 220, 22, 15, 4, 66845, 128878, 5140, 223, 234, 31079, 130137, 135379, 28733, 90686, 13, 30520, 113, 128747, 56290, 28754, 20401, 77353, 63256, 17380, 16560, 69441, 238, 20487, 137075, 126291, 126317, 42905, 45130, 120, 29281, 56475, 133396, 42905, 130847, 17877, 55673, 17380, 40720, 126204, 141258, 74361, 250, 126345, 53955, 129835, 66790, 53955, 17877, 126563, 23573, 74808, 76337, 17877, 131937, 126588, 126321, 19391, 135969, 42905, 74808, 126246, 47985, 140968, 126835, 128555, 131698, 130357, 90686, 382, 93672, 127105, 85403, 51588, 20401, 56419, 20487, 56290, 130005, 136361, 85997, 43590, 13146, 13, 90667, 66790, 16186, 131573, 17877, 73986, 143719, 23573, 48364, 254, 47985, 130472, 12802, 58034, 64795, 129881, 56419, 20487, 56290, 134521, 127378, 129392, 17380, 127048, 125160, 136448, 56419, 20487, 125625, 19969, 5140, 117, 254, 125548, 57801, 63332, 128911, 130357, 90686, 13, 139293, 56419, 20487, 125625, 20401, 20136, 113, 125512, 85403, 125678, 31328, 73669, 33861, 133886, 141091, 42905, 45130, 120, 29281, 56475, 38523, 101, 125086, 19969, 24897, 19969, 135968, 21329, 50696, 57801, 73669, 69923, 52300, 130822, 18585, 238, 17877, 126429, 125476, 129264, 129112, 13, 56419, 20487, 125625, 19391, 40720, 128841, 56419, 131777, 46832, 242, 28754, 126835, 65865, 42039, 141091, 47836, 49052, 11, 129882, 131837, 20401, 137138, 47818, 126898, 54330, 20487, 56475, 56419, 20487, 125625, 19969, 66136, 125568, 131854, 125625, 129885, 126366, 129875, 38523, 101, 125086, 19969, 24897, 18411, 51917, 51876, 13, 23084, 16560, 129882, 131837, 20401, 56419, 20487, 56290, 19969, 129242, 76435, 142653, 17380, 20401, 56419, 65238, 53680, 129676, 141210, 136398, 89659, 50972, 17877, 5140, 250, 119, 51876, 13, 220, 17, 15, 15, 24, 126216, 54116, 129044, 126310, 134798, 20401, 56419, 28754, 98869, 28626, 144087, 28002, 57268, 18411, 81718, 144059, 42039, 94203, 85057, 33883, 41671, 32290, 56419, 20487, 125625, 20401, 74361, 226, 43590, 130609, 69923, 131837, 12802, 66136, 125568, 131854, 19391, 73986, 33883, 74361, 226, 43590, 73669, 69923, 131837, 12802, 81173, 66845, 220, 20, 21, 4, 135968, 33704, 132091, 135513, 126588, 13146, 382, 13146, 72553, 90486, 127085, 21329, 65865, 65238, 20401, 128772, 85997, 43590, 19391, 73986, 33883, 132028, 127105, 85403, 51588, 20401, 56419, 20487, 56290, 126377, 73077, 136499, 130217, 12802, 131961, 135379, 132091, 63332, 31328, 13146, 13, 132270, 44518, 126333, 56475, 73986, 124528, 20487, 80573, 60960, 128909, 46832, 242, 126251, 127819, 116, 144329, 11, 129296, 129471, 20401, 56419, 20487, 56290, 16560, 124685, 125476, 93672, 10764, 236, 116, 125489, 13, 23084, 64850, 132028, 127105, 23259, 125068, 33704, 60960, 53496, 57133, 12802, 126871, 96137, 129359, 135375, 129264, 128956, 73669, 33861, 28002, 20401, 65722, 102, 131837, 47985, 132182, 124419, 11, 73669, 33861, 28002, 20401, 125149, 57801, 17380, 58677, 33883, 54116, 49543, 70943, 131837, 12802, 66790, 60315, 59698, 57801, 132376, 251, 19969, 16186, 137621, 141540, 132841, 137032, 37195, 106, 52959, 21329, 20487, 137141, 13, 60960, 53496, 57133, 12802, 134028, 132028, 127105, 23259, 125068, 126377, 60960, 126246, 42039, 56419, 20487, 17380, 141091, 23573, 81718, 12802, 57268, 125568, 63256, 60315, 28733, 43590, 18411, 126563, 42905, 74808, 126246, 12802, 131698, 70943, 125489, 624, 34764, 51588, 26698, 18, 25, 143005, 48364, 255, 29281, 142653, 73523, 126835, 19391, 140429, 47455, 239, 28754, 66425, 130679, 3369, 17, 15, 16, 19, 85413, 242, 29346, 48606, 129807, 142653, 128808, 125894, 527, 12802, 220, 17, 17, 93, 17, 19, 32077, 43115, 131226, 43115, 136559, 10764, 252, 238, 98933, 47324, 128100, 56475, 130847, 129807, 13146, 13, 23573, 124785, 132249, 82528, 51588, 53680, 43115, 55902, 131226, 47985, 13935, 65306, 136559, 19969, 55673, 128215, 42905, 132183, 98869, 125894, 126377, 3369, 48606, 129807, 142653, 45104, 101, 60294, 13146, 93701, 20401, 60960, 126667, 65238, 527, 12802, 129804, 55673, 37087, 17380, 220, 18, 15, 57026, 59761, 124785, 36055, 13935, 124780, 13935, 124632, 13935, 134066, 124781, 92751, 124781, 25715, 220, 18, 15, 15, 15, 57026, 79632, 12802, 127969, 129150, 51876, 13, 48364, 104, 83666, 129378, 53680, 5140, 239, 246, 83666, 129378, 126377, 127969, 129150, 25715, 126253, 74361, 250, 126345, 126861, 129093, 25715, 28754, 60039, 139269, 131040, 90486, 127085, 21329, 14467, 54470, 77353, 63256, 65865, 21329, 77002, 220, 20, 59761, 128618, 53680, 17380, 73518, 144478, 125674, 56039, 60315, 18411, 77353, 13146, 13, 17, 15, 16, 15, 126216, 127042, 144094, 132249, 124632, 55902, 28733, 55902, 25715, 31328, 143230, 28002, 24897, 124154, 57160, 235, 120, 142510, 55054, 28002, 124419, 24897, 5140, 253, 108, 125615, 29281, 59698, 132249, 66845, 142220, 80573, 23084, 24897, 125544, 32077, 24485, 246, 21329, 131303, 28002, 54116, 127033, 137525, 19391, 130207, 36055, 63089, 16778, 226, 47455, 239, 20401, 49543, 85403, 20401, 40853, 12802, 77353, 55054, 17380, 127969, 129150, 51876, 13, 43115, 55902, 131226, 47985, 16560, 66790, 126321, 25715, 59698, 137438, 142180, 136065, 42039, 73523, 126835, 47985, 55902, 124785, 126429, 80901, 128911, 58677, 55054, 128355, 126310, 136233, 134066, 131570, 20487, 88259, 77002, 138249, 20487, 88259, 80573, 129676, 73523, 47985, 124785, 20401, 90486, 127085, 21329, 47455, 239, 135818, 132376, 251, 85251, 126204, 54116, 127033, 137525, 60960, 131518, 74808, 126246, 17877, 127041, 120, 20401, 51876, 13, 140887, 129378, 126377, 130092, 54321, 25715, 28754, 132892, 78125, 125068, 77002, 127165, 108, 124517, 126407, 137471, 141875, 126204, 23872, 120, 62275, 126377, 43115, 54330, 21329, 126346, 92751, 126861, 133120, 5140, 239, 246, 60294, 41671, 16560, 37195, 62618, 28626, 10764, 230, 105, 31079, 47985, 94203, 127324, 136398, 90686, 13, 128808, 125894, 33704, 69441, 238, 61741, 76337, 56475, 66790, 88259, 144089, 20401, 90486, 127085, 21329, 5140, 117, 230, 142561, 10764, 229, 112, 59698, 80573, 143005, 48364, 255, 29281, 142653, 73523, 126835, 136111, 11, 133196, 20401, 140429, 84621, 125144, 128355, 66790, 126299, 143964, 73523, 126835, 17877, 57835, 88259, 16186, 132343, 130213, 17877, 34143, 112, 33704, 43115, 54330, 125519, 129709, 51588, 17877, 3315, 109, 226, 127919, 51876, 624, 34764, 51588, 26698, 19, 25, 124973, 126702, 126337, 25715, 54321, 33704, 23084, 129062, 83315, 62071, 54330, 47985, 19391, 132185, 17877, 73523, 14559, 44680, 244, 19946, 13, 48408, 53955, 66845, 68232, 137075, 73523, 131837, 33883, 127820, 131976, 128739, 17877, 130729, 41671, 42905, 98927, 57852, 126702, 134953, 125489, 13, 61298, 126641, 47985, 54116, 127033, 19969, 48408, 53955, 66845, 32831, 42039, 46319, 132537, 70943, 131793, 56039, 56475, 132311, 124973, 56290, 53680, 28927, 251, 26699, 53955, 129865, 48408, 131131, 132618, 81133, 60315, 129624, 52959, 126445, 28002, 129616, 20401, 3315, 109, 226, 43590, 142209, 223, 105, 50340, 136357, 127165, 108, 21329, 19969, 130092, 42039, 81718, 144194, 125761, 13, 56419, 131793, 13935, 65306, 131793, 60716, 126246, 128878, 48408, 53955, 66845, 68232, 126251, 20401, 127042, 21329, 57132, 130609, 19969, 95351, 33883, 21329, 131611, 126932, 126216, 220, 16, 15, 15, 126337, 12802, 143835, 16560, 48408, 53955, 66845, 68232, 126251, 127820, 131976, 19969, 36330, 250, 54321, 130357, 35339, 130898, 53900, 13146, 13, 61298, 126641, 47985, 20401, 48408, 53955, 66845, 56290, 16560, 127041, 117, 125625, 144533, 17877, 56419, 131793, 63332, 32831, 56475, 129413, 54321, 126429, 32831, 128878, 139963, 55902, 135298, 16560, 77002, 5140, 228, 235, 85057, 126251, 66790, 47985, 18411, 81718, 144379, 5140, 228, 241, 34395, 90686, 13, 136277, 55054, 131573, 28733, 133085, 42905, 48408, 53955, 66845, 68232, 126251, 53955, 66845, 68232, 126251, 31328, 32985, 251, 34395, 11, 65722, 102, 53680, 77002, 33704, 62071, 54330, 47985, 73986, 144039, 16186, 40281, 24897, 56475, 129242, 130609, 132306, 13, 23894, 101, 126893, 131573, 19391, 72553, 37195, 250, 126321, 17877, 53900, 32290, 125149, 28002, 131702, 10764, 92120, 126893, 28733, 90686, 13, 5140, 116, 234, 17380, 144063, 28002, 131380, 12802, 47818, 133507, 130847, 129865, 18411, 137767, 117, 16560, 48408, 131131, 132618, 81133, 16560, 23894, 101, 126893, 131573, 130612, 28733, 133085, 12802, 95351, 129330, 13, 37087, 54330, 47985, 127871, 130651, 56419, 131793, 83556, 66845, 47985, 48408, 53955, 66845, 68232, 126251, 129242, 130609, 48108, 112, 133864, 143861, 246, 31079, 60315, 34395, 90686, 13, 56419, 131793, 133084, 124517, 131040, 54321, 19391, 125686, 125548, 32290, 220, 17, 15, 15, 24, 126216, 220, 18, 23, 72553, 144562, 12802, 125615, 56419, 131793, 20401, 5140, 116, 242, 126746, 132947, 28002, 5140, 228, 235, 40853, 134313, 129439, 16560, 133146, 33883, 220, 16, 18, 16, 72553, 144562, 17380, 89095, 97, 141798, 13, 28733, 43866, 19391, 72553, 124970, 130999, 16186, 125615, 48408, 53955, 66845, 3315, 109, 226, 43590, 97929, 142209, 223, 105, 50340, 11, 58677, 89235, 129709, 29326, 125052, 59698, 77002, 33704, 60716, 131793, 13935, 130262, 85251, 13935, 40853, 138609, 83556, 66845, 19391, 68232, 126216, 126558, 220, 16, 17, 72553, 144562, 134313, 129439, 17380, 129242, 130609, 141874, 93721, 128836, 13, 12802, 32831, 54330, 124973, 126702, 126337, 25715, 54321, 62071, 54330, 137709, 40853, 33704, 1036, 17, 15, 15, 22, 126216, 136331, 48408, 53955, 66845, 68232, 126251, 28927, 57160, 240, 230, 126337, 127820, 131976, 69923, 54321, 128753, 125118, 130270, 135444, 8620, 122, 116, 129044, 125511, 143861, 246, 31079, 139465, 126377, 77353, 62275, 220, 18, 15, 57026, 59761, 68232, 126251, 220, 16, 15, 15, 57026, 59761, 10764, 240, 230, 126337, 42039, 132376, 251, 19969, 128836, 854, 34395, 126254, 128836, 13, 130508, 129034, 65238, 5140, 228, 235, 144089, 85251, 138609, 125118, 38523, 101, 126588, 56290, 66845, 131518, 134310, 131698, 54321, 33704, 1036, 37087, 54330, 56419, 131793, 43115, 131793, 60716, 126246, 132184, 90667, 48408, 53955, 66845, 54116, 127033, 17380, 128618, 97929, 126204, 90686, 854, 34395, 133828, 128836, 13, 141185, 56475, 129242, 130609, 128841, 48408, 53955, 66845, 68232, 126251, 33704, 55673, 17380, 126429, 128911, 5140, 254, 230, 24897, 129283, 133738, 19391, 37195, 102, 125678, 64119, 127451, 58677, 33861, 128013, 136671, 3369, 52959, 29326, 52959, 125544, 28626, 527, 134454, 131582, 74884, 116, 124785, 31328, 132812, 140568, 129865, 132306, 13, 139465, 126377, 141185, 126291, 70582, 25715, 129360, 28733, 35711, 47985, 132376, 251, 19969, 33883, 22042, 109, 56290, 126333, 136448, 141185, 85057, 48408, 53955, 66845, 68232, 137075, 45104, 242, 20487, 93721, 128836, 13, 128753, 41429, 124781, 130728, 56290, 126333, 33704, 38523, 105, 129901, 62071, 54330, 85057, 3369, 133087, 92031, 126445, 126746, 28626, 527, 140568, 129865, 18411, 93721, 128836, 13, 23872, 57160, 242, 234, 130472, 34395, 80573, 65722, 102, 53680, 140568, 129865, 131837, 33704, 56419, 126216, 60960, 70582, 220, 17, 15, 4, 143861, 246, 125761, 13, 16778, 97, 60294, 28002, 52959, 130728, 56290, 126333, 20401, 28927, 251, 125678, 124780, 126429, 84667, 12802, 19, 24, 19, 129889, 23084, 129062, 129901, 62071, 54330, 85057, 3369, 52959, 130229, 129439, 89659, 7, 144018, 92192, 30395, 57160, 242, 234, 8, 527, 18411, 129296, 41671, 139836, 13, 61298, 22042, 243, 24897, 7, 18, 145189, 8, 19391, 220, 17, 15, 72553, 54321, 42039, 35509, 126614, 33704, 130572, 73986, 140611, 125590, 95577, 83036, 52959, 137812, 12802, 129419, 33861, 130109, 134497, 135511, 133828, 125489, 13, 136277, 125118, 54330, 128878, 38523, 105, 50340, 130000, 61298, 50340, 135402, 55054, 53680, 77002, 55673, 35711, 68232, 126251, 20401, 127165, 108, 130974, 139963, 55902, 70943, 125489, 13, 62071, 54330, 127820, 85057, 126251, 17380, 126310, 79632, 23573, 61298, 50340, 135402, 53680, 129423, 145237, 33704, 90667, 36330, 102, 131226, 48364, 255, 54330, 136448, 141091, 132306, 13, 60960, 88259, 77002, 12802, 55673, 127165, 108, 21329, 31328, 32129, 53680, 16560, 43115, 20487, 98869, 129034, 11, 129413, 54321, 126440, 128514, 77002, 136448, 130966, 129242, 130609, 130357, 90686, 13, 3369, 41671, 32831, 144067, 125625, 527, 17380, 126310, 79632, 23573, 56419, 131793, 63332, 32831, 33704, 127041, 117, 125625, 20401, 55673, 127165, 108, 133120, 129413, 54321, 126429, 32831, 19391, 66136, 131303, 140568, 125489, 13, 23259, 85057, 126251, 47985, 28927, 105, 55902, 59698, 50696, 13146, 13, 220, 16, 15, 128514, 12802, 62071, 131573, 31328, 62099, 105, 126251, 19391, 32129, 16560, 37195, 247, 130974, 81718, 13146, 23259, 130000, 12802, 73077, 125548, 131611, 38523, 105, 129901, 28733, 133085, 131837, 12802, 60960, 135257, 129423, 43590, 128836, 13, 63757, 129923, 125686, 136196, 23573, 133553, 17877, 138779, 42905, 8620, 121, 225, 57801, 16560, 38523, 105, 33883, 10764, 240, 235, 126216, 125489, 13, 77353, 126742, 47985, 20401, 38523, 105, 33883, 8620, 121, 225, 57801, 124685, 127324, 131837, 7, 24, 15, 15, 83, 8, 33704, 68232, 126216, 129885, 220, 21, 22, 4, 143861, 246, 125761, 13, 60716, 126345, 23259, 85057, 63089, 19391, 125686, 125548, 32290, 220, 17, 15, 15, 15, 126216, 66845, 129624, 33883, 126246, 20401, 69441, 231, 139746, 28733, 130000, 33704, 220, 16, 24, 13, 17, 47985, 17380, 220, 16, 24, 22, 15, 126216, 66845, 19391, 73986, 33883, 220, 15, 13, 23, 47985, 38523, 105, 144110, 13146, 13, 62071, 54330, 47985, 136605, 131937, 56475, 72553, 89860, 76337, 16186, 125615, 16778, 107, 19969, 57132, 11, 46832, 235, 13146, 28002, 143279, 144171, 131097, 40281, 77002, 48408, 53955, 66845, 47818, 126251, 12802, 129624, 33883, 126246, 56419, 126346, 56475, 142300, 130357, 90686, 13, 63757, 32290, 28733, 130000, 12802, 37195, 106, 52959, 89659, 126720, 64577, 129615, 130508, 125714, 126346, 131170, 125544, 77002, 33704, 129624, 33883, 126246, 56475, 28733, 133085, 66425, 19969, 124685, 125476, 130109, 128036, 130822, 128618, 129150, 125489, 624, 34764, 51588, 26698, 20, 25, 136905, 129413, 26698, 88259, 95577, 139822, 21329, 88259, 19391, 83518, 20401, 47985, 78125, 54321, 220, 17, 130609, 134313, 129439, 20401, 48408, 29326, 52959, 81173, 66845, 47818, 86372, 78125, 54321, 7, 92817, 129567, 47985, 8, 12802, 65510, 32831, 132306, 13, 136905, 29326, 16560, 95577, 139822, 21329, 88259, 19391, 220, 20, 15, 15, 15, 57026, 126337, 20401, 28927, 251, 137075, 143143, 144079, 129392, 29326, 128909, 28927, 251, 126251, 54321, 31328, 3369, 26698, 126893, 46832, 242, 87608, 54321, 7, 99232, 75405, 101620, 8, 527, 7, 19969, 141676, 8, 17877, 220, 17, 15, 16, 21, 126216, 220, 16, 17, 128514, 128878, 138267, 78125, 47836, 95617, 29281, 130939, 220, 17, 16, 32077, 142234, 128836, 13, 136905, 89860, 131793, 128739, 20401, 48364, 104, 60960, 128909, 125466, 54321, 12802, 25715, 136905, 20401, 3369, 144067, 77226, 10764, 245, 230, 131196, 527, 127864, 47836, 17877, 95002, 132091, 54116, 66845, 132306, 13, 127165, 108, 124517, 125068, 21329, 7, 16, 24, 15, 72553, 144562, 8, 19391, 38150, 54330, 42905, 54116, 124517, 131698, 132555, 19391, 23084, 31079, 55673, 92192, 125068, 21329, 7, 16, 15, 21, 72553, 144562, 8, 66136, 48408, 143083, 134664, 79716, 126345, 12802, 129238, 126614, 56290, 128841, 95577, 139822, 21329, 88259, 19391, 91043, 57132, 17380, 68232, 26699, 47836, 56419, 130472, 125489, 13, 26698, 126893, 29326, 19391, 125686, 125548, 32290, 54116, 130999, 20401, 60960, 131005, 129439, 10764, 29389, 21329, 131570, 21329, 88259, 19391, 65510, 32831, 128841, 125466, 54321, 12802, 126352, 54330, 25715, 129360, 83518, 19969, 13935, 137266, 126345, 54116, 66019, 19391, 83315, 138913, 131417, 145063, 133099, 136905, 56290, 87608, 54321, 33704, 58677, 125722, 95577, 139822, 144078, 125068, 134066, 21329, 88259, 7, 134066, 8, 80573, 43115, 124781, 18411, 10764, 245, 230, 126251, 34395, 38523, 249, 95577, 139822, 20401, 10764, 240, 235, 126861, 17877, 129242, 126407, 42905, 5140, 41902, 90711, 246, 126402, 51876, 13, 23894, 116, 57132, 36055, 125519, 20401, 3369, 126337, 33883, 125118, 92817, 7, 100401, 55135, 103637, 100227, 8, 527, 19391, 135513, 126588, 38523, 249, 79302, 239, 129034, 131937, 20401, 136665, 17877, 129242, 126407, 66425, 130039, 58677, 78125, 44518, 125624, 137075, 81173, 43590, 56290, 126204, 133553, 57132, 76435, 134310, 20401, 129242, 76435, 23259, 80573, 5140, 117, 245, 137075, 125466, 54321, 91043, 23259, 17380, 140969, 42905, 128584, 127820, 135946, 125489, 13, 137138, 48108, 112, 80968, 33704, 220, 20, 15, 72553, 18, 19, 18, 16, 144562, 17380, 83518, 20401, 47985, 125466, 54321, 7, 17, 18, 72553, 144562, 8, 20401, 220, 17, 130609, 19969, 143835, 34395, 124685, 129807, 12802, 66845, 78125, 54321, 7, 20, 21, 72553, 144562, 8, 129885, 23872, 121, 62275, 68232, 13146, 13, 46832, 242, 87608, 54321, 126377, 220, 21, 72553, 144562, 134313, 129439, 28927, 251, 126251, 54321, 53680, 28927, 251, 126251, 131655, 134310, 19969, 129901, 125519, 13146, 13, 28927, 251, 126251, 131655, 134310, 16560, 220, 18, 59761, 130270, 126251, 7, 125568, 32290, 80968, 220, 16, 72553, 144562, 8, 19391, 56419, 29326, 130000, 125086, 11, 28927, 251, 126251, 47985, 26698, 124780, 11, 35509, 29346, 144262, 134310, 77002, 12802, 143143, 144162, 85251, 13146, 13, 44518, 16560, 126844, 129062, 70943, 92751, 124781, 131854, 47455, 239, 20401, 18411, 126352, 133847, 136089, 132597, 126246, 17877, 130729, 29281, 126204, 57852, 124781, 19391, 129901, 131498, 95617, 29281, 125489, 13, 151645, 198, 151644, 872, 198, 20487, 127033, 137525, 19969, 5140, 228, 235, 124517, 19391, 125714, 59698, 16560, 126440, 129321, 33704, 132779, 31328, 19969, 35711, 30, 151645, 198, 151644, 77091, 198, 20487, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 135392, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 54116, 127033, 137525, 17380, 58677, 33883, 5140, 228, 235, 124517, 46832, 246, 65306, 12802, 137525, 132537, 68232, 126251, 129242, 130609, 131937, 53680, 74808, 76337, 12802, 34143, 105, 50340, 134497, 128472, 382, 136689, 83666, 11, 54116, 127033, 137525, 17380, 58677, 33883, 61298, 126641, 47985, 80573, 129381, 131937, 129889, 48408, 53955, 66845, 68232, 126251, 20401, 129242, 130609, 19969, 95351, 33883, 134497, 128472, 13, 95617, 18411, 129901, 11, 61298, 126641, 47985, 20401, 54116, 127033, 19969, 48408, 53955, 66845, 32831, 42039, 46319, 132537, 70943, 131793, 56039, 56475, 132311, 48408, 131131, 132618, 81133, 60315, 129624, 52959, 126445, 28002, 129616, 20401, 142209, 223, 105, 50340, 136357, 68232, 126251, 12802, 130092, 136448, 129242, 130609, 130357, 128472, 13, 23084, 16560, 5140, 228, 235, 85057, 126251, 66790, 47985, 18411, 81718, 136892, 34395, 132931, 11, 62071, 54330, 47985, 80573, 56419, 131793, 83556, 66845, 56475, 48408, 53955, 66845, 68232, 126251, 129242, 130609, 48108, 112, 133864, 143861, 246, 31079, 60315, 34395, 128472, 13, 130005, 11, 62071, 54330, 47985, 56475, 129242, 130609, 64119, 125615, 61298, 50340, 135402, 53680, 129423, 145237, 12802, 36330, 102, 131226, 48364, 255, 54330, 136448, 141091, 130357, 132931, 11, 32129, 53680, 20401, 55673, 127165, 108, 133692, 139963, 55902, 126204, 128472, 15505, 1097, 19, 5053, 382, 136904, 83666, 11, 54116, 127033, 137525, 16560, 28733, 85057, 126251, 130612, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 81718, 13146, 28733, 130000, 12802, 73077, 125548, 131611, 62099, 105, 126251, 19391, 32129, 16560, 37195, 247, 21329, 20401, 28733, 133085, 131837, 33704, 129423, 43590, 126204, 11, 125686, 136196, 23573, 133553, 17877, 138779, 42905, 8620, 121, 225, 57801, 20401, 124685, 127324, 131837, 33704, 132376, 251, 19969, 126204, 128472, 13, 130005, 11, 62071, 54330, 47985, 136605, 131937, 56475, 72553, 89860, 76337, 16186, 125615, 16778, 107, 19969, 57132, 11, 46832, 235, 13146, 28002, 143279, 144171, 131097, 40281, 77002, 48408, 53955, 66845, 47818, 126251, 12802, 129624, 33883, 126246, 56419, 126346, 56475, 142300, 130357, 128472, 13, 63757, 32290, 11, 28733, 130000, 12802, 37195, 106, 52959, 89659, 126720, 64577, 129615, 130508, 11, 125714, 126346, 11, 131170, 125544, 136357, 28733, 133085, 33704, 124685, 125476, 130109, 134497, 128472, 15505, 1097, 19, 5053, 382, 12802, 80573, 131050, 54116, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 132182, 126440, 129321, 17877, 125714, 59698, 124905, 11, 68232, 126251, 129242, 130609, 131937, 20401, 137525, 80573, 134585, 68232, 126251, 20401, 129392, 43866, 11, 28733, 85057, 126251, 20401, 124685, 127324, 131837, 137525, 134454, 83315, 53442, 126204, 128472, 13, 23084, 16560, 5140, 228, 235, 124517, 141091, 32831, 53680, 28927, 251, 131837, 95170, 41671, 19391, 136361, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c10d8b0-3604-45b8-95c1-c91c02599db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 20487, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 135392, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 54116, 127033, 137525, 17380, 58677, 33883, 5140, 228, 235, 124517, 46832, 246, 65306, 12802, 137525, 132537, 68232, 126251, 129242, 130609, 131937, 53680, 74808, 76337, 12802, 34143, 105, 50340, 134497, 128472, 382, 136689, 83666, 11, 54116, 127033, 137525, 17380, 58677, 33883, 61298, 126641, 47985, 80573, 129381, 131937, 129889, 48408, 53955, 66845, 68232, 126251, 20401, 129242, 130609, 19969, 95351, 33883, 134497, 128472, 13, 95617, 18411, 129901, 11, 61298, 126641, 47985, 20401, 54116, 127033, 19969, 48408, 53955, 66845, 32831, 42039, 46319, 132537, 70943, 131793, 56039, 56475, 132311, 48408, 131131, 132618, 81133, 60315, 129624, 52959, 126445, 28002, 129616, 20401, 142209, 223, 105, 50340, 136357, 68232, 126251, 12802, 130092, 136448, 129242, 130609, 130357, 128472, 13, 23084, 16560, 5140, 228, 235, 85057, 126251, 66790, 47985, 18411, 81718, 136892, 34395, 132931, 11, 62071, 54330, 47985, 80573, 56419, 131793, 83556, 66845, 56475, 48408, 53955, 66845, 68232, 126251, 129242, 130609, 48108, 112, 133864, 143861, 246, 31079, 60315, 34395, 128472, 13, 130005, 11, 62071, 54330, 47985, 56475, 129242, 130609, 64119, 125615, 61298, 50340, 135402, 53680, 129423, 145237, 12802, 36330, 102, 131226, 48364, 255, 54330, 136448, 141091, 130357, 132931, 11, 32129, 53680, 20401, 55673, 127165, 108, 133692, 139963, 55902, 126204, 128472, 15505, 1097, 19, 5053, 382, 136904, 83666, 11, 54116, 127033, 137525, 16560, 28733, 85057, 126251, 130612, 126440, 129321, 17877, 125714, 59698, 34395, 128472, 13, 81718, 13146, 28733, 130000, 12802, 73077, 125548, 131611, 62099, 105, 126251, 19391, 32129, 16560, 37195, 247, 21329, 20401, 28733, 133085, 131837, 33704, 129423, 43590, 126204, 11, 125686, 136196, 23573, 133553, 17877, 138779, 42905, 8620, 121, 225, 57801, 20401, 124685, 127324, 131837, 33704, 132376, 251, 19969, 126204, 128472, 13, 130005, 11, 62071, 54330, 47985, 136605, 131937, 56475, 72553, 89860, 76337, 16186, 125615, 16778, 107, 19969, 57132, 11, 46832, 235, 13146, 28002, 143279, 144171, 131097, 40281, 77002, 48408, 53955, 66845, 47818, 126251, 12802, 129624, 33883, 126246, 56419, 126346, 56475, 142300, 130357, 128472, 13, 63757, 32290, 11, 28733, 130000, 12802, 37195, 106, 52959, 89659, 126720, 64577, 129615, 130508, 11, 125714, 126346, 11, 131170, 125544, 136357, 28733, 133085, 33704, 124685, 125476, 130109, 134497, 128472, 15505, 1097, 19, 5053, 382, 12802, 80573, 131050, 54116, 127033, 137525, 16560, 5140, 228, 235, 124517, 19391, 132182, 126440, 129321, 17877, 125714, 59698, 124905, 11, 68232, 126251, 129242, 130609, 131937, 20401, 137525, 80573, 134585, 68232, 126251, 20401, 129392, 43866, 11, 28733, 85057, 126251, 20401, 124685, 127324, 131837, 137525, 134454, 83315, 53442, 126204, 128472, 13, 23084, 16560, 5140, 228, 235, 124517, 141091, 32831, 53680, 28927, 251, 131837, 95170, 41671, 19391, 136361, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf6d01",
   "metadata": {},
   "source": [
    "LLM 학습에서 `input_ids`와 `labels`는 모델의 학습 목표에 따라 생성됩니다. 이를 예시 문장과 정수 인코딩을 통해 상세히 설명하겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다.  \n",
    "사용자가 **\"안녕하세요, 오늘 날씨는 어떤가요?\"**라고 물었고,  \n",
    "모델은 **\"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"**라고 응답해야 한다고 합시다.\n",
    "\n",
    "이 데이터를 학습하기 위해 먼저 전체 대화 데이터를 정수로 인코딩합니다.  \n",
    "토크나이저는 대화의 구조를 구분하기 위해 `<|im_start|>`와 `<|im_end|>`을 사용하여 정수 시퀀스를 생성한다고 가정해봅시다.  \n",
    "(실제로는 LLM 템플릿이 이보다는 복잡함을 기억하고 혼동하지 맙시다.)\n",
    "정수 시퀀스는 다음과 같이 구성될 수 있습니다.  \n",
    "\n",
    "---\n",
    "<|im_start|>user 안녕하세요, 오늘 날씨는 어떤가요?<|im_end|>  \n",
    "<|im_start|>assistant 안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|>\n",
    "---\n",
    "\n",
    "이를 정수로 변환하면 다음과 같습니다.  \n",
    "`input_ids = [1001, 2001, 3001, 4001, 5001, 6001, 7001, 1002, 1001, 8001, 9001, 1003, 2002]`  \n",
    "모델이 예측해야 하는 부분은 `assistant`의 응답인 \"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"입니다. 따라서, `labels`는 다음과 같이 설정됩니다.\n",
    "\n",
    "`labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, 8001, 9001, 1003, 2002]`  \n",
    "\n",
    "이처럼 `labels`는 모델의 출력이 필요한 영역만을 포함하고, 나머지 부분은 `-100`으로 채워져 모델이 실제로 예측하고 오차를 계산해야 하는 대상(학습 대상)에서 제외됩니다. 이를 통해 모델은 불필요한 영역을 학습하지 않고, 필요한 응답 영역에만 집중할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff0f9e",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a511a4b7-d2f8-4942-a501-7d7a4e4cfb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb9fbff-19ef-4b4b-8381-11df04aa96a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 31:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.420500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc989b25",
   "metadata": {},
   "source": [
    "## 6. 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a7972",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 '<|im_start|>assistant'가 부착되어서 넣는 것이 좋습니다. 그래야만 모델이 바로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b58447a2-f506-4529-ac01-a8c280cb76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed095c91-c0ab-4b6c-b60c-cf8c8314db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 검색 결과를 바탕으로 질문에 답변해야 합니다.\n",
      "\n",
      "다음의 지시사항을 따르십시오.\n",
      "1. 질문과 검색 결과를 바탕으로 답변하십시오.\n",
      "2. 검색 결과에 없는 내용을 답변하려고 하지 마십시오.\n",
      "3. 질문에 대한 답이 검색 결과에 없다면 검색 결과에는 \"해당 질문~에 대한 내용이 없습니다.\" 라고 답변하십시오.\n",
      "4. 답변할 때 특정 문서를 참고하여 문장 또는 문단을 작성했다면 뒤에 출처는 이중 리스트로 해당 문서 번호를 남기십시오. 예를 들어서 특정 문장이나 문단을 1번 문서에서 인용했다면 뒤에 [[ref1]]이라고 기재하십시오.\n",
      "5. 예를 들어서 특정 문장이나 문단을 1번 문서와 5번 문서에서 동시에 인용했다면 뒤에 [[ref1]], [[ref5]]이라고 기재하십시오.\n",
      "6. 최대한 다수의 문서를 인용하여 답변하십시오.\n",
      "\n",
      "검색 결과:\n",
      "-----\n",
      "문서1: ‘관피아 논란’에도 불구하고 일찌감치 민간으로 옮겨 전문성과 능력을 인정받는 관료 출신들도 적지 않다. 대표적인 인물이 권용원 키움증권 사장이다. 권 사장은 1999년까지 산업자원부 산업기술개발과장(기술고시 21회)을 지내다가 이듬해 다우그룹 전략경영실장으로 이직했다. 다우그룹에서 신사업을 전담한 그는 신생 온라인 증권사였던 키움증권을 대형 증권사로 키워냈다.금융투자업계에선 전병조 KB투자증권 부사장이 눈에 띈다. 6년 전 기획재정부 관료(행정고시 29회)에서 기업금융(IB) 부문 전문가로 변신했다. 재정경제부 출신 이형승 케이스템셀 회장(행시 29회)은 삼성증권(이사) CJ그룹(소장) IBK투자증권(사장)을 거쳐 바이오회사로 갔다. 재정경제부 출신 이현승 전 SK증권 사장(행시 32회)도 AT커니(이사) 메릴린치(이사) GE에너지(사장) 등을 거쳤다.대기업 쪽에도 많다. 박영춘 SK 전무(행시 31회)는 금융위원회 금융정책과장을 거쳐 청와대 비상경제상황실 금융팀장을 지낸 금융 전문가로 SK그룹의 전략업무를 맡고 있다. 박 전무와 행시 동기인 문홍성 두산 부사장은 기획재정부 출신으로 국제금융 전문가로 손꼽힌다.\n",
      "-----\n",
      "문서2: 대구지역 섬유업체인 (주)이주의 이창석 사장은 건축학도의 길을 걷던 1996년 고심 끝에 가업을 이어받기로 했다. 부친의 뜻을 따르기로 했지만 무엇부터 해야 할지 막막했다. 이 사장은 “섬유업은 이미 1990년대 후반부터 선진국과 후발국 사이에 낀 샌드위치 신세라 사업을 어떻게 유지할지 고민이 많았다”고 회상했다.그는 해외로 공장을 옮기는 대신 국내 기술 수준을 끌어올리기로 했다. 생산원가를 낮춰 중국에 대응하는 것보다 선진국처럼 고부가가치 산업으로 탈바꿈하는 게 낫다는 생각에서다.2005년 대표이사로 취임한 뒤 사명을 이주염공에서 (주)이주로 바꿨다. 또 아웃도어 의류에 많이 쓰이는 기능성 소재를 염색하고 가공하는 염색가공업에 집중하기로 했다. 이듬해 8월 기술연구소를 설립했다. 직원이 32명에 불과한 점을 고려하면 파격적 조치였다. 직원 중 20%가 넘는 7명을 연구원으로 충원했다.연구개발(R&D)을 강화한 뒤 길이 열리기 시작했다. 2006년 정부로부터 부품소재전문기업으로 인정받았고 2008년엔 기술혁신형 중소기업으로 선정됐다. 좋은 조건으로 쓸 수 있는 자금도 많아졌다. 2009년과 2011년엔 대구시장 표창과 교육과학기술부 장관 표창을 받았다.(주)이주의 경우 소품종 대량 생산으로 생산원가를 낮추는 방법 대신 다품종 소량 생산을 선택하며 성공을 일군 드문 경우다. 거래 업체에서 주문받는 원단 수를 줄이더라도 고급 염색과 가공에 초점을 맞췄다. 이 사장은 “염색가공업은 의류 원단을 직접 만들어 부가가치를 창출하는 게 아니라 원단을 생산하는 업체에 염색가공 컨설팅을 하는 것이나 마찬가지”라고 설명했다.그래도 30여명의 인력으로 염색가공업을 하는 것은 벅찬 일이었다. 이 사장은 외부 연구소와의 협력으로 인력 부족 문제를 해결했다. 정부 과제를 수주한 뒤 대구·경북 지역의 전문 연구소와 제휴를 맺어 연구 과제를 수행했다.안성익 영남대 경영학과 교수는 “(주)이주의 성공 사례는 경영자의 노력에 따라 사양산업인 섬유업도 지식산업으로 탈바꿈할 수 있다는 점을 보여준다”고 말했다.\n",
      "-----\n",
      "문서3: “물건이 없어서 못 팔 정도로 인기가 많았다.”조창섭 영신물산 대표(사진)가 섬유염색업에 뛰어든 1971년은 업황이 좋았다. 당시 염색기술을 가진 공장이 몇 군데 없었다. 물건을 만들면 바로 동이 났다. 고등학교를 졸업하고 처음 입사한 서울 성수동의 한 염색공장에서 꼬박 19년을 일했다. 이 경험을 바탕으로 1990년 1월5일 경기 양주에 영신물산을 설립했다. 국내업체 간 경쟁이 격화되면서 2003년 남미 과테말라 공장을 세우고 해외로 눈을 돌렸다. 과테말라에서 생산되는 모든 물량은 미국으로 수출된다. 지난해 매출 586억원 가운데 70%에 이른다. 조 대표는 11일 섬유패션업계 발전에 이바지한 공로를 인정받아 산업통상자원부로부터 금탑산업훈장을 받았다.영신물산은 25년간 섬유염색업 한우물만 팠다. 남들은 사양산업이라고 사업을 축소했지만 조 대표는 지속적으로 투자를 늘렸다. 외환위기 당시에도 이탈리아에서 설비를 들여왔다. “이 위기만 넘기면 살아날 수 있다. 여기서 투자를 멈춰선 안 된다”는 신념 때문이었다. 2003년 과테말라로 생산설비를 옮길 때 회사 내부에서 반대가 있었다. “회사가 잘되고 있는데 굳이 모험을 할 필요가 있느냐”는 것이었다. 그는 안주하기 싫었다. 과감히 5000만달러를 투자했다. 전략이 먹혔다. 현재 과테말라에 공장 3개가 있다. 지난해 매출 586억원 가운데 400억원 이상이 과테말라에서 나왔다.산업부는 조 대표의 이 같은 노력이 한국 기업들이 중남미 섬유시장 점유율의 90%를 차지하는 데 기여한 것으로 평가하고 있다. 조 대표는 “지속적인 설비투자와 연구개발을 통해 경쟁력을 유지하겠다”고 말했다.이날 행사에는 정우영 제원화섬 대표와 안희정 한아인터내셔날 대표가 각각 은탑산업훈장과 동탑산업훈장을 받았다.\n",
      "-----\n",
      "문서4: 삼성그룹이 26일 제일모직과 삼성물산의 합병으로 탄생할 통합 법인의 이름을 삼성물산으로 정하면서 ‘제일모직’이라는 사명은 역사 속으로 사라지게 된다. 삼성 창업주인 고(故) 이병철 선대회장이 1954년 9월 당시 자본금 1억환을 들여 제일모직공업주식회사를 설립한 지 61년 만이다.제일모직은 이병철 선대회장이 삼성물산과 제일제당에 이어 세 번째로 세운 회사다. 섬유 원단 대부분을 해외에서 수입하다 보니 양복 한 벌 가격이 직장인 월급 3개월치와 맞먹는 상황을 해결하기 위해 섬유를 국산화하겠다는 목표를 내걸면서다. 제일모직은 1956년 대구에 국내 최초의 모직 공장을 세웠고, 독일 전문기술자를 초빙해 민간기업 최초로 해외 기술을 도입했다. 이 선대회장은 1987년 별세할 때까지 제일모직 등기이사를 맡을 만큼 애정을 가진 것으로 전해졌다.삼성그룹은 2013년 12월 삼성에버랜드와 제일모직 패션부문을 합병할 때도 제일모직이라는 이름을 고수했다. 당시 매출 비중을 놓고 보면 6 대 4 정도로 삼성에버랜드의 사업 규모가 더 컸다. 통합 법인이 부동산 및 레저사업 구조까지 아우르고 있어 ‘에버랜드’라는 사명이 더 적합하다는 평가가 지배적이었지만 제일모직을 사명으로 채택했다. 그만큼 제일모직이라는 이름에 애착이 많았다는 의미다.삼성물산은 이 선대회장이 삼성그룹 계열사 가운데 가장 먼저 창업한 회사다. 1938년 3월 대구시 수동(현 인교동)에서 청과물과 건어물 수출업으로 창업한 ‘삼성상회’가 그 뿌리다. 당시 지상 4층, 지하 1층 건물에 지은 250평 규모의 창고가 전부였다. 삼성상회는 광복 이후 서울로 자리를 옮겨 1948년 삼성물산공사로 이름을 바꿨다. 1951년에는 삼성물산주식회사로 개명했다. 1975년에는 한국 종합상사 1호로 지정됐다. 삼성물산은 1995년 삼성건설을 합병한 이후 건설부문과 상사부문으로 나뉘어 세계 50여개국에서 사업을 전개하고 있다.\n",
      "-----\n",
      "문서5: 이병철 삼성그룹 창업주는 1983년 2월 반도체사업에 뛰어들기로 결정했다. 당시 이 회장은 “삼성의 이익만을 위해서가 아니라 국가적 견지에서 삼성이 먼저 반도체사업을 한다”는 어록을 남겼다. 임원 회의 때마다 “국가가 부흥하면 삼성 같은 건 망해도 또 생길 수 있지만 국가가 망하면 삼성은 영원히 없어진다”고 말한 것과 같은 맥락이었다.전국경제인연합회는 5일 지금은 고인이 된 1세대 창업자 특유의 기업가 정신을 소개한 책을 내놨다. 제목은 ‘한강의 기적과 기업가 정신’이다. 월간조선 편집장 출신인 김용삼 미래한국 편집장이 썼다. 고(故) 이병철 창업주 등 타계한 1세대 기업인들을 탐구했다. 김 편집장은 사업을 통해 국가에 보답한다는 사업보국(事業報國) 정신을 한국 기업가 정신의 전형으로 정의했다. 이 때문에 정부와 기업 간 협업이 많았고 신뢰도 두터웠다는 게 저자의 판단이다. 정주영 현대그룹 명예회장과 박정희 대통령을 대표적 예로 들었다. 정 회장은 1960년대 초 현대건설이 시공하는 소양강댐 건설 방식을 두고 댐 설계 전문가인 구보다 유타카 일본공영 회장과 설전을 벌였다. 구보다 회장은 소양강댐도 일본 댐들처럼 콘크리트와 철근을 이용해 만들어야 한다고 주장했다. 하지만 정 회장은 “소양강 주변에 흙과 모래, 자갈이 지천으로 널려 있으니 암석을 이용한 사력(砂礫)댐으로 건설해야 한다”고 맞섰다.정 회장은 댐에 대해 문외한이나 마찬가지였지만 박 대통령은 정 회장의 손을 들어줬다. 그 결과 공사비를 30% 절감했고 더 안전하다는 평가를 받았다. 코오롱 창업주인 이원만 회장은 1963년 박정희 대통령을 설득해 서울 구로동 수출공업단지 건설을 이끌어냈다. 당시 이 회장은 “일본은 별 기술 없이 포크와 나이프를 수출해 연간 3억달러 이상 벌고 있다”며 “한국 근로자들의 손재주는 일본에 뒤지지 않으니 스테인리스를 잘 닦아 광택을 내면 된다”고 주장했다. 구인회 LG그룹 창업주는 “우리도 머리 싸매고 땀 흘리면 남처럼 못 살 게 없다”고 강조했다. 기아자동차 창업주인 김철호 회장이 보여준 기업 간 신뢰도 한국 기업가 정신의 특이한 점으로 소개됐다. 김 회장은 1973년 국내 최초로 2.0L 가솔린 자동차 엔진을 국산화한 뒤 임원진의 반대를 무릅쓰고 정주영 회장이 기아차 공장을 둘러보도록 허락했다. 김 회장은 “기업이란 주변의 모든 기업이 함께 발전해야 더 클 수 있다”고 강조했다.<|im_end|>\n",
      "<|im_start|>user\n",
      "정부와 기업 간 협업의 사례와 그 결과는 무엇인가요?<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bc52e99-eb23-46d9-a85f-85ef08d3e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "정부와 기업 간 협업의 사례와 그 결과는 다음과 같습니다.\n",
      "\n",
      "1. **(주)이주와 정부의 협업**:\n",
      "   - **사례**: 대구지역 섬유업체인 (주)이주는 2006년 정부로부터 부품소재전문기업으로 인정받고, 2008년에는 기술혁신형 중소기업으로 선정되었습니다. 이 회사는 정부 과제를 수주한 뒤 대구·경북 지역의 전문 연구소와 제휴를 맺어 연구 과제를 수행했습니다.\n",
      "   - **결과**: 이러한 협업을 통해 (주)이주는 좋은 조건으로 자금을 사용할 수 있게 되었고, 대구시장 표창과 교육과학기술부 장관 표창을 받는 등 여러 성과를 거두었습니다. 이는 섬유업이 지식산업으로 탈바꿈할 수 있다는 가능성을 보여주는 사례로 평가받고 있습니다[[ref2]].\n",
      "\n",
      "2. **정주영 현대그룹 명예회장과 박정희 대통령의 협업**:\n",
      "   - **사례**: 1960년대 초 현대건설이 시공하는 소양강댐 건설 방식을 두고 정주영 회장과 박정희 대통령이 협업했습니다. 정 회장은 일본의 댐 설계 전문가와의 설전에서 소양강 주변의 자원을 이용한 사력댐 건설을 주장했고, 박 대통령은 정 회장의 손을 들어주었습니다.\n",
      "   - **결과**: 이 협업의 결과로 공사비를 30% 절감할 수 있었고, 더 안전한 댐을 건설할 수 있었습니다[[ref5]].\n",
      "\n",
      "3. **코오롱 창업주 이원만 회장과 박정희 대통령의 협업**:\n",
      "   - **사례**: 이원만 회장은 1963년 박정희 대통령을 설득해 서울 구로동 수출공업단지 건설을 이끌어냈습니다. 그는 한국 근로자들의 손재주를 활용해 스테인리스 제품을 수출할 수 있다는 점을 강조했습니다.\n",
      "   - **결과**: 이 협업을 통해 구로동 수출공업단지가 건설되었고, 이는 한국의 수출 산업 발전에 기여했습니다[[ref5]].\n",
      "\n",
      "이와 같은 사례들은 정부와 기업 간의 협업이 기업의 성장과 국가 경제 발전에 중요한 역할을 할 수 있음을 보여줍니다.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3522a-b7c9-4406-8df4-9d989044b873",
   "metadata": {},
   "source": [
    "## 7. 파인 튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfe38b",
   "metadata": {},
   "source": [
    "`AutoPeftModelForCausalLM()`의 입력으로 LoRA Adapter가 저장된 체크포인트의 주소를 넣으면 LoRA Adapter가 기존의 LLM과 부착되어 로드됩니다. 이 과정은 LoRA Adapter의 가중치를 사전 학습된 언어 모델(LLM)에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n",
    "\n",
    "`peft_model_id` 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. `\"qwen2-7b-rag-ko/checkpoint-285\"`는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서 해당 가중치를 불러옵니다.\n",
    "\n",
    "`fine_tuned_model`은 `AutoPeftModelForCausalLM.from_pretrained` 메서드를 통해 체크포인트를 로드하여 생성됩니다. 이 메서드는 LLM과 LoRA Adapter를 결합하고, 최적화된 설정으로 모델을 메모리에 로드합니다. `device_map=\"auto\"` 옵션은 모델을 자동으로 GPU에 배치합니다.\n",
    "\n",
    "`pipeline`은 Hugging Face의 고수준 유틸리티로, NLP 작업(예: 텍스트 생성, 번역, 요약 등)을 간단히 수행할 수 있게 해줍니다. 이 코드에서 사용된 `pipeline(\"text-generation\")`은 텍스트 생성 작업을 수행하기 위한 파이프라인 객체를 생성합니다. 파이프라인은 내부적으로 모델과 토크나이저를 관리하여, 입력 텍스트를 토큰화하고, 모델을 통해 생성된 결과를 다시 디코딩하여 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "\n",
    "이 코드는 미세 조정된 LLM을 로드한 뒤, 이를 이용해 텍스트 생성 작업을 간단히 수행할 수 있도록 준비하는 데 목적이 있습니다. `pipeline`을 통해 텍스트 생성 작업을 실행하면, 입력 텍스트에 기반하여 모델이 다음 토큰을 예측하고 이를 반복적으로 생성합니다. 이 과정은 사용자에게 자연스러운 텍스트를 출력하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15a4262b-186b-4abf-b6cf-65d22f3c10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27d9d395-a3c2-47b7-b135-34f4f78f2ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8592a1b604458a95340753327e7d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-285\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b6fbdd3-a8db-48dc-92fa-fb51b20f2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fad932e6-1c1a-4bd8-b1b5-c877a575cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c41ebf47-6905-45ad-b685-a1751613caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "스마트 팩토리 솔루션은 식품 제조업체와 철강업체에 다양한 방식으로 적용되고 있습니다.\n",
      "\n",
      "### 식품 제조업체\n",
      "#### 진성에프엠\n",
      "- **사례**: 에어릭스는 진성에프엠 화성 공장에 스마트팩토리 IoT 솔루션을 구축했습니다. 이 솔루션은 급기·배기 팬의 온도 및 진동을 모니터링하는 IoT 회전체 모니터링 시스템(iCMS)과 PLC 설비 모니터링 시스템을 설치하여, 공장 내 모든 설비에 대한 이상 유무를 실시간 모니터링할 수 있도록 했습니다. 또한, IoT 종합 모니터링 시스템(iTMS)을 구축하여 관리자가 중앙에서 모니터링할 수 있도록 했습니다. 이로 인해 설비 이상을 미연에 방지하고 생산 관리를 개선할 수 있게 되었습니다 [[ref3]].\n",
      "\n",
      "### 철강업체\n",
      "#### 현대제철\n",
      "- **사례**: 현대제철은 현대차그룹의 첫 냉연공장을 통합하고, 철강 사업을 확장했습니다. 이로 인해 쇳물 생산부터 자동차 외장재로 쓰이는 철강 제품 생산에 이르는 전 과정을 통합할 수 있게 되었습니다. 이 결과, 제품 포트폴리오가 다양화되었고, 냉연제품 매출이 증가했습니다. 또한, 원가절감 효과와 무형의 시너지가 발생하여 재무실적이 개선되었습니다. 예를 들어, 코일 무게(단중)가 증가하고, 작업의 안정성과 효율성이 높아졌습니다 [[ref4]].\n",
      "\n",
      "이와 같이, 스마트 팩토리 솔루션은 식품 제조업체와 철강업체에서 생산 과정의 효율性和 안정성을 향상시키는 데 중요한 역할을 하고 있습니다.\n",
      "    label:\n",
      "\n",
      "스마트 팩토리 솔루션은 식품 제조업체와 철강업체에 각각 다른 방식으로 적용되고 있습니다.\n",
      "\n",
      "식품 제조업체의 경우, 스마트 팩토리 솔루션은 주로 생산 관리와 설비 모니터링에 중점을 두고 있습니다. 예를 들어, 농축액 음료 제조업체 '진성에프엠'의 화성 공장에서는 에어릭스의 스마트 팩토리 IoT 솔루션이 도입되었습니다. 이 솔루션은 급기·배기 팬의 온도 및 진동을 모니터링하는 IoT 회전체 모니터링 시스템(iCMS)과 PLC 설비 모니터링 시스템을 포함하고 있습니다. 이를 통해 중앙 모니터링 시스템(iTMS)을 구축하여 관리자가 중앙에서 모든 설비의 이상 유무를 실시간으로 모니터링할 수 있게 되었습니다. 또한, 스마트폰이나 웹을 통해 원격으로 모니터링이 가능하며, 이슈 발생 시 즉시 알림을 받을 수 있습니다. 이러한 시스템은 설비 이상을 미연에 방지하고 생산 관리를 개선하는 데 큰 도움이 됩니다[[ref3]].\n",
      "\n",
      "철강업체의 경우, 스마트 팩토리 솔루션은 주로 생산 공정의 효율성과 품질 향상에 중점을 두고 있습니다. 현대제철의 냉연공장에서는 대형 용융아연도금설비(CGL)를 통해 코일판을 처리하는 과정에서 스마트 팩토리 솔루션이 적용되고 있습니다. 이 공장은 열연과 냉연 과정을 통합하여 제품 포트폴리오를 다양화하고, 생산성을 크게 향상시켰습니다. 예를 들어, 더 큰 코일을 생산함으로써 작업의 안정성과 효율성을 높이고, 버려지는 부분을 줄여 생산성을 향상시켰습니다. 또한, 두 회사의 합병을 통해 자재 구매와 조직 운영을 통합하여 원가 절감 효과를 얻고 있습니다[[ref4]].\n",
      "\n",
      "이와 같이, 스마트 팩토리 솔루션은 식품 제조업체와 철강업체에 각각의 특성에 맞게 적용되어 생산 관리, 설비 모니터링, 생산 공정의 효율성 및 품질 향상에 기여하고 있습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "한국 법조계에서 정치적 독립과 중립성의 중요성은 여러 법조인들의 발언과 행동을 통해 강조되어 왔습니다. 예를 들어, 이강국 헌법재판소 소장은 퇴임식에서 \"헌재는 확실한 정치적 독립과 중립은 물론, 여론과 언론으로부터도 독립해야 한다\"고 강조했습니다. 이는 헌법재판소가 정치적 영향을 받지 않고 독립적으로 판단하는 것이 중요하다는 것을 보여줍니다[[ref1]].\n",
      "\n",
      "또한, 강중인 법조인은 1943년 대전지방법원 검사로 근무하면서 일본 제국의 태평양 전쟁 중 시국 연설과 기고를 통해 사회 활동을 병행했습니다. 그는 경제사범을 경고하고, 국민의 일대수치이므로 엄벌주의로 대처할 것을 주장했습니다. 이는 법조인들이 정치적 독립과 중립성을 유지하면서도 사회적 책임을 수행하는 것이 중요하다는 것을 보여줍니다[[ref5]].\n",
      "\n",
      "이처럼 한국 법조계에서는 정치적 독립과 중립성의 중요성을 강조하며, 이를 통해 법의 공정性和 독립성을 유지하려는 노력을 계속하고 있습니다.\n",
      "    label:\n",
      "\n",
      "한국 법조계에서 정치적 독립과 중립성의 중요성은 여러 법조인들의 발언과 행보를 통해 강조되어 왔습니다. \n",
      "\n",
      "이강국 헌법재판소 소장은 퇴임식에서 헌법재판소의 정치적 독립과 중립성을 강조하며, 헌재가 여론과 언론으로부터도 독립해야 한다고 언급했습니다. 이는 헌법재판소가 정치적 압력이나 외부의 영향 없이 독립적으로 판결을 내려야 한다는 점을 명확히 한 것입니다. 이 소장은 헌재의 위상을 높이고, 헌법의 이념과 가치를 사회 전반에 공고히 뿌리내리도록 노력할 것을 당부했습니다[[ref1]].\n",
      "\n",
      "또한, 서울고등법원 부장판사로 재직했던 한 법관은 과천시장에 대한 무죄 판결을 내린 후, 검찰의 압력에도 불구하고 법적 절차의 공정성을 지키기 위해 노력했다고 밝혔습니다. 이는 법관이 정치적 압력에 굴하지 않고 독립적으로 판결을 내리는 것이 중요하다는 점을 보여줍니다[[ref3]].\n",
      "\n",
      "이러한 사례들은 한국 법조계에서 정치적 독립과 중립성이 얼마나 중요한지, 그리고 이를 지키기 위해 법조인들이 어떤 노력을 기울여왔는지를 잘 보여줍니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "주민등록번호 개편과 관련된 대체 수단으로는 다음과 같은 것들이 있습니다:\n",
      "\n",
      "1. **휴대폰 인증**: 금융거래 등 사적 영역에서 주민번호 사용을 제한하기 위해 휴대폰 인증을 도입할 계획입니다. 이는 휴대폰 번호를 통해 본인 확인을 수행하는 방법입니다[[ref1]].\n",
      "\n",
      "2. **아이핀(I-PIN)**: 휴대폰 인증 외에도, 인터넷 개인 식별 번호(I-PIN)을 사용할 수 있습니다. 이는 온라인에서 본인 확인을 위한 대체 수단입니다[[ref1]].\n",
      "\n",
      "3. **마이핀(My-PIN)**: 주민번호로 본인 여부를 확인해온 개인들에게는 마이핀(My-PIN)을 활용하는 것이 좋다고 권장됩니다. 마이핀은 오프라인 본인 확인 수단으로, 주민번호와 함께 사용될 수 있습니다[[ref1]].\n",
      "\n",
      "이와 같이, 주민등록번호 개편과 관련된 대체 수단으로 휴대폰 인증, I-PIN, 그리고 마이핀(My-PIN)이 주요한 대체 수단들입니다.\n",
      "    label:\n",
      "\n",
      "주민등록번호 개편과 관련된 대체 수단으로는 여러 가지 방안이 검토되고 있습니다. \n",
      "\n",
      "첫째, 정부는 '주민등록 발행번호'를 신설하는 방안을 추진하고 있습니다. 기존 주민등록번호는 주민등록을 위한 본래 목적에만 사용하고, 금융거래 등 개인 인증이 필요한 경우에는 발행번호를 사용하도록 하는 것입니다. 발행번호는 '발행연도+숫자+검증번호'로 구성되어 개인정보 유출 가능성을 낮추는 방식입니다 [[ref1]].\n",
      "\n",
      "둘째, 안전행정부는 휴대폰 인증과 아이핀(I-PIN) 등의 대체 수단을 도입하여 사적 영역에서 주민등록번호 사용을 엄격히 제한할 계획입니다 [[ref1]].\n",
      "\n",
      "셋째, 국가인권위원회는 의료보험 업무에는 건강보험증 번호를, 사회복지 업무에는 사회복지번호를 사용하는 등 목적별 번호제도의 도입과 확산을 대안으로 제시했습니다. 이는 행정 서비스의 종류에 따라 각각 고유한 번호를 발급함으로써 모든 개인 식별 기능이 주민번호에 집중되는 현실을 개선하기 위한 방안입니다 [[ref2]].\n",
      "\n",
      "넷째, 마이핀(My-PIN)이라는 오프라인 본인 확인 수단도 제안되었습니다. 이는 아이핀을 본떠 만든 것으로, 주민번호 대신 사용할 수 있는 대체 수단입니다 [[ref3]].\n",
      "\n",
      "이와 같은 대체 수단들은 주민등록번호의 과도한 사용을 줄이고 개인정보 보호를 강화하기 위한 다양한 방안들입니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "케냐타의 정치적 활동은 케냐의 독립과 공화국 수립에 중요한 역할을 했습니다. 케냐타는 1946년 크와메 은크루마와 함께 범아프리카주의 연합을 창립하였고, 케냐로 귀국하여 그레이스 완지쿠에게 결혼하여 케냐의 정치적 인물로 자리잡았습니다. 그는 케냐 티처스 칼리지의 교장이 되었고, 1947년 캐냐 아프리카 동맹의 회장이 되었습니다[[ref1]].\n",
      "\n",
      "케냐타는 케냐의 독립과 공화국 수립을 위해 노력했습니다. 1961년 5월 14일, 케냐타는 케냐 아프리카 민족 동맹의 의장이 되었고, 1961년 8월 21일 완전히 석방되었습니다. 그는 1963년 12월 12일 케냐가 독립을 선언한 후 총리직을 유지하였고, 1964년 12월 12일 케냐는 공화국이 되었습니다[[ref2]].\n",
      "\n",
      "케냐타의 정치적 활동은 케냐의 독립과 공화국 수립을 가능하게 하였습니다. 그는 케냐의 독립과 공화국 수립을 위해 노력하였고, 케냐의 정치적 안정과 발전에 중요한 역할을 했습니다. 케냐타는 케냐의 독립과 공화국 수립을 위해 노력하였고, 케냐의 정치적 안정과 발전에 중요한 역할을 했습니다[[ref2]].\n",
      "    label:\n",
      "\n",
      "조모 케냐타는 케냐의 독립과 공화국 수립에 중요한 역할을 한 인물입니다. 그의 정치적 활동은 여러 단계를 거쳐 케냐의 독립과 공화국 수립에 큰 영향을 미쳤습니다.\n",
      "\n",
      "케냐타는 1947년 케냐 아프리카 동맹(KAU)의 회장이 되었고, 이로 인해 백인 정착자들로부터 위협을 받기 시작했습니다. 1952년 마우 마우 폭동과 관련된 혐의로 체포되어 7년간 강제 노동형을 선고받고 추방되었으나, 이는 후에 조작된 혐의로 밝혀졌습니다. 그는 1959년까지 교도소에 있었고, 이후 망명 생활을 하였습니다[[ref1]].\n",
      "\n",
      "1960년대 초, 케냐의 긴급 상태가 해제되면서 케냐타는 1961년 완전히 석방되었습니다. 그는 케냐 아프리카 민족 동맹(KANU)의 의장이 되었고, 1963년 케냐가 독립을 선언한 후 총리직을 유지하였습니다. 1964년 케냐가 공화국이 되면서 케냐타는 초대 대통령이 되었습니다[[ref2]].\n",
      "\n",
      "케냐타의 정책은 식민지 시절의 공무원들을 유지하며 연속성을 강조하였고, 영국군의 도움을 받아 소말리인 반란자들을 진압하는 등 안정적인 국가 운영을 추구했습니다. 그는 또한 대지 개혁을 통해 부패와 종족 간의 갈등을 해결하려 했으나, 일부 정책은 부패와 종족 편애로 비판받기도 했습니다. 그의 외교 정책은 친서방적 반공주의를 유지하며 해외 투자를 유치하는 데 기여했습니다[[ref2]].\n",
      "\n",
      "케냐타는 1966년과 1974년에 재선되었으며, 그의 통치 기간 동안 케냐는 상대적으로 안정적인 성장을 이루었지만, 독재적인 정책과 반대파에 대한 탄압으로 논란이 되기도 했습니다. 그의 통치는 케냐의 독립과 초기 공화국 수립에 중요한 기여를 했으며, 케냐의 정치적, 경제적 기반을 다지는 데 큰 역할을 했습니다[[ref2]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "조모 케냐타는 케냐의 정치적 경력과 정책에 대해 다음과 같이 설명할 수 있습니다.\n",
      "\n",
      "**정치적 경력**\n",
      "조모 케냐타는 케냐의 독립 이후의 정치적 중심 인물로, 케냐 아프리카 민족 동맹(KANU)의 일원으로 활동했습니다. 그는 케냐의 독립 이후 총리직을 수행하였고, 1963년 케냐가 독립한 후 총리직을 유지했습니다. 1964년 케냐는 공화국으로 변경되면서 케냐타는 대통령으로서 케냐를 이끌었습니다. 그는 케냐의 정치적 안정을 유지하고, 밀턴 오보테의 우간다와 줄리어스 니에레레의 탄자니아와의 무역 협정을 체결하는 등 중요한 역할을 했습니다. 그러나 그의 독재주의적 정책들은 비판을 받았고, 여러 정치적 반대를 일으켰습니다[[ref1]].\n",
      "\n",
      "**정책**\n",
      "케냐타의 정책은 연속성을 추구하며, 식민지 시절의 공무원들을 그들의 직업에 계속 활용했습니다. 그는 북동부에서 소말리인 반란자들에 대항하는 데 영국군의 도움을 요청하였고, 1971년 나이로비에서 육군 반란이 좌절되도록 강요했습니다. 그는 상대적으로 평화적 대지 개혁을 세웠으며, 나쁜 쪽에서 그의 대지 정책들은 그의 친척들과 친구들에게 주어진 대지의 구획 선택권과 함께 깊게 부패를 참호로 에워싸졌습니다[[ref1]].\n",
      "\n",
      "케냐타는 케냐의 유엔 가입을 감시하고, 밀턴 오보테의 우간다와 줄리어스 니에레레의 탄자니아와의 무역 협정을 체결하는 등 국제적인 역할을 했습니다. 그는 친서방적 반공주의 외교를 속행하였고, 해외 투자를 끌어들였습니다[[ref1]].\n",
      "\n",
      "**비판**\n",
      "그의 정책들은 비판을 받았고, 정치적 반대를 일으켰습니다. 그는 자신의 지배적 키쿠유 족의 남성들이 다른 종족으로부터 대통령을 가지는 아이디어를 좋아하지 않은 것으로 주어진 종족 경쟁로부터 위험에서 케냐 공화국을 남긴 것으로 비판받았습니다[[ref1]].\n",
      "\n",
      "**정치적 위기**\n",
      "케냐타는 1960년 12월 긴급 상태가 향상되었고, 1961년 케냐 아프리카 민족 동맹과 케냐 아프리카 민주 동맹이 그의 석방을 요구했습니다. 그는 1961년 8월 21일 완전히 석방되었고, 1963년 12월 12일 케냐가 독립한 후 총리직을 유지했습니다. 1964년 12월 12일 케냐는 공화국으로 변경되면서 케냐타는 대통령으로서 케냐를 이끌었습니다[[ref1]].\n",
      "\n",
      "**정치적 위기**\n",
      "케냐타는 1966년 재선되었고, 다음해 연장된 권력을 얻기 위해 헌법을 바꾸었습니다. 이 기간 동안 소말리아와 국경 분쟁과 더 많은 정치적 반대를 겪었습니다. 그는 키쿠유 족이 이끄는 케냐 아프리카 민족 동맹을 실지로 케냐의 단 하나의 정당으로 만들었습니다. 그의 비밀 경찰들은 반체제자들을 괴롭혔고, 피오 가마 핀토, 톰 음보야와 J. M. 카리우키 같은 야당 인물들의 살인 사건으로 비판을 받았습니다[[ref1]].\n",
      "\n",
      "**정치적 위기**\n",
      "케냐타는 1974년 재선되었지만, 이 선거는 혼자 나간 자유적도, 공\n",
      "    label:\n",
      "\n",
      "조모 케냐타는 케냐의 독립과 초기 정치적 발전에 중요한 역할을 한 인물입니다. 그의 정치적 경력과 정책을 다음과 같이 요약할 수 있습니다.\n",
      "\n",
      "### 정치적 경력\n",
      "1. **초기 활동과 체포**: 케냐타는 1947년 케냐 아프리카 동맹(KAU)의 회장이 되었고, 1952년 마우 마우 폭동과 관련된 혐의로 체포되어 7년간 강제 노동형을 선고받았습니다. 그는 1959년까지 교도소에 있었고, 이후 망명 생활을 하였습니다[[ref2]].\n",
      "\n",
      "2. **석방과 정치 복귀**: 1961년 케냐타는 완전히 석방되었고, 케냐 아프리카 민족 동맹(KANU)의 의장이 되었습니다. 1963년 케냐가 독립을 선언한 후 총리직을 유지하였고, 1964년 케냐가 공화국이 되면서 초대 대통령이 되었습니다[[ref1]].\n",
      "\n",
      "3. **재선과 권력 강화**: 케냐타는 1966년과 1974년에 재선되었으며, 헌법을 개정하여 권력을 강화하였습니다. 그는 KANU를 사실상 케냐의 유일한 정당으로 만들었고, 비밀 경찰을 통해 반체제 인사들을 억압하였습니다[[ref1]].\n",
      "\n",
      "### 정책\n",
      "1. **연속성 유지**: 케냐타는 많은 식민지 시절의 공무원들을 그들의 직위에 유지시키며 연속성을 강조하였습니다. 또한, 북동부에서 소말리인 반란자들에 대항하기 위해 영국군의 도움을 요청하기도 했습니다[[ref1]].\n",
      "\n",
      "2. **대지 개혁**: 케냐타는 상대적으로 평화적인 대지 개혁을 추진했으나, 그의 대지 정책은 부패와 친족 중심의 특혜로 비판받았습니다. 그는 키쿠유 족을 우대하며 케냐의 가장 큰 대지주가 되었습니다[[ref1]].\n",
      "\n",
      "3. **외교 정책**: 케냐타는 친서방적 반공주의 외교를 유지하며, 케냐의 유엔 가입을 감시하고 우간다와 탄자니아와의 무역 협정을 체결하였습니다. 이러한 안정은 해외 투자를 끌어들였고, 그는 아프리카 여러 곳에서 영향력 있는 인물이 되었습니다[[ref1]].\n",
      "\n",
      "4. **독재적 통치**: 케냐타의 독재적 정책은 비판을 받았으며, 반체제 인사들에 대한 억압과 정치적 암살 사건들로 이어졌습니다. 그의 통치는 종족 간의 경쟁을 심화시키고, 키쿠유 족의 지배를 강화하는 방향으로 나아갔습니다[[ref1]].\n",
      "\n",
      "조모 케냐타는 케냐의 독립과 초기 발전에 중요한 역할을 했지만, 그의 통치는 부패와 독재적 성향으로 인해 논란이 많았습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77bc0b-135e-44bc-aa6f-6e105c9dcbc1",
   "metadata": {},
   "source": [
    "## 8. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7539771-6a09-4669-b828-697ff508d389",
   "metadata": {},
   "source": [
    "이번에는 LoRA Adapter를 merge하지 않은 기본 모델로 테스트 데이터에 대해서 인퍼런스해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "918de7bf-b690-460e-95af-46d74c189cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce15c05125f4d66833310108e13a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa95b03c-71e8-4453-8d97-31255c38b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "스마트 팩토리 솔루션은 식품 제조업체와 철강업체에 다양한 방식으로 적용되고 있습니다.\n",
      "\n",
      "식품 제조업체에 있어서, 에어릭스(대표이사 김군호)가 농축액 음료 제조업체 '진성에프엠' 화성 공장에 스마트팩토리 IoT 솔루션을 구축하였습니다. 이 솔루션은 급기·배기 팬의 온도 및 진동을 모니터링하는 IoT 회전체 모니터링 시스템(iCMS)과 PLC 설비 모니터링 시스템을 설치함으로써 설비 이상을 미연에 방지하고 생산 관리를 개선하는데 도움을 주고 있습니다. 또한, IoT 종합 모니터링 시스템(iTMS)을 구축하여 관리자가 중앙에서 현장 내 모든 설비에 대한 이상 유무를 실시간 모니터링 할 수 있도록 지원하고 있습니다.\n",
      "\n",
      "철강업체에 있어서, 현대제철은 냉연사업 인수를 통해 새로운 날개를 달았습니다. 이로 인해 제품 포트폴리오가 다양화되었고, 재무실적도 개선되었습니다. 특히, 쇳물 생산부터 자동차강판 생산에 이르는 전 과정을 통합함으로써 원가절감 효과와 시너지 효과를 얻었습니다. 이는 코일 무게(단중) 증가와 같은 합병 효과를 통해 나타났습니다. 또한, 각 회사에서의 자재 구매를 통합함으로써 원가절감 효과를 얻었고, 하이스코의 15년 이상 축적한 냉연제품 생산 노하우를 공유함으로써 무형의 시너지를 얻었습니다.\n",
      "    label:\n",
      "\n",
      "스마트 팩토리 솔루션은 식품 제조업체와 철강업체에 각각 다른 방식으로 적용되고 있습니다.\n",
      "\n",
      "식품 제조업체의 경우, 스마트 팩토리 솔루션은 주로 생산 관리와 설비 모니터링에 중점을 두고 있습니다. 예를 들어, 농축액 음료 제조업체 '진성에프엠'의 화성 공장에서는 에어릭스의 스마트 팩토리 IoT 솔루션이 도입되었습니다. 이 솔루션은 급기·배기 팬의 온도 및 진동을 모니터링하는 IoT 회전체 모니터링 시스템(iCMS)과 PLC 설비 모니터링 시스템을 포함하고 있습니다. 이를 통해 중앙 모니터링 시스템(iTMS)을 구축하여 관리자가 중앙에서 모든 설비의 이상 유무를 실시간으로 모니터링할 수 있게 되었습니다. 또한, 스마트폰이나 웹을 통해 원격으로 모니터링이 가능하며, 이슈 발생 시 즉시 알림을 받을 수 있습니다. 이러한 시스템은 설비 이상을 미연에 방지하고 생산 관리를 개선하는 데 큰 도움이 됩니다[[ref3]].\n",
      "\n",
      "철강업체의 경우, 스마트 팩토리 솔루션은 주로 생산 공정의 효율성과 품질 향상에 중점을 두고 있습니다. 현대제철의 냉연공장에서는 대형 용융아연도금설비(CGL)를 통해 코일판을 처리하는 과정에서 스마트 팩토리 솔루션이 적용되고 있습니다. 이 공장은 열연과 냉연 과정을 통합하여 제품 포트폴리오를 다양화하고, 생산성을 크게 향상시켰습니다. 예를 들어, 더 큰 코일을 생산함으로써 작업의 안정성과 효율성을 높이고, 버려지는 부분을 줄여 생산성을 향상시켰습니다. 또한, 두 회사의 합병을 통해 자재 구매와 조직 운영을 통합하여 원가 절감 효과를 얻고 있습니다[[ref4]].\n",
      "\n",
      "이와 같이, 스마트 팩토리 솔루션은 식품 제조업체와 철강업체에 각각의 특성에 맞게 적용되어 생산 관리, 설비 모니터링, 생산 공정의 효율성 및 품질 향상에 기여하고 있습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "한국 법조계에서 정치적 독립과 중립성의 중요성은 여러 문서에서 강조되어 왔습니다.\n",
      "\n",
      "첫번째로, 문서1에서 이강국 헌법재판소 소장은 \"헌재는 확실한 정치적 독립과 중립은 물론, 여론과 언론으로부터도 독립해야 한다\"고 헌재의 중립성을 강조했습니다. 이는 법조계가 정치적 영향을 받지 않고 독립적으로 판단하는 것이 중요하다는 것을 보여줍니다.\n",
      "\n",
      "두번째로, 문서2에서 이 소장은 헌재를 세계적인 헌법재판기관으로 키워보겠다는 꿈을 가지고 있었으며, 이를 통해 법조계의 정치적 독립과 중립성의 중요성을 보여줍니다.\n",
      "\n",
      "세번째로, 문서3에서 서울고등법원 부장판사가 기자와의 만남에서 \"유일한 증거로 제출한 피고인 진술조서를 받으면서 잠 안재우기, 쪼그려 뛰기는 물론 변호인 접견도 기술적으로 막은 것 등이 법정에서 충분히 입증됐다\"고 말함으로써, 법조계는 공정한 판단을 유지해야한다는 점을 강조합니다.\n",
      "\n",
      "이처럼, 한국 법조계는 정치적 독립과 중립성의 중요성을 강조하며, 이러한 가치를 지키기 위한 노력과 책임을 지고 있습니다.\n",
      "    label:\n",
      "\n",
      "한국 법조계에서 정치적 독립과 중립성의 중요성은 여러 법조인들의 발언과 행보를 통해 강조되어 왔습니다. \n",
      "\n",
      "이강국 헌법재판소 소장은 퇴임식에서 헌법재판소의 정치적 독립과 중립성을 강조하며, 헌재가 여론과 언론으로부터도 독립해야 한다고 언급했습니다. 이는 헌법재판소가 정치적 압력이나 외부의 영향 없이 독립적으로 판결을 내려야 한다는 점을 명확히 한 것입니다. 이 소장은 헌재의 위상을 높이고, 헌법의 이념과 가치를 사회 전반에 공고히 뿌리내리도록 노력할 것을 당부했습니다[[ref1]].\n",
      "\n",
      "또한, 서울고등법원 부장판사로 재직했던 한 법관은 과천시장에 대한 무죄 판결을 내린 후, 검찰의 압력에도 불구하고 법적 절차의 공정성을 지키기 위해 노력했다고 밝혔습니다. 이는 법관이 정치적 압력에 굴하지 않고 독립적으로 판결을 내리는 것이 중요하다는 점을 보여줍니다[[ref3]].\n",
      "\n",
      "이러한 사례들은 한국 법조계에서 정치적 독립과 중립성이 얼마나 중요한지, 그리고 이를 지키기 위해 법조인들이 어떤 노력을 기울여왔는지를 잘 보여줍니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "주민등록번호 개편과 관련된 대체 수단으로는 '아이핀(I-PIN·인터넷 개인 식별 번호)'이 제시되었습니다. 이는 주민번호 기반의 본인 확인 수단으로, 온라인 상에서 주민번호를 요구받지 않고 휴대폰 번호와 생년월일만 사용하여 본인 확인을 할 수 있습니다. 또한, '마이핀(My-PIN)'도 주민번호로 본인 여부를 확인해온 개인들에게는 활용할 수 있는 오프라인 본인 확인 수단으로 제안되었습니다. 이러한 대체 수단들은 주민등록번호의 수집이 제한되는 상황에서 개인 식별을 위해 사용될 예정입니다.\n",
      "    label:\n",
      "\n",
      "주민등록번호 개편과 관련된 대체 수단으로는 여러 가지 방안이 검토되고 있습니다. \n",
      "\n",
      "첫째, 정부는 '주민등록 발행번호'를 신설하는 방안을 추진하고 있습니다. 기존 주민등록번호는 주민등록을 위한 본래 목적에만 사용하고, 금융거래 등 개인 인증이 필요한 경우에는 발행번호를 사용하도록 하는 것입니다. 발행번호는 '발행연도+숫자+검증번호'로 구성되어 개인정보 유출 가능성을 낮추는 방식입니다 [[ref1]].\n",
      "\n",
      "둘째, 안전행정부는 휴대폰 인증과 아이핀(I-PIN) 등의 대체 수단을 도입하여 사적 영역에서 주민등록번호 사용을 엄격히 제한할 계획입니다 [[ref1]].\n",
      "\n",
      "셋째, 국가인권위원회는 의료보험 업무에는 건강보험증 번호를, 사회복지 업무에는 사회복지번호를 사용하는 등 목적별 번호제도의 도입과 확산을 대안으로 제시했습니다. 이는 행정 서비스의 종류에 따라 각각 고유한 번호를 발급함으로써 모든 개인 식별 기능이 주민번호에 집중되는 현실을 개선하기 위한 방안입니다 [[ref2]].\n",
      "\n",
      "넷째, 마이핀(My-PIN)이라는 오프라인 본인 확인 수단도 제안되었습니다. 이는 아이핀을 본떠 만든 것으로, 주민번호 대신 사용할 수 있는 대체 수단입니다 [[ref3]].\n",
      "\n",
      "이와 같은 대체 수단들은 주민등록번호의 과도한 사용을 줄이고 개인정보 보호를 강화하기 위한 다양한 방안들입니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "케냐타의 정치적 활동은 케냐의 독립과 공화국 수립에 중요한 역할을 했습니다. \n",
      "\n",
      "첫째, 1946년 케냐타는 크와메 은크루마와 함께 범아프리카주의 연합을 창립하였고, 이는 아프리카의 독립을 촉구하는 중요한 움직임이었습니다. 그의 이 행동은 아프리카의 독립운동에 대한 그의 끈질긴 지지와 헌신을 보여주었고, 이는 결국 케냐의 독립을 가능하게 했습니다.\n",
      "\n",
      "둘째, 케냐타는 1947년 캐냐 아프리카 동맹의 회장이 되었고, 이는 그가 아프리카의 정치적 이슈에 대해 주도적인 역할을 하는 것을 보여줍니다. 그의 이 역할은 케냐의 독립을 촉진시키고, 그 후 케냐가 1963년 12월 12일에 공식적으로 독립을 선언할 수 있도록 했습니다.\n",
      "\n",
      "셋째, 케냐타는 케냐의 공화국 수립에 중요한 역할을 했습니다. 그는 케냐가 공화국으로 전환될 때 총리직을 유지하였고, 이는 그가 케냐의 정치적 안정과 발전에 대한 그의 헌신을 보여줍니다. 그의 이 역할은 케냐가 공화국으로 전환될 때의 정치적 안정을 보장하였고, 이는 케냐의 정치적 발전에 중요한 기여를 했습니다.\n",
      "\n",
      "따라서 케냐타의 정치적 활동은 케냐의 독립과 공화국 수립에 중요한 영향을 미쳤습니다.\n",
      "    label:\n",
      "\n",
      "조모 케냐타는 케냐의 독립과 공화국 수립에 중요한 역할을 한 인물입니다. 그의 정치적 활동은 여러 단계를 거쳐 케냐의 독립과 공화국 수립에 큰 영향을 미쳤습니다.\n",
      "\n",
      "케냐타는 1947년 케냐 아프리카 동맹(KAU)의 회장이 되었고, 이로 인해 백인 정착자들로부터 위협을 받기 시작했습니다. 1952년 마우 마우 폭동과 관련된 혐의로 체포되어 7년간 강제 노동형을 선고받고 추방되었으나, 이는 후에 조작된 혐의로 밝혀졌습니다. 그는 1959년까지 교도소에 있었고, 이후 망명 생활을 하였습니다[[ref1]].\n",
      "\n",
      "1960년대 초, 케냐의 긴급 상태가 해제되면서 케냐타는 1961년 완전히 석방되었습니다. 그는 케냐 아프리카 민족 동맹(KANU)의 의장이 되었고, 1963년 케냐가 독립을 선언한 후 총리직을 유지하였습니다. 1964년 케냐가 공화국이 되면서 케냐타는 초대 대통령이 되었습니다[[ref2]].\n",
      "\n",
      "케냐타의 정책은 식민지 시절의 공무원들을 유지하며 연속성을 강조하였고, 영국군의 도움을 받아 소말리인 반란자들을 진압하는 등 안정적인 국가 운영을 추구했습니다. 그는 또한 대지 개혁을 통해 부패와 종족 간의 갈등을 해결하려 했으나, 일부 정책은 부패와 종족 편애로 비판받기도 했습니다. 그의 외교 정책은 친서방적 반공주의를 유지하며 해외 투자를 유치하는 데 기여했습니다[[ref2]].\n",
      "\n",
      "케냐타는 1966년과 1974년에 재선되었으며, 그의 통치 기간 동안 케냐는 상대적으로 안정적인 성장을 이루었지만, 독재적인 정책과 반대파에 대한 탄압으로 논란이 되기도 했습니다. 그의 통치는 케냐의 독립과 초기 공화국 수립에 중요한 기여를 했으며, 케냐의 정치적, 경제적 기반을 다지는 데 큰 역할을 했습니다[[ref2]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "조모 케냐타는 케냐의 정치가이며, 그의 정치적 경력은 다음과 같습니다:\n",
      "\n",
      "1. **정부의 역할**: 케냐타는 1960년 12월 긴급 상태가 향상되었을 때 케냐 아프리카 동맹당의 후임 케냐 아프리카 민족 동맹 (KANU)과 케냐 아프리카 민주 동맹 (KADU)의 둘 다 석방을 요구했습니다. 그는 1961년 5월 14일 케냐 아프리카 민족 동맹의 의장이 되었고, 1961년 8월 21일에 완전히 석방되었습니다. 그는 1963년 12월 12일 케냐가 독립한 후 총리직을 유지하였고, 1964년 12월 12일 케냐는 공화국이 되었습니다.\n",
      "\n",
      "2. **정책**: 케냐타의 정책은 연속성이 있었고, 그는 식민지 시절의 공무원들을 그들의 이전 직업들에 간직하였습니다. 그는 북동부에서 소말리인 반란자들에 대항하는 데 영국군의 도움을 요청하였고, 1971년 이어서 일어난 나이로비에서 육군 반란은 당시 법무 장관 키틸리 음웬다와 육군 사령관 은돌로 소령이 사임하는 데 강요되면서 좌절되었습니다. \n",
      "\n",
      "3. **경제 및 외교**: 케냐타는 상대적으로 평화적 대지 개혁을 세웠습니다. 그는 나쁜 쪽에서 그의 대지 정책들은 그의 친척들과 친구들에게 주어진 대지의 구획 선택권과 함께 케냐 안에서 깊게 부패를 참호로 에워싸졌습니다. 그는 자신의 명성으로 케냐의 유엔 가입을 감시하고, 밀턴 오보테의 우간다와 줄리어스 니에레레의 탄자니아와의 무역 협정들을 체결하였습니다. 그는 친서방적 반공주의 외교를 속행하였고, 안정은 해외 투자를 끌어들였습니다.\n",
      "\n",
      "4. **독재**: 그의 독재주의적 정책들은 비판을 끌어내고 의견 차이를 일으켰습니다. 그는 1966년 재선되었고, 다음해 연장한 권력을 얻기 위해 헌법을 바꾸었습니다. 이 기간은 소말리아와 국경 분쟁과 더많은 정치적 반대를 가져왔습니다.\n",
      "\n",
      "5. **후기**: 케냐타는 1974년 재선되었는데, 이 선거는 자유적도, 공명하지도 않은 선거였습니다. 그는 자신의 지배적 키쿠유 족의 남성들이 다른 종족으로부터 대통령을 가지는 아이디어를 좋아하지 않은 것으로 주어진 종족 경쟁드로부터 위험에서 케냐 공화국을 남긴 것으로 그의 비평가들에 의하여 고발되었습니다. 그는 대니얼 아랍 모이에 의하여 이어졌습니다.\n",
      "\n",
      "이렇게 볼 때, 조모 케냐타는 케냐의 정치적 역사를 크게 형성한 인물로, 그의 정책과 행동은 케냐의 정치적, 사회적 상황에 큰 영향을 미쳤습니다.\n",
      "    label:\n",
      "\n",
      "조모 케냐타는 케냐의 독립과 초기 정치적 발전에 중요한 역할을 한 인물입니다. 그의 정치적 경력과 정책을 다음과 같이 요약할 수 있습니다.\n",
      "\n",
      "### 정치적 경력\n",
      "1. **초기 활동과 체포**: 케냐타는 1947년 케냐 아프리카 동맹(KAU)의 회장이 되었고, 1952년 마우 마우 폭동과 관련된 혐의로 체포되어 7년간 강제 노동형을 선고받았습니다. 그는 1959년까지 교도소에 있었고, 이후 망명 생활을 하였습니다[[ref2]].\n",
      "\n",
      "2. **석방과 정치 복귀**: 1961년 케냐타는 완전히 석방되었고, 케냐 아프리카 민족 동맹(KANU)의 의장이 되었습니다. 1963년 케냐가 독립을 선언한 후 총리직을 유지하였고, 1964년 케냐가 공화국이 되면서 초대 대통령이 되었습니다[[ref1]].\n",
      "\n",
      "3. **재선과 권력 강화**: 케냐타는 1966년과 1974년에 재선되었으며, 헌법을 개정하여 권력을 강화하였습니다. 그는 KANU를 사실상 케냐의 유일한 정당으로 만들었고, 비밀 경찰을 통해 반체제 인사들을 억압하였습니다[[ref1]].\n",
      "\n",
      "### 정책\n",
      "1. **연속성 유지**: 케냐타는 많은 식민지 시절의 공무원들을 그들의 직위에 유지시키며 연속성을 강조하였습니다. 또한, 북동부에서 소말리인 반란자들에 대항하기 위해 영국군의 도움을 요청하기도 했습니다[[ref1]].\n",
      "\n",
      "2. **대지 개혁**: 케냐타는 상대적으로 평화적인 대지 개혁을 추진했으나, 그의 대지 정책은 부패와 친족 중심의 특혜로 비판받았습니다. 그는 키쿠유 족을 우대하며 케냐의 가장 큰 대지주가 되었습니다[[ref1]].\n",
      "\n",
      "3. **외교 정책**: 케냐타는 친서방적 반공주의 외교를 유지하며, 케냐의 유엔 가입을 감시하고 우간다와 탄자니아와의 무역 협정을 체결하였습니다. 이러한 안정은 해외 투자를 끌어들였고, 그는 아프리카 여러 곳에서 영향력 있는 인물이 되었습니다[[ref1]].\n",
      "\n",
      "4. **독재적 통치**: 케냐타의 독재적 정책은 비판을 받았으며, 반체제 인사들에 대한 억압과 정치적 암살 사건들로 이어졌습니다. 그의 통치는 종족 간의 경쟁을 심화시키고, 키쿠유 족의 지배를 강화하는 방향으로 나아갔습니다[[ref1]].\n",
      "\n",
      "조모 케냐타는 케냐의 독립과 초기 발전에 중요한 역할을 했지만, 그의 통치는 부패와 독재적 성향으로 인해 논란이 많았습니다.<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
