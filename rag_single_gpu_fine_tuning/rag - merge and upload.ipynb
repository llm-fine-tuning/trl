{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cc034cc-c1bd-4e96-ab85-8b2e35a12f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949ad597-e89b-4c7d-9290-75e80e5fc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c62376-7722-42fd-930c-e34bcfb923d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'Qwen/Qwen2-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fa5af99-44da-4d54-801d-38a0015c77ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67338bb30607457484e040cd8369a76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-141\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c63864c-b601-4161-8175-bf995494b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "롯데백화점의 '활기차고 재미있는 전통시장 만들기' 지원 대상은 서울 약수시장·방이시장, 인천 모래내시장, 부산 서면시장 등 8개 전통시장입니다[[ref4]].\n",
      "    label:\n",
      "\n",
      "롯데백화점의 '활기차고 재미있는 전통시장 만들기' 지원 대상이 되는 시장의 개수는 8개입니다. 지원 대상 시장으로는 서울 약수시장, 방이시장, 인천 모래내시장, 부산 서면시장 등이 포함됩니다 [[ref4]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "박영선 의원이 외국인투자촉진법 개정에 반대하며 예산안 처리에 '빨간불'을 켜준 날짜는 2012년 12월 31일 오전입니다. 이날 민주당이 예산안 처리 조건으로 국가정보원 개혁안을 합의했지만, 박영선 의원 등 민주당 일부는 외국인투자촉진법 개정에 반대하여 예산안 처리를 저지했습니다 [[ref4]].\n",
      "    label:\n",
      "\n",
      "박영선 의원이 외국인투자촉진법(외촉법) 개정을 반대한 날짜는 12월 31일입니다. 이 날 박영선 의원은 예산안 처리 조건으로 내세운 국가정보원 개혁안에 합의한 후, 외촉법 개정에 반대하며 예산안 처리에 '빨간불'을 켰습니다. [[ref4]]<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "이번 해에 국내 맥도날드 가맹점 수는 50개로 증가할 예정입니다. 한국 맥도날드는 올해 중 가맹점 50개를 새로 열어 총 103개의 가맹점을 운영할 계획이라고 발표했습니다. 이는 2010년부터 시작된 가맹사업을 통해 4년간 가맹점 53개를 열어온 후의 계획입니다 [[ref4]].\n",
      "    label:\n",
      "\n",
      "한국 맥도날드는 올해 중 가맹점 50개를 새로 열어 총 103개의 가맹점을 운영할 계획이라고 발표했습니다. 또한, 직영점도 50개를 새로 열어 올해 말까지 총 점포 수를 444개 정도로 늘릴 계획입니다[[ref4]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "\"자금성 이야기\" 저자가 이야기의 출발점으로 삼은 지역은 자금성입니다. 자금성은 중국 베이징의 중심부에 위치한 대형 궁궐이며, 청나라의 황제들이 살았던 곳입니다. 저자는 자금성의 주요 건축물을 하나하나 찾아가며, 청나라의 역사와 그 시대에 살았던 사람들에 대해 이야기를 풀어나갑니다 [[ref5]].\n",
      "    label:\n",
      "\n",
      "《자금성 이야기》의 저자는 이야기를 베이징의 톈안먼에서 출발하여 자금성의 주요 건축물을 하나하나 찾아가며 시작합니다. 이 책은 청나라 역사에 각별한 관심을 갖고 공부해 온 저자가 자금성의 설계와 그곳에 살았던 사람들의 삶을 중심으로 이야기를 풀어나갑니다 [[ref5]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "오비맥주 카스 라이트의 대표 색상은 하늘색입니다. 이 색상은 카스 라이트의 브랜드 컬러로, 저칼로리 맥주의 젊고 산뜻한 이미지를 부각하는 역할을 합니다 [[ref1]].\n",
      "    label:\n",
      "\n",
      "오비맥주 카스 라이트의 대표 색상은 하늘색입니다. 최근 디자인 변경에서 카스 라이트의 브랜드 컬러인 하늘색을 기존보다 더욱 진한 색으로 적용하여 저칼로리 맥주의 젊고 산뜻한 이미지를 부각시켰습니다 [[ref1]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()\n",
    " \n",
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f44db99-0241-4910-887f-6e64e0a09aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-08 05:56:12--  https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1351 (1.3K) [text/plain]\n",
      "Saving to: ‘merge.py.1’\n",
      "\n",
      "merge.py.1          100%[===================>]   1.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-08 05:56:12 (99.0 MB/s) - ‘merge.py.1’ saved [1351/1351]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ukairia777/LLM-Finetuning-tutorial/main/merge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ae0d49-e0b2-4e1f-a557-0f0d42bd7d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen2-7B-Instruct\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:28<00:00,  7.07s/it]\n",
      "Loading PEFT: ./qwen2-7b-rag-ko/checkpoint-141\n",
      "Running merge_and_unload\n",
      "Model saved to ./output_dir\n"
     ]
    }
   ],
   "source": [
    "!python merge.py \\\n",
    "    --base_model_name_or_path Qwen/Qwen2-7B-Instruct \\\n",
    "    --peft_model_path ./qwen2-7b-rag-ko/checkpoint-141 \\\n",
    "    --output_dir ./output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59dd41ee-4ed3-4172-b5e5-b54771de906b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978bd89edce84d22b59f93bf7e1c137a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "롯데백화점의 '활기차고 재미있는 전통시장 만들기' 지원 대상은 서울 약수시장·방이시장, 인천 모래내시장, 부산 서면시장 등 8개 전통시장입니다[[ref4]].\n",
      "    label:\n",
      "\n",
      "롯데백화점의 '활기차고 재미있는 전통시장 만들기' 지원 대상이 되는 시장의 개수는 8개입니다. 지원 대상 시장으로는 서울 약수시장, 방이시장, 인천 모래내시장, 부산 서면시장 등이 포함됩니다 [[ref4]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "박영선 의원이 외국인투자촉진법 개정을 반대한 날짜는 2012년 12월 31일 오전입니다. 박영선 의원은 외국인투자촉진법 개정을 처리하지 않으면 국가정보원 개혁안을 처리하지 않겠다고 주장했으나, 결국 외국인투자촉진법 개정을 처리하지 않으면 국가정보원 개혁안을 처리하지 않겠다는 새누리당의 입장에 맞서 외국인투자촉진법 개정을 반대했습니다 [[ref4]].\n",
      "    label:\n",
      "\n",
      "박영선 의원이 외국인투자촉진법(외촉법) 개정을 반대한 날짜는 12월 31일입니다. 이 날 박영선 의원은 예산안 처리 조건으로 내세운 국가정보원 개혁안에 합의한 후, 외촉법 개정에 반대하며 예산안 처리에 '빨간불'을 켰습니다. [[ref4]]<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "이번 해에 국내 맥도날드 가맹점 수는 50개로 증가할 예정입니다. 한국 맥도날드는 올해 중 가맹점 50개를 새로 열어 총 103개의 가맹점을 운영할 계획이라고 발표했습니다. 이는 2010년부터 시작된 가맹사업을 통해 4년간 가맹점 53개를 열어온 후의 계획입니다 [[ref4]].\n",
      "    label:\n",
      "\n",
      "한국 맥도날드는 올해 중 가맹점 50개를 새로 열어 총 103개의 가맹점을 운영할 계획이라고 발표했습니다. 또한, 직영점도 50개를 새로 열어 올해 말까지 총 점포 수를 444개 정도로 늘릴 계획입니다[[ref4]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "\"자금성 이야기\" 저자가 이야기의 출발점으로 삼은 지역은 자금성입니다. 자금성은 중국 베이징의 중심부에 위치한 대형 궁궐이며, 청나라의 황제들이 살았던 곳입니다. 저자는 자금성의 주요 건축물을 찾아가며, 청나라의 역사와 그 시대에 살았던 사람들에 대해 이야기를 풀어나갑니다 [[ref5]].\n",
      "    label:\n",
      "\n",
      "《자금성 이야기》의 저자는 이야기를 베이징의 톈안먼에서 출발하여 자금성의 주요 건축물을 하나하나 찾아가며 시작합니다. 이 책은 청나라 역사에 각별한 관심을 갖고 공부해 온 저자가 자금성의 설계와 그곳에 살았던 사람들의 삶을 중심으로 이야기를 풀어나갑니다 [[ref5]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "오비맥주 카스 라이트의 대표 색상은 하늘색입니다. 이 색상은 카스 라이트의 브랜드 컬러로, 저칼로리 맥주의 젊고 산뜻한 이미지를 부각하는 역할을 합니다 [[ref1]].\n",
      "    label:\n",
      "\n",
      "오비맥주 카스 라이트의 대표 색상은 하늘색입니다. 최근 디자인 변경에서 카스 라이트의 브랜드 컬러인 하늘색을 기존보다 더욱 진한 색으로 적용하여 저칼로리 맥주의 젊고 산뜻한 이미지를 부각시켰습니다 [[ref1]].<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_id = './output_dir'\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbe5d8a5-e415-4812-99e8-f9843c8989bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "username = \"iamjoon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9fd9a1d-bc4c-44ad-890f-5ea9fb8e4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'qwen2-7b-rag-ko-checkpoint-141'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42f119af-0073-48b8-8dd0-e8d8f5adf7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f967a9a3bfb4bd29365e6dcd32d8667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8075b933da594632bbee23b41439cea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5892cf72ed8c40728684be0a19c30037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d1887416034462ac4bd6b4609df612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846cc5deabc34662a86140493698dca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4553a0029c58494e9d0d2a0afa983237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/iamjoon/qwen2-7b-rag-ko-checkpoint-141/commit/ca0e47a492ac9cfad8411de952460cf2aaeeedc7', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ca0e47a492ac9cfad8411de952460cf2aaeeedc7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/iamjoon/qwen2-7b-rag-ko-checkpoint-141', endpoint='https://huggingface.co', repo_type='model', repo_id='iamjoon/qwen2-7b-rag-ko-checkpoint-141'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(\n",
    "    token=\"hf_여러분의 KEY\",\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    token=\"hf_여러분의 KEY\",\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"output_dir\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
