{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5ef664-1b79-40dc-97e3-a47d7d49540e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.86.0)\n",
      "Requirement already satisfied: vllm==0.8.2 in /usr/local/lib/python3.10/dist-packages (0.8.2)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (6.0.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (5.9.6)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (4.67.1)\n",
      "Requirement already satisfied: blake3 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (1.0.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (9.0.0)\n",
      "Collecting transformers>=4.48.2 (from vllm==0.8.2)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.20.3)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (6.31.1)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.8.2) (0.115.12)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (3.12.13)\n",
      "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (2.11.7)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.18.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (11.2.1)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.9.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.10.11)\n",
      "Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.7.29)\n",
      "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.1.11)\n",
      "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.16 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.1.16)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (4.14.0)\n",
      "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (3.18.0)\n",
      "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.2.1.1.post5)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (24.0.1)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.19.0)\n",
      "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from vllm==0.8.2) (4.6.4)\n",
      "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from mistral_common[opencv]>=1.5.4->vllm==0.8.2) (1.6.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (6.0.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.8.1)\n",
      "Requirement already satisfied: compressed-tensors==0.9.2 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.9.2)\n",
      "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.18.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (3.1.1)\n",
      "Requirement already satisfied: watchfiles in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (1.0.5)\n",
      "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (2.0.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (1.15.3)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (1.11.1.4)\n",
      "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.60.0)\n",
      "Requirement already satisfied: ray>=2.43.0 in /usr/local/lib/python3.10/dist-packages (from ray[cgraph]>=2.43.0->vllm==0.8.2) (2.47.0)\n",
      "Collecting torch==2.6.0 (from vllm==0.8.2)\n",
      "  Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (2.6.0)\n",
      "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.21.0)\n",
      "Requirement already satisfied: xformers==0.0.29.post2 in /usr/local/lib/python3.10/dist-packages (from vllm==0.8.2) (0.0.29.post2)\n",
      "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from depyf==0.18.0->vllm==0.8.2) (0.8.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from depyf==0.18.0->vllm==0.8.2) (0.3.8)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.60.0->vllm==0.8.2) (0.43.0)\n",
      "Requirement already satisfied: interegular in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (3.1.6)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (1.5.8)\n",
      "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (5.6.3)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (0.30.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (4.24.0)\n",
      "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (20250523)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.8.2) (0.1.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm==0.8.2) (3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm==0.8.2) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm==0.8.2) (9.1.0.70)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm==0.8.2) (0.6.2)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm==0.8.2) (12.4.127)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->vllm==0.8.2)\n",
      "  Using cached triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->vllm==0.8.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm==0.8.2) (1.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.46.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.0.7)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.8.2) (0.0.20)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[standard]>=0.115.0->vllm==0.8.2) (2.2.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.34.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm==0.8.2) (23.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common[opencv]>=1.5.4->vllm==0.8.2) (4.11.0.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.8.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.8.2) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.8.2) (0.4.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm==0.8.2) (8.2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm==0.8.2) (1.1.1)\n",
      "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.10/dist-packages (from ray[cgraph]>=2.43.0->vllm==0.8.2) (13.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.8.2) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.8.2) (1.26.13)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm==0.8.2) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm==0.8.2) (0.33.0)\n",
      "Collecting tokenizers>=0.19.1 (from vllm==0.8.2)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.48.2->vllm==0.8.2) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.8.2) (1.20.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm==0.8.2) (2.7.0)\n",
      "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.16.0)\n",
      "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.14.7)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.8.2) (1.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.1.11->vllm==0.8.2) (2.1.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.8.2) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.1.11->vllm==0.8.2) (0.12.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (1.1.0)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (15.0.1)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x->ray[cgraph]>=2.43.0->vllm==0.8.2) (0.8.3)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (14.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm==0.8.2) (0.1.2)\n",
      "Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, tokenizers, transformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.0.0\n",
      "    Uninstalling triton-3.0.0:\n",
      "      Successfully uninstalled triton-3.0.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 tokenizers-0.21.1 torch-2.6.0 transformers-4.52.4 triton-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai vllm==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d455e7-3c11-4a33-a3ca-e5a72639bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-15 16:11:57 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90713ee6-2d2e-4d57-8225-dd4a6d0f257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-15 16:11:58 [config.py:2614] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 06-15 16:12:06 [config.py:585] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-15 16:12:06 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-15 16:12:08 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='iamjoon/qwen2-7b-rag-ko-checkpoint-285', speculative_config=None, tokenizer='iamjoon/qwen2-7b-rag-ko-checkpoint-285', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=iamjoon/qwen2-7b-rag-ko-checkpoint-285, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 06-15 16:12:08 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7570cfded0c0>\n",
      "INFO 06-15 16:12:09 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-15 16:12:09 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-15 16:12:09 [gpu_model_runner.py:1174] Starting to load model iamjoon/qwen2-7b-rag-ko-checkpoint-285...\n",
      "WARNING 06-15 16:12:09 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-15 16:12:10 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41848060f8ab4bba96ec0cc6cdf0ecf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deec176b35c34eb08f6e05a1a13a4206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf17d9b8eb54e9aa089726bcecf025c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261d249d3c4e4993b612dcd187a7e27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-15 16:14:28 [weight_utils.py:281] Time spent downloading weights for iamjoon/qwen2-7b-rag-ko-checkpoint-285: 137.856358 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ec101c9313457e8c5114ea51cbde0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50ff212825044ccad7e033999f35dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-15 16:14:41 [loader.py:447] Loading weights took 12.99 seconds\n",
      "INFO 06-15 16:14:42 [gpu_model_runner.py:1186] Model loading took 14.2487 GB and 152.313211 seconds\n",
      "INFO 06-15 16:14:48 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/0fe1359ac4/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-15 16:14:48 [backends.py:425] Dynamo bytecode transform time: 6.52 s\n",
      "INFO 06-15 16:14:52 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 06-15 16:15:14 [backends.py:144] Compiling a graph for general shape takes 25.80 s\n",
      "INFO 06-15 16:15:29 [monitor.py:33] torch.compile takes 32.32 s in total\n",
      "INFO 06-15 16:15:29 [kv_cache_utils.py:566] GPU KV cache size: 953,616 tokens\n",
      "INFO 06-15 16:15:29 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 29.10x\n",
      "INFO 06-15 16:15:52 [gpu_model_runner.py:1534] Graph capturing finished in 23 secs, took 0.48 GiB\n",
      "INFO 06-15 16:15:52 [core.py:151] init engine (profile, create kv cache, warmup model) took 70.67 seconds\n"
     ]
    }
   ],
   "source": [
    "vllm_model = LLM(\n",
    "    model=\"iamjoon/qwen2-7b-rag-ko-checkpoint-285\",\n",
    "    dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872b0c31-76af-4bbb-9190-8c177df4453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('test_dataset')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"iamjoon/qwen2-7b-rag-ko-checkpoint-285\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63e1802-9b18-4700-8350-2083cd1698a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "questions = []\n",
    "contexts = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1].split('<|im_end|>')[0]\n",
    "    question = text.split('<|im_start|>user')[1].split('<|im_end|>')[0].strip()\n",
    "    context = text.split('검색 결과:\\n-----')[1].split('<|im_end|>')[0].strip()\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)\n",
    "    questions.append(question)\n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0ebc98-6cfb-493e-823e-15b1588fb50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'리자베타가 꿈 속에서 들었던 장소에 실제로 찾아가 발견한 것은?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72237b06-cf5e-486c-a86f-7667341d62b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n리자베타는 꿈속에서 남자가 나타나 억울한 사연을 말하자, 꿈속에서 말한 장소에 가서 땅을 파보니 남자의 시체를 발견했습니다. 리자베타는 시체를 끌고 갈 힘이 없어서 머리통만 잘라서 들고 갔습니다. 이후 그녀는 남자의 머리를 통에 집어 넣고 흙으로 묻어 주었고, 그 자리에서 아름다운 꽃이 피어났습니다 [[ref2]].'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lst[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1858d8e1-1073-4c83-939e-8a84bc2b046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb19a33-e6e7-4be1-9602-804c47872d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 50/50 [00:15<00:00,  3.32it/s, est. speed input: 7252.43 toks/s, output: 562.13 toks/s]\n"
     ]
    }
   ],
   "source": [
    "preds = vllm_model.generate(prompt_lst[:50], sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c689597d-d9ef-42b2-9770-a4a814ef21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [pred.outputs[0].text for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8afa3f-99a7-4290-9d42-71c836409727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API 초기화\n",
    "client = OpenAI(api_key=\"여러분의 키 값\")  # 본인의 API 키로 교체하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffde7ff-c2ce-44a1-9094-9e7d0599ddf7",
   "metadata": {},
   "source": [
    "## LLM 기반의 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69bf5c92-fc01-4b17-8fb1-a6d802db7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_with_llm(questions, contexts, predictions, labels):\n",
    "    \"\"\"\n",
    "    LLM 평가 메트릭으로 RAG 시스템 평가\n",
    "    \n",
    "    Args:\n",
    "        questions: 질문 리스트\n",
    "        contexts: 검색 결과 리스트\n",
    "        predictions: 예측 리스트\n",
    "        labels: 레이블 리스트\n",
    "    \n",
    "    Returns:\n",
    "        평가 결과가 포함된 데이터프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    results = []\n",
    "    \n",
    "    # 평가 프롬프트\n",
    "    prompt_template = \"\"\"\n",
    "당신은 RAG(Retrieval-Augmented Generation) 시스템 평가 전문가입니다. 아래 정보를 바탕으로 생성된 답변의 품질을 철저히 평가해주세요.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "검색된 컨텍스트:\n",
    "{context}\n",
    "\n",
    "생성된 답변:\n",
    "{prediction}\n",
    "\n",
    "참조 답변(정답):\n",
    "{label}\n",
    "\n",
    "다음 4가지 평가 기준으로 1-5점 척도로 점수를 매겨주세요:\n",
    "\n",
    "1. 응답 정확성 (Answer Correctness) [1-5]:\n",
    "   * 생성된 답변이 참조 답변과 비교하여 정확하고 완전한 정보를 제공하는지 평가\n",
    "   * 1점: 완전히 잘못된 정보\n",
    "   * 2점: 부분적으로 관련된 정보를 담고 있으나 대부분 부정확함\n",
    "   * 3점: 정확한 정보와 부정확한 정보가 혼재되어 있음\n",
    "   * 4점: 대부분 정확하지만 일부 정보가 누락되거나 미미한 오류가 있음\n",
    "   * 5점: 참조 답변과 비교했을 때 완전히 정확하고 포괄적인 정보를 제공함\n",
    "\n",
    "2. 컨텍스트 관련성 (Context Relevance) [1-5]:\n",
    "   * 검색된 컨텍스트가 질문에 대답하기 위해 관련성이 높은지 평가\n",
    "   * 1점: 컨텍스트가 질문과 전혀 관련이 없음\n",
    "   * 2점: 컨텍스트가 질문과 간접적으로만 관련됨\n",
    "   * 3점: 컨텍스트 중 일부만 질문과 직접적으로 관련됨\n",
    "   * 4점: 대부분의 컨텍스트가 질문과 직접적으로 관련됨\n",
    "   * 5점: 모든 컨텍스트가 질문에 완벽하게 관련되어 있고 불필요한 정보가 없음\n",
    "\n",
    "3. 컨텍스트 충실성 (Context Faithfulness) [1-5]:\n",
    "   * 생성된 답변이 주어진 컨텍스트에만 기반하는지, 아니면 없는 정보를 추가했는지 평가\n",
    "   * 1점: 답변이 컨텍스트에 없는 정보로만 구성됨 (심각한 환각)\n",
    "   * 2점: 답변이 주로 컨텍스트에 없는 정보로 구성됨\n",
    "   * 3점: 답변이 컨텍스트 정보와 없는 정보가 혼합되어 있음\n",
    "   * 4점: 답변이 주로 컨텍스트에 기반하지만 약간의 추가 정보가 있음\n",
    "   * 5점: 답변이 전적으로 컨텍스트에 있는 정보만을 사용함\n",
    "\n",
    "4. 컨텍스트 충분성 (Context Recall) [1-5]:\n",
    "   * 검색된 컨텍스트가 질문에 완전히 답변하기에 충분한 정보를 포함하는지 평가\n",
    "   * 1점: 컨텍스트가 답변에 필요한 정보를 전혀 포함하지 않음\n",
    "   * 2점: 컨텍스트가 필요한 정보의 일부만 포함함\n",
    "   * 3점: 컨텍스트가 필요한 정보의 약 절반을 포함함\n",
    "   * 4점: 컨텍스트가 필요한 정보의 대부분을 포함하지만 일부 누락됨\n",
    "   * 5점: 컨텍스트가 질문에 완전히 답변하기 위한 모든 필요한 정보를 포함함\n",
    "\n",
    "반드시 다음 JSON 형식으로만 응답하세요. 마크다운은 사용하지 않습니다.:\n",
    "{\n",
    "  \"answer_correctness\": 정수로 된 점수(1-5),\n",
    "  \"context_relevance\": 정수로 된 점수(1-5),\n",
    "  \"context_faithfulness\": 정수로 된 점수(1-5),\n",
    "  \"context_recall\": 점수(1-5),\n",
    "  \"analysis\": \"종합적인 분석 의견\"\n",
    "}\n",
    "\n",
    "다른 형식의 응답은 하지 마세요. 오직 마크다운이 아닌 JSON만 반환하세요.\n",
    "\"\"\"\n",
    "    \n",
    "    # 각 항목에 대해 평가 수행\n",
    "    for i in tqdm(range(len(questions)), total=len(questions), desc=\"RAG 평가 진행 중\"):\n",
    "        try:\n",
    "            # 프롬프트 생성 - format 대신 replace 사용\n",
    "            prompt = prompt_template\n",
    "            prompt = prompt.replace(\"{question}\", str(questions[i]) if questions[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{context}\", str(contexts[i]) if contexts[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{prediction}\", str(predictions[i]) if predictions[i] is not None else \"\")\n",
    "            prompt = prompt.replace(\"{label}\", str(labels[i]) if labels[i] is not None else \"\")\n",
    "\n",
    "            # GPT-4 API 호출\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 RAG 평가 도구입니다. 반드시 유효한 JSON 형식으로만 응답하세요.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            # 결과 파싱\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # 개별 메트릭 점수 추출\n",
    "            answer_correctness = result['answer_correctness']\n",
    "            context_relevance = result['context_relevance']\n",
    "            context_faithfulness = result['context_faithfulness']\n",
    "            context_recall = result['context_recall']\n",
    "            \n",
    "            # 총점 직접 계산 (개별 메트릭의 합)\n",
    "            total_score = answer_correctness + context_relevance + context_faithfulness + context_recall\n",
    "            \n",
    "            # 원본 데이터와 평가 결과 결합\n",
    "            row_result = {\n",
    "                'id': i,\n",
    "                'question': questions[i],\n",
    "                'answer_correctness': answer_correctness,\n",
    "                'context_relevance': context_relevance,\n",
    "                'context_faithfulness': context_faithfulness,\n",
    "                'context_recall': context_recall,\n",
    "                'total_score': total_score,\n",
    "                'analysis': result['analysis']\n",
    "            }\n",
    "            \n",
    "            results.append(row_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"항목 {i} 평가 중 오류 발생: {e}\")\n",
    "            results.append({\n",
    "                'id': i,\n",
    "                'question': questions[i],\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # 결과 데이터프레임 생성\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 요약 통계 계산\n",
    "    if 'total_score' in results_df.columns:\n",
    "        metrics_summary = {\n",
    "            '평균 총점': results_df['total_score'].mean(),\n",
    "            '응답 정확성 평균': results_df['answer_correctness'].mean(),\n",
    "            '컨텍스트 관련성 평균': results_df['context_relevance'].mean(),\n",
    "            '컨텍스트 충실성 평균': results_df['context_faithfulness'].mean(),\n",
    "            '컨텍스트 충분성 평균': results_df['context_recall'].mean()\n",
    "        }\n",
    "        print(\"\\n===== 평가 요약 =====\")\n",
    "        for metric, value in metrics_summary.items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "    \n",
    "    return results_df, metrics_summary if 'total_score' in results_df.columns else results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4096629-1b5e-485e-8789-31077d1340cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG 평가 진행 중: 100%|██████████| 50/50 [02:34<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 평가 요약 =====\n",
      "평균 총점: 18.64\n",
      "응답 정확성 평균: 4.30\n",
      "컨텍스트 관련성 평균: 4.88\n",
      "컨텍스트 충실성 평균: 4.52\n",
      "컨텍스트 충분성 평균: 4.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df, metrics_summary = evaluate_rag_with_llm(questions[:50], contexts[:50], preds[:50], label_lst[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f231219b-2bf3-4728-87ed-354536bedab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>context_relevance</th>\n",
       "      <th>context_faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>total_score</th>\n",
       "      <th>analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>유아용 전동자동차와 장난감 레고 세트를 경품 추첨으로 주는 아파트는?</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>생성된 답변은 질문에 대한 정확한 정보를 제공하지 못했습니다. '김포한강신도시 반도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009년 이혼 소송 접수 건수는 몇 건인가?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 정확히 일치하며, 검색된 컨텍스트에서 제공된 정보를 정확...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>규제로 인해 어떤 종류의 매장이 증가했는가?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>조텍컵에서 입상하면 어떤 플랫폼을 통해 상금을 받을 수 있는가?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>직무부트캠프를 제공하는 회사를 운영하는 사람의 이름은?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                question  answer_correctness  \\\n",
       "0   0  유아용 전동자동차와 장난감 레고 세트를 경품 추첨으로 주는 아파트는?                   1   \n",
       "1   1               2009년 이혼 소송 접수 건수는 몇 건인가?                   5   \n",
       "2   2                규제로 인해 어떤 종류의 매장이 증가했는가?                   5   \n",
       "3   3     조텍컵에서 입상하면 어떤 플랫폼을 통해 상금을 받을 수 있는가?                   5   \n",
       "4   4          직무부트캠프를 제공하는 회사를 운영하는 사람의 이름은?                   5   \n",
       "\n",
       "   context_relevance  context_faithfulness  context_recall  total_score  \\\n",
       "0                  5                     1               5           12   \n",
       "1                  5                     5               5           20   \n",
       "2                  5                     5               5           20   \n",
       "3                  5                     5               5           20   \n",
       "4                  5                     5               5           20   \n",
       "\n",
       "                                            analysis  \n",
       "0  생성된 답변은 질문에 대한 정확한 정보를 제공하지 못했습니다. '김포한강신도시 반도...  \n",
       "1  생성된 답변은 참조 답변과 정확히 일치하며, 검색된 컨텍스트에서 제공된 정보를 정확...  \n",
       "2  생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....  \n",
       "3  생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....  \n",
       "4  생성된 답변은 참조 답변과 비교했을 때 정확하고 포괄적인 정보를 제공하고 있습니다....  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6cfcc-792b-4514-8bea-f47b84444fd1",
   "metadata": {},
   "source": [
    "## 정량 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9dccc4-45f4-435b-a186-0f34c9033f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ref_numbers(text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    텍스트에서 [[ref숫자]] 패턴의 숫자들을 추출하여 중복 없이 정렬된 리스트로 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분석할 텍스트\n",
    "\n",
    "    Returns:\n",
    "        List[int]: 추출된 숫자들의 정렬된 리스트\n",
    "\n",
    "    Example:\n",
    "        >>> text = \"이것은 [[ref1]]과 [[ref2]], [[ref1]]입니다.\"\n",
    "        >>> extract_ref_numbers(text)\n",
    "        [1, 2]\n",
    "    \"\"\"\n",
    "    # [[ref숫자]] 패턴을 찾는 정규표현식\n",
    "    pattern = r'\\[\\[ref(\\d+)\\]\\]'\n",
    "\n",
    "    # 모든 매치를 찾아서 숫자만 추출\n",
    "    numbers = [int(match.group(1)) for match in re.finditer(pattern, text)]\n",
    "\n",
    "    # 중복 제거하고 정렬하여 반환\n",
    "    return sorted(list(set(numbers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cd765ac-db09-4ecf-a245-b2c7cbdc9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = '''제3차 중동 전쟁과 아랍의 겨울의 주요 원인과 결과는 다음과 같습니다.\n",
    "\n",
    "### 제3차 중동 전쟁 (6일 전쟁)\n",
    "**주요 원인:**\n",
    "1. **티란 해협 봉쇄:** 이집트가 이스라엘 선박의 티란 해협 통과를 봉쇄하면서 긴장이 고조되었습니다. 이스라엘은 이를 전쟁의 명분으로 삼았습니다[[ref1]].\n",
    "2. **군사적 준비:** 이집트는 이스라엘과의 국경에 군을 배치하고, 이스라엘은 이에 대응해 예방적 공습을 감행했습니다[[ref1]].\n",
    "\n",
    "**주요 결과:**\n",
    "1. **영토 확장:** 이스라엘은 가자 지구, 시나이 반도, 동예루살렘, 요르단 강 서안 지구, 골란 고원을 점령하여 영토를 3배로 확장했습니다[[ref1]].\n",
    "2. **난민 발생:** 전쟁으로 인해 약 100,000명의 시리아인과 300,000명의 팔레스타인인이 난민이 되었습니다[[ref1]].\n",
    "3. **군사적 자만:** 이스라엘의 승리는 군사적 자만을 초래하여 이후 1973년 욤키푸르 전쟁에서 아랍 연합군의 초기 승리를 가능하게 했습니다[[ref1]].\n",
    "\n",
    "### 아랍의 겨울\n",
    "**주요 원인:**\n",
    "1. **아랍의 봄의 실패:** 2011년 아랍의 봄 이후 민주화와 개혁을 목표로 한 시도들이 실패하면서 권위주의가 복고되고, 사회적 불안정이 증가했습니다[[ref4]].\n",
    "2. **이슬람 극단주의의 부상:** 이슬람 극단주의와 테러 단체들이 활동을 강화하면서 지역 내 폭력과 혼란이 가중되었습니다[[ref4]].\n",
    "\n",
    "**주요 결과:**\n",
    "1. **내전과 사회 불안정:** 시리아 내전, 이라크 내전, 예멘 내전 등 여러 지역에서 내전과 사회 불안정이 발생했습니다[[ref4]].\n",
    "2. **경제적 손실:** 아랍의 겨울로 인해 발생한 사회적 손실비용은 약 8천억 달러에 달하며, 수백만 명의 난민이 발생했습니다[[ref4]].\n",
    "3. **인도적 위기:** 시리아, 이집트, 이라크 등 여러 지역에서 인도적 지원이 필요한 사람들의 수가 급증했습니다[[ref4]].\n",
    "\n",
    "이와 같이 제3차 중동 전쟁과 아랍의 겨울은 각각 중동 지역의 군사적, 정치적, 사회적 변화를 초래한 중요한 사건들입니다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19f36d40-167f-4607-a26f-1f2a7cd71f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "cited_docs = extract_ref_numbers(label)\n",
    "print(cited_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0072e11f-41f6-4165-a63f-fadd66b0818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = '''제3차 중동 전쟁과 아랍의 겨울은 각각 다른 시기에 발생한 사건이며, 주요 원인과 결과는 다음과 같습니다.\n",
    "\n",
    "제3차 중동 전쟁은 1967년 6월에 시작된 이스라엘과 주변 국가인 이집트, 요르단, 시리아, 레바논 사이의 전쟁입니다. 이 전쟁의 주요 원인은 이스라엘과 주변 국가 사이의 긴장과 불안정한 관계였습니다. 이스라엘은 1956년 수에즈 위기 때 주요 목표 중 하나였던 티란 해협을 확보하려고 했고, 이 지역은 1950년 이래 이집트가 이스라엘의 항해를 봉쇄하고 있었습니다. 이스라엘은 이 지역의 봉쇄를 계속해서 유지하려고 했지만, 이는 결국 1967년 6월에 이스라엘과 주변 국가 사이의 전쟁으로 이어졌습니다.\n",
    "\n",
    "전쟁의 결과로, 이스라엘은 이집트로부터 가자 지구와 시나이 반도를, 요르단으로부터 동예루살렘과 요르단 강 서안 지구를, 시리아로부터 골란 고원을 획득했습니다. 이스라엘의 영토는 3배나 커졌고, 이스라엘군은 군사적 승리를 거두었습니다. 그러나 이 전쟁으로 인해 군사들은 자만감에 빠졌고, 이는 1973년 욤키푸르 전쟁 당시 아랍 연합군이 초기에 승리를 거두는 계기가 되었습니다. 또한, 이 전쟁으로 인해 100,000명의 시리아인이 골란 고원을 떠났고, 300,000명의 팔레스타인인이 서안 지구를 떠났는데 이들은 모두 난민이 되었습니다.\n",
    "\n",
    "아랍의 겨울은 2011년 아랍의 봄 이후 아랍 일대에서 벌어지고 있는 광범위한 혼란과 폭력과 사회불안정 사태를 가리키는 말입니다. 이 겨울에는 시리아 내전, 이라크 내전, 이집트 위기, 예멘 내전이 모두 포함됩니다. 권위주의가 복고되고 자유 민권이 억압되고 있으며, 이슬람 극단주의와 테러 단체가 곳곳에서 준동하고 있습니다. 아랍의 겨울은 복수의 지역에서 내란과 내전이 발생하고, 사회가 불안정해지며, 아랍 지역의 경제와 인구가 쇠퇴하고 인종주의와 종교적 종파주의가 판을 치는 것으로 특징지어집니다. 아랍의 겨울의 가장 극단적 사례는 시리아입니다. 바샤르 알 아사드 대통령의 독재에 반대하여 일어난 시위에 알누스라 전선 등의 이슬람주의 집단이 개입하고, 자유 시리아군의 부패와 범죄가 밝혀지면서 이슬람주의자들의 입지가 강화되었습니다. 그리하여 시리아는 독재정부와 반정부 반군과 이슬람주의 테러리스트들의 삼파내전에 휩싸이게 되었으며, 그 여파는 이웃나라인 레바논과 이라크까지 번지고 있습니다. [[ref1]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8a21f3e-600d-4cc6-9dec-14f18561aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "model_pred = extract_ref_numbers(model_prediction)\n",
    "print(model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0468b30-636e-4efd-a025-90c09142a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(labels, predictions):\n",
    "    \"\"\"\n",
    "    F1 score 계산 (multilabel 케이스) - 이중 리스트 입력 지원\n",
    "    \n",
    "    Args:\n",
    "        labels: 샘플별 실제 레이블이 담긴 리스트 또는 이중 리스트\n",
    "        predictions: 샘플별 예측 레이블이 담긴 리스트 또는 이중 리스트\n",
    "    \n",
    "    Returns:\n",
    "        F1 점수\n",
    "    \"\"\"\n",
    "    # 입력 형식 확인\n",
    "    is_nested_list = isinstance(labels, list) and len(labels) > 0 and isinstance(labels[0], list)\n",
    "    \n",
    "    # 이중 리스트가 아니면 이중 리스트로 변환\n",
    "    if not is_nested_list:\n",
    "        labels = [labels]\n",
    "        predictions = [predictions]\n",
    "    \n",
    "    # 길이 확인\n",
    "    if len(labels) != len(predictions):\n",
    "        raise ValueError(\"라벨과 예측의 길이가 일치하지 않습니다.\")\n",
    "    \n",
    "    # 전체 통계 초기화\n",
    "    total_true_positives = 0\n",
    "    total_false_positives = 0\n",
    "    total_false_negatives = 0\n",
    "    \n",
    "    # 각 샘플에 대해 계산\n",
    "    for sample_labels, sample_preds in zip(labels, predictions):\n",
    "        # 빈 리스트는 빈 집합으로 변환\n",
    "        if not sample_labels:\n",
    "            sample_labels = set()\n",
    "        else:\n",
    "            sample_labels = set(sample_labels)\n",
    "            \n",
    "        if not sample_preds:\n",
    "            sample_preds = set()\n",
    "        else:\n",
    "            sample_preds = set(sample_preds)\n",
    "        \n",
    "        # 통계 계산\n",
    "        true_positives = len(sample_labels.intersection(sample_preds))\n",
    "        false_positives = len(sample_preds - sample_labels)\n",
    "        false_negatives = len(sample_labels - sample_preds)\n",
    "        \n",
    "        # 전체 통계에 추가\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "    \n",
    "    # 특수 케이스: 둘 다 빈 리스트인 경우는 F1 = 1.0으로 처리\n",
    "    if sum(bool(l) for l in labels) == 0 and sum(bool(p) for p in predictions) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # 정밀도와 재현율 계산\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    \n",
    "    # F1 점수 계산\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ac7d495-5286-4d1e-bf87-8d138c9c317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1, 4]\n",
      "Predictions: [1]\n",
      "F1 Score: 0.6667\n",
      "------------------------------\n",
      "Labels: [1, 2, 3]\n",
      "Predictions: [1, 2, 3]\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n",
      "Labels: [1, 2, 3]\n",
      "Predictions: [4, 5, 6]\n",
      "F1 Score: 0.0000\n",
      "------------------------------\n",
      "Labels: [1]\n",
      "Predictions: [1, 2, 3]\n",
      "F1 Score: 0.5000\n",
      "------------------------------\n",
      "Labels: []\n",
      "Predictions: []\n",
      "F1 Score: 1.0000\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    ([1, 4], [1]),  # 기본 케이스\n",
    "    ([1, 2, 3], [1, 2, 3]),  # 완벽한 예측\n",
    "    ([1, 2, 3], [4, 5, 6]),  # 완전히 틀린 예측\n",
    "    ([1], [1, 2, 3]),  # 과대 예측\n",
    "    ([], []),  # 빈 리스트\n",
    "]\n",
    "\n",
    "for labels, predictions in test_cases:\n",
    "    f1 = calculate_f1_score([labels], [predictions])\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f879349c-2acc-48d8-ba94-195420293ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ref_numbers = []\n",
    "pred_ref_numbers = []\n",
    "\n",
    "for label, pred in zip(label_lst[:50], preds[:50]):\n",
    "    label_ref_numbers.append(extract_ref_numbers(label))\n",
    "    pred_ref_numbers.append(extract_ref_numbers(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8db2e1ac-d57d-4577-84ec-a9f6977a202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1], [2], [1], [3], [1, 3], [1], [2], [1], [1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ref_numbers[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6040a7f6-9919-4bc7-b8ec-a33fffdbbb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [1], [2], [1], [3], [1, 2, 3], [1], [2], [1], [1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ref_numbers[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d81c0dad-be59-4910-a886-82b6eb680916",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = calculate_f1_score(label_ref_numbers, pred_ref_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7662a306-a167-448e-bf31-6cb576fc6540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
