{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faad77c2-2f5b-4990-b13f-44d3b6199dfc",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf692774-7219-4da9-ac0f-cb6ce9d04db9",
   "metadata": {},
   "source": [
    "해당 데이터를 전처리해서 허깅페이스에 데이터셋을 업로드하기까지의 과정은 아래의 Colab 주소에서 확인 가능합니다.  \n",
    "https://colab.research.google.com/drive/1ZVwJ24AX92XnosIpS5-sbDqcKz_SloE9?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f397d83-3851-4f47-9f93-f232cd73891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.8.0\n",
      "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch==2.8.0)\n",
      "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch==2.8.0) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.8.0) (2.1.5)\n",
      "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m240.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m321.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m287.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m353.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m280.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m194.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m295.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m310.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m347.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m268.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m297.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m281.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m226.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m328.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m227.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.55\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.55:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.55\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.61\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.25.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.25.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.25.1\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.55\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.55:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.55\n",
      "  Attempting uninstall: nvidia-cufile-cu12\n",
      "    Found existing installation: nvidia-cufile-cu12 1.13.0.11\n",
      "    Uninstalling nvidia-cufile-cu12-1.13.0.11:\n",
      "      Successfully uninstalled nvidia-cufile-cu12-1.13.0.11\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.57\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.57:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.3.14\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.3.14:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.3.14\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.7.53\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.7.53:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.41\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.41:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.41\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.8.0.87\n",
      "    Uninstalling nvidia-cudnn-cu12-9.8.0.87:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.8.0.87\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.2.55\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.2.55:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0.dev20250319+cu128\n",
      "    Uninstalling torch-2.8.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torch-2.8.0.dev20250319+cu128\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0.dev20250319+cu128 requires torch==2.8.0.dev20250319, but you have torch 2.8.0 which is incompatible.\n",
      "torchvision 0.22.0.dev20250319+cu128 requires torch==2.8.0.dev20250319, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 triton-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchvision==0.23.0\n",
      "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.23.0) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.23.0) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.23.0) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch==2.8.0->torchvision==0.23.0) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision==0.23.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.8.0->torchvision==0.23.0) (2.1.5)\n",
      "Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m298.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.0.dev20250319+cu128\n",
      "    Uninstalling torchvision-0.22.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torchvision-0.22.0.dev20250319+cu128\n",
      "Successfully installed torchvision-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torchaudio==2.8.0\n",
      "  Downloading torchaudio-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch==2.8.0->torchaudio==2.8.0) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.8.0->torchaudio==2.8.0) (2.1.5)\n",
      "Downloading torchaudio-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0.dev20250319+cu128\n",
      "    Uninstalling torchaudio-2.6.0.dev20250319+cu128:\n",
      "      Successfully uninstalled torchaudio-2.6.0.dev20250319+cu128\n",
      "Successfully installed torchaudio-2.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.55.2\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.2) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers==4.55.2)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.2) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.2) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.55.2)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.55.2) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.55.2)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.55.2)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.55.2)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.55.2) (2025.1.31)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m255.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m208.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m349.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m415.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 regex-2025.9.1 safetensors-0.6.2 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.55.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tokenizers==0.21.4 in /usr/local/lib/python3.11/dist-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.21.4) (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: safetensors==0.6.2 in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface-hub==0.34.4 in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.34.4) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.34.4) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.34.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.34.4) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.34.4) (2025.1.31)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets==4.0.0\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==4.0.0)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==4.0.0)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==4.0.0)\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (4.67.1)\n",
      "Collecting xxhash (from datasets==4.0.0)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets==4.0.0)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==4.0.0) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==4.0.0)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==4.0.0)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.16.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m332.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m321.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m277.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 multidict-6.6.4 multiprocess-0.70.16 pandas-2.3.2 propcache-0.3.2 pyarrow-21.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting accelerate==1.10.0\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.10.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (1.1.9)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate==1.10.0) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.10.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.10.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (2025.1.31)\n",
      "Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting trl==0.21.0\n",
      "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.21.0) (1.10.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.21.0) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.21.0) (4.55.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl==0.21.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (2024.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0->trl==0.21.0) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0->trl==0.21.0) (0.21.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.21.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.21.0) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (77.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.21.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.21.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl==0.21.0) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.21.0) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (2.1.5)\n",
      "Downloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting peft==0.17.0\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (2.8.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (4.55.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (1.10.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.17.0) (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (1.1.9)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=1.13.0->peft==0.17.0) (77.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.17.0) (2025.9.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.17.0) (0.21.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (2025.1.31)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.8.0\"\n",
    "%pip install \"torchvision==0.23.0\" \n",
    "%pip install \"torchaudio==2.8.0\"\n",
    "%pip install \"transformers==4.55.2\"\n",
    "%pip install \"tokenizers==0.21.4\"\n",
    "%pip install \"safetensors==0.6.2\"\n",
    "%pip install \"huggingface-hub==0.34.4\"\n",
    "%pip install \"datasets==4.0.0\"\n",
    "%pip install \"accelerate==1.10.0\"\n",
    "%pip install \"trl==0.21.0\"\n",
    "%pip install \"peft==0.17.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4376af-2280-4600-bf67-fc7f0ee60cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0388c1bb-0d1c-441c-9239-51addbe0f1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd5deb5ec2f44c9bd3a93f7b9fb76e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26790936e01a49c2b66427ddf776623f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b669d6e18b42f3a580c82220ace18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 991\n",
      "\n",
      "전체 데이터 분할 결과: Train 496개, Test 495개\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/finance_news_summarizer\", split=\"train\")\n",
    "\n",
    "# 2. system_message 정의\n",
    "# 데이터셋에 이미 포함된 system_prompt 열을 사용할 것이므로 따로 정의하지 않음\n",
    "\n",
    "# 3. 원본 데이터의 type별 분포 출력\n",
    "# 데이터셋에 type 열이 없으므로 전체 데이터 크기만 출력\n",
    "print(\"전체 데이터 크기:\", len(dataset))\n",
    "\n",
    "# 4. train/test 분할 비율 설정 (0.5면 5:5로 분할)\n",
    "test_ratio = 0.5\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 5. 전체 데이터의 인덱스를 train/test로 분할\n",
    "data_indices = list(range(len(dataset)))\n",
    "test_size = int(len(data_indices) * test_ratio)\n",
    "\n",
    "test_data = data_indices[:test_size]\n",
    "train_data = data_indices[test_size:]\n",
    "\n",
    "# 6. OpenAI format으로 데이터 변환을 위한 함수\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sample[\"system_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"user_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(sample[\"assistant\"])\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 7. 분할된 데이터를 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "# 8. 최종 데이터셋 크기 출력\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9afcd35-8265-4853-ad48-cf5d85a653cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}'},\n",
       " {'role': 'user',\n",
       "  'content': '중소기업 하반기 경기전망 작년보다 악화…원자잿값 상승 우려\\n서울 연합뉴스 신선미 기자 중소기업의 올해 하반기 경기전망 지수가 지난해 같은 기간보다 하락한 것으로 나타났다. 5일 중소기업중앙회에 따르면 지난달 15∼24일 중소기업 500곳을 대상으로 실시한 중소기업 경영애로 및 2022년 하반기 경기전망조사 결과 하반기 경기전망지수 SBHI 는 87.6으로 지난해 하반기 91.6 보다 4.0포인트 p 하락했다. 이 지수가 100 이상이면 경기가 개선될 것으로 보는 응답자가 더 많고 100 미만이면 그 반대라는 의미다. 중기중앙회 중기중앙회 제공 하반기 SBHI를 업종별로 보면 제조업의 경우 펄프·종이 및 종이제품업 54.2 섬유제품업 54.2 전기장비업 68.2 은 경기가 악화될 것으로 내다봤고 기타 운송장비업 127.3 가죽·가방 및 신발업 104.6 은 경기가 호전될 것으로 전망했다. 서비스업에서는 부동산업 및 임대업 60.0 도매 및 소매업 84.0 은 경기가 악화될 것으로 봤지만 예술·스포츠 및 여가 관련 서비스업 112.0 은 업황 개선을 전망했다. 하반기 예상되는 애로 요인 복수응답 은 원자재 가격 상승 58.8% 내수 부진 31.2% 인력 수급난 29.8% 금리상승 28.4% 최저임금 상승 19.4% 등의 순이었다. 또 상반기 겪은 애로 요인으로는 원자재가격 상승 62.6% 내수부진 35.2% 인력 수급난 29.8% 금리상승 25.2% 최저임금 상승 22.8% 등의 순으로 응답률이 높았다. 소상공인·중소기업의 경기 개선을 위해 필요한 정부 정책 복수응답 으로는 세금 및 각종 부담금 인하 61.4% 금융지원 45.0% 인력난 해소 34.6% 원자재 수급 안정화 28.6% 근로시간 유연화 20.0% 순으로 꼽혔다. 코로나19 이전 수준의 경영실적 회복 예상 시기에 대해서는 응답자의 27.0%가 2024년 이후 라고 답했고 이어 2023년 상반기 와 2023년 하반기 각 23.0% 2022년 하반기 14.8% 등이었다.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'negative_keywords': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'positive_impact_stocks': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'positive_keywords': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'reason_for_negative_impact': '펄프·종이 및 종이제품업, 섬유제품업, 전기장비업, 부동산업 및 임대업, 도매 및 소매업은 하반기 경기 전망에서 경기가 악화될 것으로 예측되고 있다.', 'reason_for_positive_impact': '기타 운송장비업, 가죽·가방 및 신발업, 예술·스포츠 및 여가 관련 서비스업은 하반기 경기 전망에서 경기가 호전될 것으로 예측되고 있다.', 'summary': '중소기업중앙회 조사 결과, 올해 하반기 중소기업 경기 전망 지수가 작년보다 하락했다. 제조업에서는 펄프, 종이, 섬유, 전기장비업이 경기 악화를 예상했고, 운송장비와 가죽, 신발업이 호전을 전망했다. 원자재 가격 상승과 내수 부진 등이 주요 애로 요인으로, 경기 개선을 위해 세금 및 부담금 인하와 금융지원 등이 필요하다고 응답됐다.'}\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5013274-411b-4d1e-99f1-9bd565c115c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf1bcbf-5a29-44c0-8895-c2ab92a39579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}',\n",
       "   'role': 'system'},\n",
       "  {'content': '이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\\n금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952450cf-8f7c-4a9d-8410-d432f351fb80",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96932879-603c-4806-a65f-8b7d68f5bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ac7c7ae0dd4a80bc2a867f1e5cef4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc857c3e40c41ae834b694655e58808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a14ddc76e474e72a8212f119029376f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c427db6172142788c6b67a70ef0292e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228bdfda59304c74a8eb889b783cfd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f77d79d17fe4e55810d4ede2179e9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c50cdd763cf41a0ba07e6eb26f1f265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5c3949a87941fcaaeb0a40a3ab5a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807b08560ddc4d2f969ffb98ab6fd2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d06e40c48d4f2c9b7d2438ec736608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c05754afae442579b84d7bf63bd3f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0a197d0adc48c4812e593252505578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen3-4B\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7599ae5e-59dd-4e7f-bd43-84c698d48c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|im_end|>\n",
      "<|im_start|>user\n",
      "이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n",
      "금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43004a-c17e-43ea-b683-5a50cf3d67cb",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d42f7d6-6633-4733-9256-328c2a812197",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173fb29-34c8-4dc0-86d3-32c9e24b958b",
   "metadata": {},
   "source": [
    "`lora_alpha`: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "\n",
    "`lora_dropout`: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. `0.1`로 설정하면 학습 중 10%의 뉴런이 비활성화됩니다.\n",
    "\n",
    "`r`: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "\n",
    "`bias`: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. `\"none\"`으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. `\"all\"` 또는 `\"lora_only\"`와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "\n",
    "`target_modules`: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 `\"q_proj\"`와 `\"v_proj\"`를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "\n",
    "`task_type`: LoRA가 적용되는 작업 유형을 지정합니다. `\"CAUSAL_LM\"`은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 `\"SEQ2SEQ_LM\"`(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e120e49f-ee9c-49d8-862e-beeedf9a839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "max_seq_length=16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afff8be8-7da4-470c-a3eb-867ae534f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen3-4b-finance-new-summarizer\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=4,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None,\n",
    "    max_length=max_seq_length,              # 최대 시퀀스 길이 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af360762-fe93-4a58-b10c-4646927198fa",
   "metadata": {},
   "source": [
    "`output_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\n",
    "`num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니다.\n",
    "\n",
    "`per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있습니다.\n",
    "\n",
    "`gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 2로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 유용합니다.\n",
    "\n",
    "`gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질 수 있습니다.\n",
    "\n",
    "`optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적화기를 사용합니다.\n",
    "\n",
    "`logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마다 로그를 기록합니다.\n",
    "\n",
    "`save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스텝마다 모델이 저장됩니다.\n",
    "\n",
    "`save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50 스텝마다 모델을 저장합니다.\n",
    "\n",
    "`bf16`: bfloat16 정밀도를 사용하도록 설정합니다. bfloat16은 FP32와 유사한 범위를 제공하면서 메모리와 계산 효율성을 높입니다.\n",
    "\n",
    "`learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 사용하여 안정적인 학습을 유도합니다.\n",
    "\n",
    "`max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 조정하여 폭발적 그래디언트를 방지합니다.\n",
    "\n",
    "`warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다. 학습의 안정성을 높이기 위해 사용됩니다.\n",
    "\n",
    "`lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"constant\"`는 학습률을 일정하게 유지합니다.\n",
    "\n",
    "`push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "\n",
    "`remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다. True로 설정하면 메모리를 절약할 수 있습니다.\n",
    "\n",
    "`dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset: True`로 설정하여 데이터셋 준비 단계를 건너뜹니다.\n",
    "\n",
    "`report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db6502-e721-4448-a852-033948aa0a5b",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de749b58-17a6-4541-8fdb-56450210806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa633337-1635-4636-83c5-54f9eae05c73",
   "metadata": {},
   "source": [
    "collate_fn(batch) 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 attention_mask를 생성합니다. 이후 assistant 답변 부분을 찾아 해당 범위에 레이블을 설정합니다. 이 범위를 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다.\n",
    "\n",
    "최종적으로, 배치 내 모든 샘플의 길이를 동일하게 맞추기 위해 패딩 작업을 수행합니다. 이 과정에서 입력 데이터에는 패딩 토큰 ID를 추가하고, 어텐션 마스크에는 0을 추가하며, 레이블에는 -100을 추가합니다. 모든 데이터는 PyTorch 텐서로 변환되어 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9936423f-be71-4141-b26d-5e7b69484200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 3200])\n",
      "어텐션 마스크 형태: torch.Size([1, 3200])\n",
      "레이블 형태: torch.Size([1, 3200])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ff007e2-fccd-4e88-91aa-d2d6cf8c2154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 55673, 31079, 85251, 5140, 231, 112, 24897, 138020, 98358, 87608, 19391, 126440, 129321, 17877, 55673, 16560, 5140, 231, 112, 24897, 134039, 140568, 126591, 42905, 40771, 230, 128024, 5140, 231, 112, 24897, 140568, 126591, 20487, 78952, 624, 126923, 127154, 143604, 3315, 120, 222, 12802, 24897, 19969, 134015, 130705, 125149, 136585, 54969, 12802, 144773, 20401, 10997, 141965, 76337, 42039, 140174, 16186, 139713, 624, 144544, 125686, 144707, 126414, 136541, 19391, 128772, 125686, 144707, 126414, 129125, 135968, 33509, 125476, 34395, 44518, 47985, 87425, 95577, 139713, 13, 23084, 16560, 10997, 54969, 144108, 17877, 60985, 98642, 128555, 128956, 129093, 135227, 72344, 48458, 13, 134084, 55673, 20401, 16186, 139713, 624, 52959, 53442, 10997, 56475, 126804, 897, 16560, 66790, 29326, 131193, 19391, 94613, 60838, 13, 66790, 55054, 131193, 17877, 126629, 135968, 21329, 125544, 139713, 13, 94613, 66790, 29326, 131193, 19391, 126629, 135968, 126550, 23573, 897, 18411, 3315, 109, 226, 130109, 143570, 33509, 139713, 624, 33883, 64795, 131193, 12802, 130671, 32290, 5140, 117, 230, 79921, 53955, 129549, 5140, 117, 230, 83634, 17380, 135968, 137571, 130650, 13, 16235, 226, 20401, 17380, 364, 127173, 48431, 6, 134454, 135968, 136108, 16560, 95170, 72344, 48458, 382, 72553, 125535, 94613, 5140, 231, 112, 24897, 19969, 142976, 98358, 87608, 7, 131110, 8, 12802, 139957, 128911, 132553, 50696, 127451, 11, 142976, 98358, 87608, 7, 131110, 8, 80573, 134006, 125120, 77353, 124780, 12802, 130768, 5140, 231, 112, 24897, 32077, 134832, 136646, 80573, 131050, 140174, 60838, 382, 132760, 126667, 510, 4913, 285, 30541, 53256, 788, 3557, 345, 1, 1708, 788, 330, 57026, 20487, 126377, 94613, 5140, 231, 112, 24897, 18411, 85997, 125535, 96137, 85997, 125535, 51588, 17877, 140174, 16186, 139713, 63159, 72553, 125535, 94613, 5140, 231, 112, 24897, 19969, 142976, 98358, 87608, 7, 131110, 8, 134771, 77353, 124780, 133245, 127451, 11, 142976, 98358, 87608, 7, 131110, 8, 53680, 134006, 125120, 77353, 124780, 12802, 130768, 5140, 231, 112, 24897, 32077, 134832, 136646, 80573, 131050, 140174, 60838, 382, 132760, 126667, 510, 4913, 285, 30541, 53256, 788, 3007, 345, 1, 30487, 36788, 531, 1261, 25183, 788, 4383, 126793, 12802, 144773, 79921, 53955, 83634, 20401, 141966, 17380, 23084, 5140, 231, 112, 24897, 19969, 40771, 235, 29281, 128533, 126440, 129321, 17877, 53989, 226, 124708, 42039, 57835, 29281, 128841, 98358, 87608, 129360, 86034, 17877, 140174, 16186, 139713, 13, 23872, 121, 25715, 17380, 135968, 127451, 30520, 226, 79632, 42039, 135968, 21329, 125544, 139713, 13, 98358, 87608, 79632, 42039, 57835, 29281, 128841, 61298, 83291, 79632, 17877, 135968, 33509, 139713, 13, 5140, 231, 112, 24897, 138020, 57835, 29281, 47836, 28733, 64521, 36055, 133085, 23573, 10764, 240, 222, 126730, 93701, 42039, 135968, 33509, 139713, 13, 139327, 11, 134015, 87425, 127728, 133099, 5140, 117, 230, 83634, 17380, 140174, 16186, 139713, 1189, 1259, 1, 19895, 5478, 54160, 36788, 531, 788, 330, 80901, 20401, 98358, 87608, 126253, 94613, 5140, 231, 112, 24897, 138020, 40771, 235, 29281, 128533, 126440, 129321, 17877, 83596, 17877, 132091, 57835, 29281, 23573, 132819, 18411, 131180, 19391, 135448, 140174, 16186, 139713, 756, 1, 30487, 51354, 788, 4383, 144203, 29281, 128533, 126440, 129321, 17877, 53989, 226, 132091, 57835, 29281, 128841, 98358, 87608, 126253, 134015, 128836, 32290, 131180, 19391, 40771, 235, 29281, 128533, 126440, 129321, 17877, 55673, 126551, 134312, 120, 92192, 19969, 97143, 139325, 55673, 35711, 23573, 130345, 55054, 10764, 92120, 130109, 29346, 129125, 54969, 12802, 144773, 79921, 53955, 83634, 141966, 17380, 140174, 16186, 139713, 13, 54116, 125880, 79632, 11, 98005, 55054, 79632, 134454, 130593, 133970, 60838, 13, 30520, 113, 128747, 130345, 55054, 130005, 10764, 245, 230, 26699, 60838, 13, 130671, 32290, 5140, 117, 230, 83634, 17380, 140174, 128747, 29326, 57268, 1189, 1259, 1, 42224, 36788, 531, 1261, 25183, 788, 4383, 126793, 12802, 144773, 79921, 53955, 83634, 20401, 141966, 17380, 23084, 5140, 231, 112, 24897, 19969, 40771, 235, 29281, 128533, 126440, 129321, 17877, 53989, 226, 124708, 42039, 57835, 29281, 128841, 98358, 87608, 129125, 140174, 16186, 139713, 13, 23872, 121, 25715, 17380, 135968, 127451, 30520, 226, 79632, 42039, 135968, 21329, 125544, 139713, 13, 98358, 87608, 79632, 42039, 57835, 29281, 128841, 61298, 83291, 79632, 17877, 135968, 33509, 139713, 13, 5140, 231, 112, 24897, 138020, 57835, 29281, 47836, 28733, 64521, 36055, 133085, 23573, 10764, 240, 222, 126730, 93701, 42039, 135968, 33509, 139713, 13, 139327, 11, 134015, 87425, 127728, 133099, 5140, 117, 230, 83634, 17380, 140174, 16186, 139713, 1189, 1259, 1, 19895, 5478, 53865, 36788, 531, 788, 330, 80901, 20401, 98358, 87608, 126253, 94613, 5140, 231, 112, 24897, 138020, 40771, 235, 29281, 128533, 126440, 129321, 17877, 83596, 17877, 132091, 57835, 29281, 23573, 132819, 18411, 131180, 19391, 135448, 140174, 16186, 139713, 756, 1, 42224, 51354, 788, 4383, 63089, 29281, 128533, 126440, 129321, 17877, 53989, 226, 132091, 57835, 29281, 128841, 98358, 87608, 126253, 134015, 128836, 32290, 131180, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 55673, 126551, 134312, 120, 92192, 19969, 97143, 139325, 55673, 35711, 23573, 130345, 55054, 10764, 92120, 130109, 29346, 129125, 54969, 12802, 144773, 79921, 53955, 83634, 141966, 17380, 140174, 16186, 139713, 13, 54116, 125880, 79632, 11, 98005, 55054, 79632, 134454, 130593, 133970, 60838, 13, 30520, 113, 128747, 130345, 55054, 130005, 10764, 245, 230, 26699, 60838, 13, 130671, 32290, 5140, 117, 230, 83634, 17380, 140174, 128747, 29326, 57268, 1189, 1259, 1, 1708, 788, 330, 57026, 20487, 126377, 94613, 5140, 231, 112, 24897, 18411, 85997, 125535, 96137, 85997, 125535, 51588, 17877, 140174, 16186, 139713, 9207, 151645, 198, 151644, 872, 198, 12802, 97834, 126407, 90711, 112, 29346, 55054, 19391, 43115, 34395, 40853, 1940, 125054, 28002, 23573, 126440, 124517, 64577, 37087, 56983, 130454, 144269, 92751, 28002, 129264, 198, 125052, 129567, 54321, 40853, 83518, 65865, 55054, 12156, 16778, 226, 125786, 61741, 126310, 57089, 32831, 56983, 140532, 92751, 28002, 1940, 137237, 125535, 85997, 31328, 126591, 60960, 70582, 129264, 131565, 125535, 125625, 54330, 126563, 126429, 125052, 28002, 40666, 248, 1940, 28002, 140532, 92751, 28002, 126871, 56983, 130454, 144269, 126488, 130973, 65865, 140568, 129865, 124657, 125476, 1940, 59761, 125519, 126321, 126246, 140175, 23084, 97834, 126407, 40771, 230, 128024, 129567, 129502, 54321, 40853, 13, 139764, 10764, 245, 230, 51588, 138143, 20487, 25715, 23084, 97834, 126407, 40771, 230, 128024, 129567, 129502, 54321, 40853, 33704, 126310, 57089, 32831, 92751, 28002, 131565, 21329, 56475, 129400, 20487, 28733, 131870, 32831, 130729, 41671, 18411, 130679, 125149, 28002, 23573, 126440, 124517, 130729, 137471, 64577, 37087, 33883, 131303, 129337, 220, 20, 32077, 125834, 63089, 128836, 13, 23084, 129062, 126558, 126799, 126591, 3315, 112, 251, 63089, 130752, 54321, 28002, 125052, 55902, 65238, 70582, 132841, 422, 14557, 220, 18, 125068, 124781, 65510, 59698, 19969, 138767, 128841, 5140, 41902, 126629, 82619, 37087, 32831, 56983, 130454, 144269, 77002, 422, 14557, 135969, 62071, 128792, 58034, 125678, 19391, 128605, 28733, 35711, 19969, 132376, 251, 19969, 47836, 28733, 64521, 62107, 127908, 56983, 140532, 92751, 28002, 19391, 126804, 126591, 125511, 128753, 65306, 3315, 235, 101, 129062, 129254, 47985, 55673, 51588, 128836, 13, 23084, 129093, 40853, 33704, 23084, 129378, 136905, 70943, 88259, 49367, 57089, 83518, 82528, 138870, 129640, 61741, 56475, 130847, 129807, 83518, 82528, 133081, 138870, 131110, 81173, 34395, 142408, 25715, 12156, 74884, 222, 20401, 16778, 226, 125786, 61741, 56475, 126310, 57089, 32831, 56983, 140532, 19391, 126804, 126591, 23573, 138449, 17877, 88846, 55673, 20487, 81718, 129804, 13146, 13, 83518, 65865, 136294, 28733, 82528, 54116, 66019, 12802, 46682, 20487, 129147, 126310, 57089, 32831, 56983, 140532, 19969, 130887, 136089, 80968, 133215, 20136, 113, 125512, 128533, 56983, 140532, 136262, 24485, 227, 124781, 141769, 92751, 28002, 47836, 126871, 19969, 90686, 48108, 108, 36330, 102, 79716, 23573, 134313, 129439, 20401, 126310, 57089, 132818, 130729, 41671, 42905, 61298, 129027, 129400, 20487, 28733, 131870, 32831, 130729, 41671, 18411, 130679, 125149, 28002, 23573, 126440, 124517, 130729, 40853, 129835, 126429, 80901, 125341, 64577, 85057, 130729, 66845, 16560, 64577, 37087, 82190, 55673, 20487, 81718, 129804, 13146, 126429, 126254, 128836, 13, 23084, 31079, 23084, 129093, 40853, 33704, 83518, 65865, 136294, 83518, 65865, 130752, 95996, 124528, 77002, 44518, 40853, 32831, 129882, 43866, 17877, 131582, 140094, 20401, 64577, 125052, 17877, 65510, 129062, 126204, 127353, 44518, 126402, 125052, 28002, 68408, 58034, 130766, 44518, 65510, 129062, 19391, 124685, 125476, 133914, 12802, 133396, 47836, 28733, 90686, 13, 125640, 64577, 125052, 132028, 26699, 3315, 116, 94, 32290, 56475, 35509, 124781, 134239, 33704, 58034, 66845, 128552, 131565, 125535, 23573, 94203, 137812, 12802, 126563, 126204, 54116, 124517, 134239, 33704, 84255, 17380, 144644, 28626, 126793, 12802, 142907, 144108, 28415, 60960, 69923, 77002, 85403, 57089, 85057, 24485, 227, 126337, 19391, 130263, 126402, 136398, 43115, 37087, 58034, 130803, 19391, 125713, 120, 129567, 128555, 126440, 129321, 17877, 83596, 16560, 13146, 48108, 108, 83518, 65865, 55054, 20401, 64577, 125052, 92817, 129062, 13935, 93672, 26699, 55902, 127820, 23259, 32831, 42039, 131565, 125535, 85997, 31328, 126591, 17380, 48364, 254, 126781, 23573, 60960, 70582, 19969, 126871, 129330, 126429, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 220, 17, 15, 17, 15, 126216, 128753, 126337, 3315, 65291, 17380, 60315, 130271, 12802, 60294, 24897, 129423, 137284, 128844, 3315, 65291, 17380, 60315, 16, 24, 133396, 140730, 83518, 65865, 130752, 79207, 44680, 62544, 126673, 29346, 19969, 130729, 66845, 64119, 131611, 83518, 65865, 130752, 128753, 131005, 95996, 124528, 12802, 130037, 55902, 70943, 125068, 128993, 136605, 70943, 43590, 128909, 83518, 65865, 136294, 28733, 73523, 128514, 62275, 126310, 57089, 32831, 23872, 254, 17380, 19391, 137351, 32290, 23573, 81718, 90686, 48108, 108, 133146, 220, 21, 128514, 136331, 83518, 65865, 130752, 79207, 44680, 62544, 126673, 29346, 19969, 220, 17, 15, 17, 15, 126216, 126310, 57089, 32831, 45710, 20487, 140730, 81173, 34395, 126333, 220, 24, 17, 17808, 16751, 226, 58034, 61741, 132537, 64577, 125052, 92817, 129062, 83518, 124873, 12802, 140084, 23872, 227, 56290, 130357, 90686, 126429, 10764, 244, 19946, 13, 125261, 131611, 23084, 129093, 40853, 33704, 136751, 128552, 63332, 23259, 128533, 58034, 130803, 17877, 35509, 29281, 33883, 126310, 57089, 32831, 141767, 28626, 126673, 24897, 10764, 72509, 53189, 18411, 126423, 29326, 126204, 73986, 55902, 64577, 125052, 65510, 129062, 94203, 127324, 47985, 131170, 138903, 18585, 238, 129845, 33883, 55673, 20487, 81718, 129804, 13146, 48108, 108, 68408, 128533, 60960, 69923, 125746, 130729, 131347, 129835, 60960, 54330, 54330, 132185, 126321, 126246, 126310, 55902, 128844, 25715, 64577, 125052, 137709, 77002, 130729, 41671, 134454, 131582, 62107, 130898, 53442, 85403, 130752, 18411, 136751, 128552, 58034, 65238, 47836, 28733, 136303, 36330, 102, 79716, 23573, 134313, 129439, 20401, 126310, 57089, 32831, 130729, 41671, 19969, 126871, 129330, 126429, 129413, 92817, 128836, 13, 48408, 126893, 60294, 23084, 129093, 40853, 33704, 35509, 124781, 134239, 17877, 95170, 29281, 128552, 92751, 28002, 126204, 77596, 238, 125086, 10764, 251, 94, 23259, 143861, 98, 135818, 130729, 131347, 42905, 5140, 41902, 47985, 130263, 126402, 33883, 34143, 105, 129254, 125834, 63089, 128836, 13, 54825, 16560, 83518, 65865, 55054, 20401, 35509, 124781, 134239, 33704, 131565, 125535, 125625, 54330, 19969, 126563, 42905, 126429, 125052, 28002, 58034, 125678, 12802, 140094, 17877, 129882, 21329, 126204, 127353, 40771, 230, 28002, 58034, 130766, 44518, 130270, 65865, 137032, 125569, 16186, 131396, 124657, 125476, 19969, 90686, 48108, 108, 131565, 125535, 125625, 54330, 19391, 128605, 126429, 125052, 28002, 60960, 69923, 131565, 128911, 44518, 129882, 130788, 58034, 65238, 143861, 98, 28754, 19391, 131417, 16560, 60960, 69923, 131565, 128911, 92751, 124528, 12802, 36055, 135375, 131396, 28733, 136303, 138449, 17877, 88846, 55673, 29326, 20487, 81718, 129804, 13146, 126429, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 23084, 129062, 126558, 138767, 52300, 422, 14557, 220, 18, 125068, 124781, 65510, 59698, 136331, 141526, 125052, 141556, 82619, 37087, 32831, 56983, 130454, 144269, 77002, 422, 14557, 135969, 60960, 55902, 56475, 62071, 128792, 128841, 58034, 125678, 19391, 128605, 28733, 35711, 19969, 132376, 251, 19969, 47836, 28733, 135354, 137621, 56983, 140532, 92751, 28002, 19391, 63332, 13146, 128753, 65306, 3315, 235, 101, 54330, 130105, 81718, 129804, 13146, 48108, 108, 136115, 77596, 238, 125086, 10764, 251, 94, 23259, 143861, 98, 28754, 130729, 131347, 17877, 130039, 143005, 56419, 130472, 17877, 63332, 23259, 128552, 65880, 33883, 60960, 130640, 131347, 64795, 125052, 17877, 36330, 102, 79716, 125511, 135968, 126702, 47836, 126871, 19969, 90686, 126429, 126365, 100, 141142, 139836, 13, 23084, 129093, 40853, 33704, 54116, 124517, 134239, 12802, 142976, 24485, 227, 126337, 19391, 10764, 236, 116, 126402, 132553, 50696, 126054, 83518, 82528, 125512, 55054, 128355, 32129, 127033, 129576, 18411, 129413, 56290, 33883, 53989, 226, 137638, 142510, 28754, 128836, 13, 54825, 16560, 83518, 65865, 136294, 45130, 120, 92192, 220, 16, 15, 126216, 62275, 125569, 125052, 28002, 54116, 92817, 128355, 43115, 131833, 28927, 105, 56290, 17380, 28415, 60960, 69923, 77002, 85403, 57089, 85057, 24485, 227, 126337, 17877, 141482, 42039, 54116, 124517, 134239, 17877, 130729, 66845, 33883, 139465, 126377, 126429, 125522, 124517, 125054, 64577, 85057, 17877, 83315, 53680, 128555, 72344, 238, 13146, 48108, 112, 26698, 130549, 85403, 57089, 85057, 35509, 126614, 16186, 132772, 19391, 128605, 124657, 125476, 19969, 5140, 228, 240, 33704, 18585, 238, 17877, 126429, 125476, 33883, 60960, 69923, 131565, 128911, 44518, 34143, 112, 41671, 126251, 12802, 134808, 3315, 109, 226, 125054, 58034, 65238, 143861, 98, 28754, 45710, 54330, 17380, 83518, 82528, 125512, 134445, 130127, 60960, 69923, 131565, 128911, 136331, 126377, 129882, 130788, 128753, 26699, 80901, 125341, 137525, 83518, 63089, 18411, 55673, 20487, 128552, 18585, 238, 129845, 47836, 126871, 19969, 90686, 126429, 126254, 128836, 13, 23084, 31079, 23084, 129093, 40853, 33704, 83518, 65865, 55054, 141769, 54116, 124517, 57026, 82528, 28927, 105, 55054, 128355, 32129, 127033, 129576, 18411, 129413, 56290, 126204, 44518, 40853, 58034, 130803, 23872, 227, 56290, 19391, 60960, 70582, 33883, 60960, 130640, 131347, 64795, 125052, 68408, 135968, 126702, 130612, 10764, 252, 246, 135660, 55673, 29326, 20487, 81718, 129804, 13146, 48108, 108, 40771, 230, 129567, 54321, 33704, 129304, 28415, 60960, 69923, 19391, 128605, 131608, 32831, 69441, 231, 19969, 18411, 126423, 29326, 42905, 77002, 54116, 124517, 134239, 60985, 225, 250, 18411, 18585, 238, 129845, 126204, 54825, 98801, 18411, 81718, 144059, 42039, 24485, 227, 124781, 80573, 54116, 124517, 57026, 82528, 28927, 105, 55054, 128355, 32129, 127033, 129576, 54070, 130765, 131005, 129044, 16751, 226, 140175, 47836, 94203, 127324, 23084, 129254, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 3315, 65291, 17380, 60315, 16, 24, 132185, 84255, 81650, 127483, 98358, 63256, 77002, 19391, 60960, 70582, 23573, 131565, 125535, 125625, 54330, 132185, 130612, 138449, 17877, 125834, 63089, 128836, 13, 54825, 16560, 83518, 65865, 55054, 19969, 136751, 132029, 70943, 31328, 84255, 28002, 130109, 81133, 52959, 138873, 77002, 3315, 109, 226, 125054, 92817, 29281, 132185, 84255, 81650, 127483, 17877, 140969, 33883, 83556, 29326, 128552, 129242, 125054, 80968, 45130, 97, 65306, 19391, 71647, 23573, 129882, 54330, 19969, 65510, 132264, 47818, 124517, 19391, 30520, 113, 133032, 47836, 28733, 136303, 135968, 131529, 128533, 132185, 17877, 85403, 133376, 29346, 129807, 13146, 48108, 108, 38523, 105, 33883, 220, 23, 128514, 126558, 98005, 55054, 126591, 40771, 230, 28002, 31328, 16186, 35711, 88259, 128739, 132029, 125086, 80968, 125466, 29326, 19969, 138767, 64119, 137621, 138501, 95170, 95218, 129413, 56290, 134454, 131582, 128753, 26699, 47985, 19969, 73523, 125519, 52300, 138501, 20401, 40771, 230, 28002, 63089, 125786, 12802, 43115, 129567, 131396, 28733, 136303, 129875, 138449, 17877, 88846, 55673, 29326, 20487, 81718, 129804, 13146, 126429, 129413, 92817, 128836, 13, 125261, 131611, 23084, 129093, 40853, 33704, 139465, 126563, 125052, 132652, 12802, 132376, 251, 19969, 42905, 82619, 37087, 32831, 56983, 130454, 144269, 33704, 131565, 125535, 125625, 130788, 58034, 65238, 85403, 125786, 17877, 83556, 29326, 128552, 53989, 226, 57026, 131303, 28733, 64521, 129359, 126333, 12802, 141258, 40771, 230, 43590, 126270, 55902, 40771, 230, 128024, 55902, 125678, 19391, 94613, 87425, 50696, 52959, 126488, 130973, 65865, 140568, 129865, 19391, 128605, 124657, 125476, 19969, 64521, 137638, 130037, 23084, 50340, 124905, 40771, 230, 129567, 54321, 33704, 40771, 230, 128024, 80901, 47455, 239, 61741, 80573, 129676, 40771, 230, 128024, 43590, 70582, 25715, 142452, 131870, 62071, 34395, 18411, 130039, 56983, 130454, 144269, 133828, 26698, 128753, 125624, 131565, 125535, 125625, 54330, 35509, 43866, 44518, 60716, 129262, 144063, 126423, 29326, 40771, 230, 28002, 127165, 108, 29281, 66136, 126346, 95170, 95218, 40771, 230, 28002, 125466, 29326, 55673, 20487, 129400, 126970, 136357, 73523, 125519, 126321, 126246, 17877, 140175, 70943, 19391, 90686, 13, 126804, 90711, 112, 29346, 55054, 12156, 131493, 47985, 73523, 125519, 126321, 126246, 140175, 56419, 128878, 138501, 19391, 128605, 133828, 125714, 144147, 77002, 42039, 58677, 33883, 126488, 130973, 65865, 140568, 129865, 19969, 133396, 87425, 50696, 126054, 136751, 128552, 92751, 133886, 129413, 56290, 33883, 55673, 29326, 131777, 125834, 63089, 29346, 129807, 13146, 126429, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 83518, 65865, 124517, 124781, 43115, 131833, 28754, 129413, 56290, 18411, 130679, 134313, 37087, 74884, 226, 56290, 77002, 36055, 126712, 80968, 132185, 17877, 48408, 142063, 21329, 50696, 127816, 16560, 5140, 250, 119, 47985, 22042, 251, 144472, 13146, 13, 54825, 16560, 76497, 242, 21329, 144046, 56419, 65238, 44518, 66845, 18411, 131417, 12802, 33883, 40771, 230, 128024, 124517, 53680, 73986, 138870, 124517, 20401, 43115, 124781, 19969, 10764, 245, 230, 126251, 31079, 134497, 128472, 13, 136115, 83518, 65865, 136294, 5140, 117, 227, 130229, 81133, 80573, 20401, 43115, 131833, 28927, 105, 56290, 17380, 83518, 125166, 24485, 227, 126337, 129885, 124685, 125476, 133914, 19391, 71647, 33883, 135354, 137621, 134585, 128677, 40853, 57089, 135818, 95996, 144071, 47836, 28733, 136303, 132185, 143353, 48108, 108, 76497, 242, 21329, 144046, 56419, 65238, 57835, 41429, 18411, 126429, 125476, 33883, 23894, 116, 125144, 128355, 85403, 23259, 124517, 125054, 20401, 47665, 242, 80901, 83518, 65865, 124517, 126591, 131565, 128911, 95351, 24485, 227, 125054, 20401, 49052, 40771, 230, 128024, 124517, 53680, 77353, 124780, 52300, 131608, 19391, 130869, 16560, 40771, 230, 128024, 80901, 19391, 130729, 66845, 18411, 130270, 20401, 143353, 13, 125640, 60716, 128792, 126616, 69923, 44518, 130612, 40771, 230, 129567, 54321, 20401, 60716, 128792, 65553, 97, 28626, 130109, 81133, 18411, 140969, 82190, 83518, 65865, 55054, 20401, 23872, 254, 17380, 131193, 17877, 60716, 43590, 47836, 28733, 136303, 10764, 252, 246, 131767, 127816, 126429, 126254, 128836, 13, 5140, 223, 251, 42039, 23084, 129093, 40853, 33704, 40771, 230, 128024, 133627, 58034, 130803, 12802, 129400, 133633, 19391, 73523, 125519, 132553, 136982, 132091, 95617, 55902, 64119, 137621, 40771, 112, 91043, 144147, 17877, 131670, 56983, 140532, 92751, 28002, 80573, 40771, 230, 128024, 43590, 70582, 25715, 63332, 47324, 19391, 130263, 126402, 33883, 55673, 29326, 131777, 125834, 63089, 29346, 129807, 13146, 48108, 108, 40771, 230, 129567, 54321, 47985, 83518, 65865, 124517, 124781, 80573, 40771, 112, 134372, 125511, 126291, 125160, 132537, 129238, 124517, 63089, 51588, 20401, 43115, 131833, 28754, 129413, 56290, 18411, 130039, 129985, 134313, 37087, 18411, 73523, 125519, 126204, 60985, 248, 101, 32831, 62071, 34395, 18411, 130679, 136111, 47985, 66790, 126299, 47836, 71108, 23084, 129254, 10764, 244, 19946, 13, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 13608, 285, 30541, 53256, 1210, 3007, 11, 364, 42224, 36788, 531, 1261, 25183, 1210, 2509, 57026, 82528, 133081, 138870, 131110, 516, 364, 129616, 29346, 55054, 4089, 364, 42224, 51354, 1210, 2509, 125522, 57089, 32831, 56983, 140532, 516, 364, 28002, 130454, 144269, 516, 364, 34395, 80901, 125341, 64577, 85057, 516, 364, 57026, 82528, 133081, 138870, 131110, 4089, 364, 30487, 36788, 531, 1261, 25183, 1210, 10071, 364, 30487, 51354, 1210, 10071, 364, 19895, 5478, 53865, 36788, 531, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 126310, 57089, 32831, 56983, 140532, 92751, 133886, 129413, 92817, 130705, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 28002, 129413, 56290, 18411, 66790, 29326, 23573, 129274, 83518, 82528, 133081, 138870, 131110, 80573, 90711, 112, 29346, 55054, 132812, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 136115, 11, 126429, 80901, 125341, 64577, 85057, 130729, 66845, 128355, 125149, 28002, 23573, 126440, 124517, 130729, 40853, 64577, 37087, 19969, 135797, 64119, 131611, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 53989, 226, 28733, 128472, 15670, 364, 19895, 5478, 54160, 36788, 531, 1210, 8981, 364, 1708, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 90711, 112, 29346, 55054, 80573, 83518, 82528, 133081, 138870, 131110, 18411, 60960, 55902, 42039, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 133886, 125834, 63089, 130705, 11, 126310, 57089, 32831, 56983, 140532, 80573, 131565, 125535, 125625, 54330, 60960, 69923, 19391, 128605, 55673, 20401, 18411, 129413, 92817, 128836, 13, 23084, 16560, 94613, 40771, 230, 128024, 55054, 129360, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 90686, 3159, 92, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3af73c91-d379-475c-92b9-4b0c5df08ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 151667, 271, 151668, 271, 13608, 285, 30541, 53256, 1210, 3007, 11, 364, 42224, 36788, 531, 1261, 25183, 1210, 2509, 57026, 82528, 133081, 138870, 131110, 516, 364, 129616, 29346, 55054, 4089, 364, 42224, 51354, 1210, 2509, 125522, 57089, 32831, 56983, 140532, 516, 364, 28002, 130454, 144269, 516, 364, 34395, 80901, 125341, 64577, 85057, 516, 364, 57026, 82528, 133081, 138870, 131110, 4089, 364, 30487, 36788, 531, 1261, 25183, 1210, 10071, 364, 30487, 51354, 1210, 10071, 364, 19895, 5478, 53865, 36788, 531, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 126310, 57089, 32831, 56983, 140532, 92751, 133886, 129413, 92817, 130705, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 28002, 129413, 56290, 18411, 66790, 29326, 23573, 129274, 83518, 82528, 133081, 138870, 131110, 80573, 90711, 112, 29346, 55054, 132812, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 136115, 11, 126429, 80901, 125341, 64577, 85057, 130729, 66845, 128355, 125149, 28002, 23573, 126440, 124517, 130729, 40853, 64577, 37087, 19969, 135797, 64119, 131611, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 53989, 226, 28733, 128472, 15670, 364, 19895, 5478, 54160, 36788, 531, 1210, 8981, 364, 1708, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 90711, 112, 29346, 55054, 80573, 83518, 82528, 133081, 138870, 131110, 18411, 60960, 55902, 42039, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 133886, 125834, 63089, 130705, 11, 126310, 57089, 32831, 56983, 140532, 80573, 131565, 125535, 125625, 54330, 60960, 69923, 19391, 128605, 55673, 20401, 18411, 129413, 92817, 128836, 13, 23084, 16560, 94613, 40771, 230, 128024, 55054, 129360, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 90686, 3159, 92, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b843f34a-42d4-4c7c-b815-e9f3ec3f5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labels 디코딩 결과 (-100 제외):\n",
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# -100이 아닌 부분만 골라 디코딩\n",
    "label_ids = [token_id for token_id in batch[\"labels\"][0].tolist() if token_id != -100]\n",
    "\n",
    "decoded_labels = tokenizer.decode(\n",
    "    label_ids,\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\nlabels 디코딩 결과 (-100 제외):\")\n",
    "print(decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75548c32-aced-461c-b82c-1c2348b4822b",
   "metadata": {},
   "source": [
    "## input_ids와 labels는 어떻게 생성되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179292b-2a1d-40ca-a8e6-a98ff7f5b4ea",
   "metadata": {},
   "source": [
    "Qwen3 모델의 챗 템플릿을 기준으로 input_ids와 labels 생성을 설명하겠습니다.\n",
    "\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다.\n",
    "사용자가 안녕하세요, 오늘 날씨는 어떤가요?라고 물었고,\n",
    "모델은 안녕하세요! 오늘 날씨는 맑고 화창합니다.라고 응답해야 합니다.\n",
    "\n",
    "Qwen3에서는 다음과 같은 템플릿 구조를 사용합니다:\n",
    "\n",
    "```python\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "안녕하세요, 오늘 날씨는 어떤가요?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "\n",
    "안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|>\n",
    "```\n",
    "\n",
    "이 전체 텍스트는 토크나이저에 의해 정수 시퀀스로 변환됩니다. (해당 정수 시퀀스는 임의로 지정한 것으로 실제 정수와 다를 수 있습니다.)\n",
    "```python\n",
    "input_ids = [\n",
    "    # <|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n\n",
    "    1001, 1002, 13, 1003, 1004, 1005, 1006, 1007, 1008, 13,\n",
    "    # <|im_start|>user\\\\n안녕하세요, 오늘 날씨는 어떤가요?<|im_end|>\\\\n\n",
    "    2001, 2002, 13, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 13,\n",
    "    # <|im_start|>assistant\\\\n\n",
    "    3001, 3002, 13,\n",
    "    # <think>\\\\n\\\\n</think>\\\\n\\\\n\n",
    "    4001, 13, 13, 4002, 13, 13,\n",
    "    # 안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|>\n",
    "    5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008\n",
    "]\n",
    "```\n",
    "모델이 예측해야 할 영역은 assistant의 응답 부분인 <think></think> 태그와 최종 응답입니다.\n",
    "\n",
    "따라서 labels는 다음과 같이 설정됩니다:\n",
    "```python\n",
    "labels = [\n",
    "    # system 부분 마스킹\n",
    "    -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "    # user 부분 마스킹\n",
    "    -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
    "    # <|im_start|>assistant\\\\n 마스킹\n",
    "    -100, -100, -100,\n",
    "    # <think>\\\\n\\\\n</think>\\\\n\\\\n (마스킹 없음)\n",
    "    4001, 13, 13, 4002, 13, 13,\n",
    "    # 안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|> (마스킹 없음)\n",
    "    5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008\n",
    "]\n",
    "```\n",
    "이처럼 labels는 모델이 학습해야 할 assistant 응답 부분만을 포함하고, 나머지 부분은 -100으로 마스킹하여 손실 계산에서 제외됩니다. 모델은 `4001, 13, 13, 4002, 13, 13, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008`에 해당하는 `<think></think>안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|>` 전체 응답 영역을 생성하도록 학습할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af72d2b-46fe-402a-b4d3-6ee4ba5c0ad0",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26206ff6-0749-4e52-b643-62c2f8dbbc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2136f0b5-ea53-4777-9a3f-be0eedd24863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 18:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.772100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.551300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.522400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.447300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82766c-106e-428b-8210-476540f79386",
   "metadata": {},
   "source": [
    "## 6. 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0a0e0-737e-4c92-b1bf-abd8fa688397",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 '<|im_start|>assistant'가 부착되어서 넣는 것이 좋습니다. 그래야만 모델이 바로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1547634a-4d70-46e9-8483-b5866ffb7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "   text = tokenizer.apply_chat_template(\n",
    "       prompt, tokenize=False, add_generation_prompt=False\n",
    "   )\n",
    "   # <|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n까지 포함\n",
    "   input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n'\n",
    "   # <think>\\n\\n</think>\\n\\n 이후 부분만 label로\n",
    "   label = text.split('<think>\\n\\n</think>\\n\\n')[1]\n",
    "   prompt_lst.append(input)\n",
    "   label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ecfb55f-46b6-4ce8-b93e-956a31e00dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|im_end|>\n",
      "<|im_start|>user\n",
      "글로벌 비즈 가트너 올해 전세계 스마트폰 판매량 7% 감소 전망\n",
      "경제와이드 모닝벨 글로벌 비즈 임선우 외신캐스터 글로벌 비즈입니다. ◇ 올해 스마트폰 판매 감소 올해 전세계 스마트폰 판매량이 크게 줄어들 것이란 전망이 나왔습니다. 시장조사업체 가트너는 글로벌 스마트폰 판매가 7% 하락할 것으로 내다봤는데요. 경제 전반에 걸친 침체 우려와 중국의 봉쇄조치 여파 그리고 인플레이션으로 소비자들이 지갑을 열기 주저하면서 수요가 줄어들 것 이라고 설명했습니다. 그러면서 올해 전체 출하량은 14억6천만대 수준에 그칠 것으로 예측했는데요. 종전 전망치인 16억대에서 대폭 낮춰 잡았습니다. 특히 세계 최대 스마트폰 시장인 중국에서 판매량은 18%가 감소할 것으로 전망했는데요. 가트너는 이같은 수요 부진으로 애플을 비롯한 스마트폰 제조사부터 엔비디아 TSMC 같은 반도체 업체까지 압력이 가해질 것이라고 진단했습니다. ◇ EU 가상자산 돈세탁 막는다 유럽연합이 가상자산을 이용한 돈세탁을 막기위해 관련 기업을 규제하는 방안에 잠정 합의했습니다. 잠정안에는 가상자산 업체가 당국에 모든 디지털자산 거래에 대한 신원 확인 정보를 제공하도록 하는 내용이 담겼는데요. 이에 따라 업체들은 관련 개인정보를 확보해야하고 당국이 이를 요구할 경우 제출해야 합니다. 또 거래액이 1천 유로 우리돈 130만 원을 넘길 경우 비인증 거래소가 관리하는 가상자산 지갑도 똑같은 규칙이 적용되는데요. 여기에 더해 송금 규제를 활용해 거래를 상시 추적하고 불법성이 의심되는 거래를 막을 수 있도록 할 방침입니다. 이와 관련해 미국 최대 가상자산 거래소 코인베이스 등 관련 기업 40여 곳은 개인정보 침해 가능성을 언급하며 줄곧 반대 입장을 밝혀왔는데요. 하지만 최근 가상자산 관련 범죄 사례가 급증하고 있는 만큼 규제 움직임이 힘을 얻고 있는 것으로 풀이됩니다. 관련 기관 논의는 오는 30일까지 마무리될 예정인데 이후 EU 위원회와 의회의 승인 절차만 남게 됩니다. ◇ 스피릿 M A 주주투표 또 미뤄 미국 저비용 항공사 스피릿 항공 인수전이 뜨거워지고 있습니다. 경쟁을 벌이고 있는 프론티어와 제트블루 항공이 앞다퉈 인수가를 높이고 있는데요. 더 높은 가격이 제시되면서 스피릿항공은 당초 어제 진행하기로 했던 프론티어 항공과의 합병안에 대한 주주투표를 이달 8일로 연기했습니다. 이미 한 차례 미뤘는데 다시 한번 주주투표를 연기하면서 스피릿이 제트블루와의 합병 여지를 되살리는 것 아니냐는 추측이 나오고 있는데요. 규제 등을 이유로 프론티어와의 인수합병에 무게가 실리는 듯 했지만 제트블루가 계속해서 인수가를 높이며 러브콜을 보내자 고민하는 모습입니다. 스피릿을 품게 되면 미국 항공 업계 다섯 손가락 안에 들 수 있게 되기 때문에 최후의 승자가 누가 될지에 관심이 쏠리고 있습니다. ◇ 텐센트·바이트댄스 하반기 또 감원 중국 대표 기술기업들이 연일 몸집 줄이기에 나서고 있습니다. 월스트리트저널은 텐센트와 바이트댄스를 비롯한 빅테크 기업들이 지난해 수만명의 인력을 줄인데 이어 또 한번 대규모 구조조정을 준비하고 있다고 전했는데요. 과거 수익성이 없는 사업을 정리하기 위해 감원에 나섰다면 최근에는 중국 경제가 고비를 겪으면서 비용을 줄이기 위해 몸집을 줄이고 있다고 설명했습니다. 특히 최근 당국이 빅테크 때리기 기조를 거둬들이고 있다는 점에서 거시적 위기가 새로운 숙제로 떠올랐다고 분석했는데요. 이미 상반기 전 부문에 걸쳐 인력의 10%가 넘게 정리한 텐센트는 하반기 대규모 추가 감원을 예고한데 더해 일부 사업의 전면 철수까지 검토 중인 것으로 알려졌는데요. 그간 규제 철퇴에 허덕이던 중국 빅테크들이 경기 둔화라는 악재까지 겹치면서 고민이 깊어지는 모습입니다. 지금까지 글로벌 비즈였습니다.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef7dd451-51fe-4653-b5f9-dd685029eb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fb671-d7b9-435f-bf09-cc501257b5fd",
   "metadata": {},
   "source": [
    "## 7. 파인 튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d307ef-32ea-464c-a0e9-20e5a3e7059d",
   "metadata": {},
   "source": [
    "`AutoPeftModelForCausalLM()`의 입력으로 LoRA Adapter가 저장된 체크포인트의 주소를 넣으면 LoRA Adapter가 기존의 LLM과 부착되어 로드됩니다. 이 과정은 LoRA Adapter의 가중치를 사전 학습된 언어 모델(LLM)에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n",
    "\n",
    "`peft_model_id` 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. `\"qwen2-7b-rag-ko/checkpoint-285\"`는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서 해당 가중치를 불러옵니다.\n",
    "\n",
    "`fine_tuned_model`은 `AutoPeftModelForCausalLM.from_pretrained` 메서드를 통해 체크포인트를 로드하여 생성됩니다. 이 메서드는 LLM과 LoRA Adapter를 결합하고, 최적화된 설정으로 모델을 메모리에 로드합니다. `device_map=\"auto\"` 옵션은 모델을 자동으로 GPU에 배치합니다.\n",
    "\n",
    "`pipeline`은 Hugging Face의 고수준 유틸리티로, NLP 작업(예: 텍스트 생성, 번역, 요약 등)을 간단히 수행할 수 있게 해줍니다. 이 코드에서 사용된 `pipeline(\"text-generation\")`은 텍스트 생성 작업을 수행하기 위한 파이프라인 객체를 생성합니다. 파이프라인은 내부적으로 모델과 토크나이저를 관리하여, 입력 텍스트를 토큰화하고, 모델을 통해 생성된 결과를 다시 디코딩하여 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "\n",
    "이 코드는 미세 조정된 LLM을 로드한 뒤, 이를 이용해 텍스트 생성 작업을 간단히 수행할 수 있도록 준비하는 데 목적이 있습니다. `pipeline`을 통해 텍스트 생성 작업을 실행하면, 입력 텍스트에 기반하여 모델이 다음 토큰을 예측하고 이를 반복적으로 생성합니다. 이 과정은 사용자에게 자연스러운 텍스트를 출력하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f1f226-0e2c-423e-8c34-5382a52e12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8064e480-4f34-41af-a5db-305da8dba1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b185a3932464208b43be44b1aac0324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen3-4b-finance-new-summarizer/checkpoint-186\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "870ab287-8236-4d4f-96aa-fc87322ec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f1a1c64-88dd-49d0-83e0-2ef87a1a9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a08b49-92e5-4b50-a2ce-f2276cc2ae7d",
   "metadata": {},
   "source": [
    "임의로 테스트 데이터 10번부터 14번까지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dfb9938-85bb-4c60-8faa-6b083f4dac5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC'], 'negative_keywords': ['스마트폰 판매량 감소', '가트너', '애플', '엔비디아', 'TSMC'], 'positive_impact_stocks': [], 'positive_keywords': [],'reason_for_negative_impact': '스마트폰 판매량 감소 전망과 관련하여 애플, 엔비디아, TSMC 등 스마트폰 제조사와 반도체 업체들이 수요 부진으로 인한 압력이 가해질 것으로 예상되기 때문입니다.','reason_for_positive_impact': '','summary': '글로벌 비즈는 올해 전세계 스마트폰 판매량이 7% 감소할 것으로 전망하며, 중국의 봉쇄조치와 인플레이션으로 소비자들이 지갑을 열기 주저하고 있다는 점을 설명했습니다. 또한, EU가 가상자산을 이용한 돈세탁을 막기 위한 규제 움직임과 미국 저비용 항공사 스피릿 항공의 인수전, 중국 대표 기술기업들의 감원 움직임에 대해 다루었습니다.'}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '동네가게 오래함께', '광고 제작', '소상공인', '지역사회'],'reason_for_negative_impact': '','reason_for_positive_impact': '야놀자는 포커스미디어와의 협력으로 지역 내 우수 소상공인을 발굴하고 맞춤형 광고를 제작하여 지역사회와 상생을 도모하는 캠페인을 진행함으로써 지역경제 활성화에 기여할 것으로 기대된다.','summary': '야놀자는 포커스미디어와 지역 내 우수 소상공인을 발굴하고 맞춤형 광고를 제작하여 지역사회와 상생을 도모하는 '동네가게 오래함께' 캠페인을 진행한다. 캠페인은 야놀자와 포커스미디어가 전액 부담하는 14억 원 규모의 광고 제작 및 송출을 통해 지역 내 소상공인의 인지도 제고와 매출 증대를 목표로 한다.'}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['삼성바이오로직스', 'MSD', '위탁생산계약', '2768억원'],'reason_for_negative_impact': '','reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증대에 기여할 것으로 예상되기 때문입니다.','summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증대에 기여할 것으로 예상됩니다.'}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스 콘텐츠', '스마트 가전', 'K POP', '디지털 라이프스타일'],'reason_for_negative_impact': '','reason_for_positive_impact': 'LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표하며, 이는 디지털 기술과 K POP 콘텐츠를 결합한 새로운 시장 진출을 의미합니다. 이는 두 회사의 시장 점유율 확대와 수익 증가에 긍정적인 영향을 미칠 수 있습니다.','summary': 'LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표하며, K POP과 디지털 기술을 결합한 새로운 시장 진출을 추진하고 있다.'}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['청정환기시스템', '공기청정', '미세먼지', '코로나19', '건강'],'reason_for_negative_impact': '','reason_for_positive_impact': '경동나비엔의 청정환기시스템은 미세먼지와 바이러스를 효과적으로 제거하여 건강한 실내 환경을 제공하는 데 기여할 수 있어, 관련 제품에 대한 수요가 증가할 가능성이 있다.','summary': '경동나비엔이 장마철에도 실내 미세먼지를 효과적으로 관리할 수 있는 청정환기시스템을 제안하며, 이는 미세먼지와 바이러스를 제거하여 건강한 실내 환경을 제공하는 데 기여할 수 있다.'}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f56c9c-12d6-41c1-967d-ee1a8b87fdc2",
   "metadata": {},
   "source": [
    "## 8. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56033b-4818-453a-8296-1daa734fc097",
   "metadata": {},
   "source": [
    "이번에는 LoRA Adapter를 merge하지 않은 기본 모델로 테스트 데이터에 대해서 인퍼런스해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86f4ed2b-dbaa-49d7-8b9e-79d63b16dce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db11adfada6422980f24eadbecd9db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "base_model_id = \"Qwen/Qwen3-4B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b1d4a2b-70f3-4f8b-8ebe-dd39d9bed272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [],\n",
      "\"reason_for_positive_impact\": \"\",\n",
      "\"positive_keywords\": [],\n",
      "\"negative_impact_stocks\": [\"애플\", \"엔비디아\", \"TSMC\", \"텐센트\", \"바이트댄스\"],\n",
      "\"reason_for_negative_impact\": \"스마트폰 판매량 감소, 중국 봉쇄 조치, 인플레이션, 소비자 지갑 열기 주저, 수요 감소, 기술 업체 및 빅테크 기업의 압력 증가, 경제 침체 우려\",\n",
      "\"negative_keywords\": [\"스마트폰 판매량\", \"봉쇄 조치\", \"인플레이션\", \"소비자 지갑\", \"수요 감소\", \"기술 업체\", \"빅테크 기업\", \"경제 침체\"], \n",
      "\"summary\": \"가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것으로 예측하며, 중국의 봉쇄 조치와 인플레이션으로 소비자 수요가 줄어들 것으로 분석했습니다. 또한, 애플, 엔비디아, TSMC, 텐센트, 바이트댄스 등 기술 업체와 빅테크 기업이 수요 감소로 인해 압력이 가중될 것으로 예상했습니다. EU는 가상자산을 이용한 돈세탁을 막기 위해 규제를 강화하고 있으며, 스피릿 항공의 인수전은 지연되고 있습니다.\"}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"야놀자\", \"포커스미디어\"],\n",
      "\"reason_for_positive_impact\": \"이 뉴스는 야놀자와 포커스미디어가 지역 소상공인을 지원하는 캠페인을 진행함으로써 두 회사의 브랜드 인지도와 지역 경제 활성화에 긍정적인 영향을 줄 것으로 예상됩니다. 또한, 광고 제작 및 송출 비용을 전액 부담하는 점도 회사의 사회적 책임과 마케팅 역량을 강조하는 데 기여할 수 있습니다.\",\n",
      "\"positive_keywords\": [\"야놀자\", \"포커스미디어\", \"지역 소상공인\", \"광고 제작\", \"광고 송출\", \"지역 경제 활성화\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"\",\n",
      "\"negative_keywords\": [],\n",
      "\"summary\": \"야놀자는 포커스미디어와 '동네가게 오래함께' 캠페인을 진행한다고 발표했다. 이번 캠페인은 지역 내 우수 소상공인을 발굴하고 맞춤형 광고를 제작해 지역 홍보를 지원하며, 총 14억 원 규모의 광고 제작 및 송출 비용은 양사가 전액 부담한다. 첫 광고는 서울시 노원구, 동작구를 시작으로 2개월 간 방영되며, 연말까지 대상 범위를 지속 확대할 예정이다.\"}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"삼성바이오로직스\"],\n",
      "\"reason_for_positive_impact\": \"삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결함으로써 기업의 매출과 수익성에 긍정적인 영향을 줄 것으로 예상된다.\",\n",
      "\"positive_keywords\": [\"삼성바이오로직스\", \"MSD\", \"위탁생산계약\", \"의약품\", \"2768억원\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"\",\n",
      "\"negative_keywords\": [],\n",
      "\"summary\": \"삼성바이오로직스는 미국 제약기업 MSD와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결했다. 계약 기간은 2022년 7월 1일부터 2028년 12월 31일이며, 고객사의 수요 증가에 따라 계약금액이 3억8186만 달러로 증가할 수 있다.\"}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"LG전자\", \"SM엔터테인먼트\"],\n",
      "\"reason_for_positive_impact\": \"LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 공동으로 출시함으로써 두 회사의 디지털 기술력과 콘텐츠 경쟁력이 강화될 것으로 예상됩니다. 이는 두 회사의 시장 점유율 확대와 관련된 긍정적인 영향을 줄 것으로 보입니다.\",\n",
      "\"positive_keywords\": [\"LG전자\", \"SM엔터테인먼트\", \"피트니스 캔디\", \"디지털 피트니스 콘텐츠\", \"디지털 기술\", \"콘텐츠 경쟁력\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"\",\n",
      "\"negative_keywords\": [],\n",
      "\"summary\": \"LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 공동으로 발표했습니다. 이는 두 회사의 디지털 기술력과 콘텐츠 경쟁력을 강화하고, 피트니스 및 헬스케어 산업에 기여할 것으로 기대됩니다.\"}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"경동나비엔\"],\n",
      "\"reason_for_positive_impact\": \"이 뉴스는 경동나비엔의 청정환기시스템이 미세먼지 문제를 해결하는 새로운 솔루션으로 주목받고 있으며, 이에 대한 수요가 증가할 것으로 예상되어 경동나비엔의 주가에 긍정적인 영향을 줄 것으로 추정됩니다.\",\n",
      "\"positive_keywords\": [\"청정환기시스템\", \"미세먼지\", \"공기청정\", \"장마철\", \"경동나비엔\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"\",\n",
      "\"negative_keywords\": [],\n",
      "\"summary\": \"장마철에 창문을 열지 않고도 실내 공기를 청정하게 유지할 수 있는 '나비엔 청정환기시스템'이 소개되었다. 이 시스템은 미세먼지와 바이러스를 효과적으로 제거하며, 주방에서 발생하는 요리 매연도 처리할 수 있는 특화 기능을 갖추고 있다. 이에 따라 미세먼지 문제를 해결하는 데 대한 수요가 증가할 것으로 예상된다.\"}\n",
      "    label:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3c20c-cd63-4ccb-ad28-6b179bd942f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
