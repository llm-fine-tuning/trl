{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b152fb-bcb6-40c7-8c93-32c90a55218e",
   "metadata": {},
   "source": [
    "이 실습은 Runpod에서 A100 SXM GPU 1개로 진행되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591659b-83e1-45d7-b166-2c85a2f74367",
   "metadata": {},
   "source": [
    "## 1. 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00257c8-0292-4e9b-97e6-58e510517249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.9.41)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.45.1 in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
      "Requirement already satisfied: datasets==3.0.1 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
      "Requirement already satisfied: trl==0.11.1 in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
      "Requirement already satisfied: peft==0.13.0 in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1) (3.12.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl==0.11.1) (0.9.22)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.9.41)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (14.0.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.11.1) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.4.0\"\n",
    "%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91394a-4b7e-415f-8a41-0ee783d41dc3",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fc9e67-2865-46c8-9d5d-99b5a227a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989235b0-b263-4b6a-8784-bd02b66bebe2",
   "metadata": {},
   "source": [
    "1. **JSON 파일 로드 함수 정의**\n",
    "- `load_dataset()` 함수를 정의하여 지정된 파일 경로에서 JSON 파일을 읽어옵니다. `open()` 함수에 `encoding='utf-8'` 옵션을 사용하여 한글 등 유니코드 문자가 포함된 데이터도 올바르게 처리할 수 있도록 합니다. `json.load()`를 통해 JSON 형식의 데이터를 파이썬 객체로 변환하여 반환합니다.\n",
    "\n",
    "2. **train/test 분할 비율 설정**\n",
    "- `test_ratio = 0.5`로 테스트 분할 비율을 50%로 설정합니다. 이는 전체 데이터 중 절반을 테스트용으로, 나머지 절반을 학습용으로 사용하겠다는 의미입니다. 이 비율은 필요에 따라 조정 가능하며, 일반적으로 0.2(20%) 또는 0.3(30%)으로 설정하기도 합니다.\n",
    "\n",
    "3. **데이터 로드 및 크기 확인**\n",
    "- `load_dataset('text_to_sql_data.json')`을 호출하여 text-to-sql 데이터셋을 메모리로 로드합니다. 이 데이터셋은 instruction과 output 필드로 구성되어 있으며, instruction은 자연어 질문을, output은 해당하는 SQL 쿼리를 담고 있습니다.\n",
    "- `print(f\"전체 데이터 개수: {len(dataset)}\")`로 로드된 데이터셋의 총 샘플 수를 출력하여 데이터 규모를 확인합니다.\n",
    "\n",
    "4. **전체 데이터의 인덱스를 train/test로 분할**\n",
    "- `data_indices = list(range(len(dataset)))`로 0부터 데이터셋 길이-1까지의 연속된 인덱스 리스트를 생성합니다.\n",
    "- `random.shuffle(data_indices)`를 사용하여 인덱스 리스트를 무작위로 섞습니다. 이는 데이터의 원래 순서에 따른 편향을 방지하고, 훈련 및 테스트 데이터가 고르게 분포되도록 보장합니다.\n",
    "- `test_size = int(len(data_indices) * test_ratio)`로 테스트 데이터셋에 포함될 샘플 수를 계산합니다.\n",
    "- `test_indices = data_indices[:test_size]`로 섞인 인덱스 리스트의 앞부분을 테스트 데이터 인덱스로 할당합니다.\n",
    "- `train_indices = data_indices[test_size:]`로 나머지 인덱스들을 훈련 데이터 인덱스로 할당합니다.\n",
    "- 각각의 인덱스 개수를 출력하여 분할이 올바르게 수행되었는지 확인합니다.\n",
    "\n",
    "5. **OpenAI format으로 데이터 변환을 위한 함수**\n",
    "- `format_data()` 함수를 정의하여 각 샘플을 OpenAI API와 호환되는 메시지 형식으로 변환합니다.\n",
    "- 이 함수는 입력된 샘플에서 \"instruction\"과 \"output\" 필드를 추출하여 \"messages\" 배열을 구성합니다.\n",
    "- 시스템 메시지에는 \"당신은 text-to-sql을 수행해야 합니다.\"라는 역할 정의를 포함하여 AI 모델에게 수행할 작업을 명확히 지시합니다.\n",
    "- 사용자 메시지에는 원본 데이터의 \"instruction\" 필드(자연어 질문)를 배치합니다.\n",
    "- 어시스턴트 응답에는 \"output\" 필드(해당하는 SQL 쿼리)를 배치합니다.\n",
    "- 반환되는 형식은 OpenAI의 chat completion API와 호환되는 형태로, 대화형 AI 훈련에 바로 사용할 수 있습니다.\n",
    "\n",
    "6. **분할된 데이터를 OpenAI format으로 변환**\n",
    "- `train_dataset = [format_data(dataset[i]) for i in train_indices]`로 훈련용 인덱스에 해당하는 모든 데이터를 OpenAI 형식으로 변환합니다.\n",
    "- `test_dataset = [format_data(dataset[i]) for i in test_indices]`로 테스트용 인덱스에 해당하는 모든 데이터를 OpenAI 형식으로 변환합니다.\n",
    "- 리스트 컴프리헨션을 사용하여 효율적으로 대량의 데이터를 일괄 변환하며, 각 샘플은 메시지 형태의 대화 구조로 재구성됩니다.\n",
    "- 변환된 데이터는 파인튜닝이나 프롬프트 엔지니어링에 즉시 활용할 수 있는 표준화된 형식을 갖게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf7d985-0ef0-47d5-a934-ab59fcd34c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-31 18:18:26--  https://github.com/llm-fine-tuning/LLaMA-Factory/raw/refs/heads/main/data/text_to_sql_data.json\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/llm-fine-tuning/LLaMA-Factory/refs/heads/main/data/text_to_sql_data.json [following]\n",
      "--2025-05-31 18:18:27--  https://raw.githubusercontent.com/llm-fine-tuning/LLaMA-Factory/refs/heads/main/data/text_to_sql_data.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3676178 (3.5M) [application/octet-stream]\n",
      "Saving to: ‘text_to_sql_data.json.1’\n",
      "\n",
      "text_to_sql_data.js 100%[===================>]   3.51M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-05-31 18:18:27 (102 MB/s) - ‘text_to_sql_data.json.1’ saved [3676178/3676178]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/llm-fine-tuning/LLaMA-Factory/raw/refs/heads/main/data/text_to_sql_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef323ce1-aeb4-446d-a331-6468e83d42c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 개수: 5000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# 1. JSON 파일 로드\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "    return dataset\n",
    "\n",
    "# 2. train/test 분할 비율 설정 (0.5면 5:5로 분할)\n",
    "test_ratio = 0.5\n",
    "\n",
    "# 3. 데이터 로드\n",
    "dataset = load_dataset('text_to_sql_data.json')\n",
    "print(f\"전체 데이터 개수: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba571c92-ab5c-40d8-b995-c410745ac78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 인덱스 개수: 2500\n",
      "Test 인덱스 개수: 2500\n"
     ]
    }
   ],
   "source": [
    "# 4. 전체 데이터의 인덱스를 train/test로 분할\n",
    "data_indices = list(range(len(dataset)))\n",
    "random.shuffle(data_indices)  # 랜덤 셔플로 더 좋은 분할\n",
    "\n",
    "test_size = int(len(data_indices) * test_ratio)\n",
    "test_indices = data_indices[:test_size]\n",
    "train_indices = data_indices[test_size:]\n",
    "\n",
    "print(f\"Train 인덱스 개수: {len(train_indices)}\")\n",
    "print(f\"Test 인덱스 개수: {len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4567b5-21ae-404f-bff4-425567676bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. OpenAI format으로 데이터 변환을 위한 함수\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 text-to-sql을 수행해야 합니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": sample[\"instruction\"]  # instruction을 user prompt로 사용\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"output\"]  # output을 assistant response로 사용\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 6. 분할된 데이터를 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_indices]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4390cd7e-a31c-414a-97d2-a4bd3cc83a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': '당신은 text-to-sql을 수행해야 합니다.'},\n",
       " {'role': 'user',\n",
       "  'content': \"입력 텍스트: 보안 사건에 대한 평균 반응 시간이 가장 긴 사용자는 누구인가요?\\n\\nDDL statements:\\nCREATE TABLE user_reactions (id INT, user_id INT, incident_id INT, reaction_time INT); CREATE TABLE users (id INT, name VARCHAR(50)); INSERT INTO user_reactions (id, user_id, incident_id, reaction_time) VALUES (1, 1, 1, 60), (2, 2, 2, 30), (3, 3, 3, 90), (4, 1, 4, 120), (5, 4, 5, 45), (6, 5, 6, 75), (7, 2, 7, 105); INSERT INTO users (id, name) VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Charlie'), (4, 'David'), (5, 'Eve');\\n\\n위의 테이블 명세와 사용자의 입력 텍스트를 바탕으로 SQL 쿼리를 작성합니다.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': '쿼리 작성: SELECT users.name, AVG(user_reactions.reaction_time) as avg_reaction_time FROM user_reactions INNER JOIN users ON user_reactions.user_id = users.id GROUP BY users.name ORDER BY avg_reaction_time DESC;'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 345번 데이터 출력\n",
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc8f819-89cc-445a-999d-a6299a08d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1769a733-1ac3-411c-9fcd-0220aedd042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 text-to-sql을 수행해야 합니다.', 'role': 'system'},\n",
       "  {'content': '입력 텍스트: 2021년 3분기에 EU에서 판매된 지속 가능성 점수가 7점 미만인 제품을 제외한 스킨케어 제품의 평균 지속 가능성 점수를 계산하세요.\\n\\nDDL statements:\\nCREATE TABLE skincare_sales(sale_id INT, product_id INT, sale_date DATE, sustainability_score INT);CREATE TABLE products(product_id INT, product_name TEXT, category TEXT, country TEXT);\\n\\n위의 테이블 명세와 사용자의 입력 텍스트를 바탕으로 SQL 쿼리를 작성합니다.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"쿼리 작성: SELECT AVG(s.sustainability_score) FROM skincare_sales s JOIN products p ON s.product_id = p.product_id WHERE p.category = 'skincare' AND p.country LIKE 'EU%' AND s.sustainability_score >= 7 AND DATE_PART('year', s.sale_date) = 2021 AND DATE_PART('quarter', s.sale_date) = 3;\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a52be8-3af6-44d8-bf39-5dc14319dc78",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "212cd2a7-ac0b-4c8c-8926-c1d5d80f2171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f6a4e9118541e29208c7c11bb343cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd53f9a9e864240a627cd1d11b3294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4466d6b0895443128ec278da9e4a3cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06ed8159f434d5596a90764754fb70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c05de8c9659444a83a18d12f27a8bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318b999a3fba481aa0b3fb6afe44b933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8c5d496c604ee383b34a52082c47ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47237af3d2d445b686ad4d9f836013bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b0287f4d5a4b33a6ab08076989a82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488caf3c951b46a696f4276e3ab4c89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f9c780a5b244e595422d79ec610ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb082fc8733440fdb3d7ec452910dfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc6e96a3-0d81-4e76-a385-d1c6e8146dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 text-to-sql을 수행해야 합니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "입력 텍스트: 2021년 3분기에 EU에서 판매된 지속 가능성 점수가 7점 미만인 제품을 제외한 스킨케어 제품의 평균 지속 가능성 점수를 계산하세요.\n",
      "\n",
      "DDL statements:\n",
      "CREATE TABLE skincare_sales(sale_id INT, product_id INT, sale_date DATE, sustainability_score INT);CREATE TABLE products(product_id INT, product_name TEXT, category TEXT, country TEXT);\n",
      "\n",
      "위의 테이블 명세와 사용자의 입력 텍스트를 바탕으로 SQL 쿼리를 작성합니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "쿼리 작성: SELECT AVG(s.sustainability_score) FROM skincare_sales s JOIN products p ON s.product_id = p.product_id WHERE p.category = 'skincare' AND p.country LIKE 'EU%' AND s.sustainability_score >= 7 AND DATE_PART('year', s.sale_date) = 2021 AND DATE_PART('quarter', s.sale_date) = 3;<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801263cf-ac85-4e26-9fc6-dbd057552905",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2266ebe9-d3ce-4fed-956b-6a830acfe0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a625541-bf3d-49fc-9b91-0c49193b37dc",
   "metadata": {},
   "source": [
    "`lora_alpha`: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "\n",
    "`lora_dropout`: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. 0.1로 설정하면 학습 중 10%의 뉴런이 비활성화됩니다.\n",
    "\n",
    "`r`: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "\n",
    "`bias`: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. \"none\"으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. \"all\" 또는 \"lora_only\"와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "\n",
    "`target_modules`: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 \"q_proj\"와 \"v_proj\"를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "\n",
    "`task_type:` LoRA가 적용되는 작업 유형을 지정합니다. \"CAUSAL_LM\"은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 \"SEQ2SEQ_LM\"(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ae4257-f495-4a5a-a7e6-852455deb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"llama3-8b-text-to-sql\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da90d2d-9e29-43e7-9c04-27e63f82d9a1",
   "metadata": {},
   "source": [
    "`utput_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\n",
    "`num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니다.\n",
    "\n",
    "`per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있습니다.\n",
    "\n",
    "`gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 2로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 유용합니다.\n",
    "\n",
    "`gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질 수 있습니다.\n",
    "\n",
    "`optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적화기를 사용합니다.\n",
    "\n",
    "`logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마다 로그를 기록합니다.\n",
    "\n",
    "`save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스텝마다 모델이 저장됩니다.\n",
    "\n",
    "`save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50 스텝마다 모델을 저장합니다.\n",
    "\n",
    "`bf16`: bfloat16 정밀도를 사용하도록 설정합니다. bfloat16은 FP32와 유사한 범위를 제공하면서 메모리와 계산 효율성을 높입니다.\n",
    "\n",
    "`learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 사용하여 안정적인 학습을 유도합니다.\n",
    "\n",
    "`max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 조정하여 폭발적 그래디언트를 방지합니다.\n",
    "\n",
    "`warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다. 학습의 안정성을 높이기 위해 사용됩니다.\n",
    "\n",
    "`lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"constant\"`는 학습률을 일정하게 유지합니다.\n",
    "\n",
    "`push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "\n",
    "`remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다. True로 설정하면 메모리를 절약할 수 있습니다.\n",
    "\n",
    "`dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset: True`로 설정하여 데이터셋 준비 단계를 건너뜁니다.\n",
    "\n",
    "`report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05ef38-fd56-4def-8569-9512bf56b81f",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a0c36ca-5d28-4bdf-b27f-ca7ab63b4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for example in batch:\n",
    "        messages = example[\"messages\"]\n",
    "\n",
    "        # LLaMA 3 채팅 템플릿 적용 (시작 토큰 포함)\n",
    "        prompt = \"<|begin_of_text|>\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"].strip()\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "\n",
    "        # 마지막 assistant 메시지는 응답으로 간주하고 레이블에 포함\n",
    "        text = prompt.strip()\n",
    "\n",
    "        # 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        # assistant 응답의 시작 위치 찾기\n",
    "        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n",
    "\n",
    "        # 레이블 범위 지정\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_tokens):\n",
    "            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n",
    "                start = i + len(assistant_tokens)\n",
    "                end = start\n",
    "                while end <= len(input_ids) - len(eot_tokens):\n",
    "                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start, end):\n",
    "                    labels[j] = input_ids[j]\n",
    "                for j in range(end, end + len(eot_tokens)):\n",
    "                    labels[j] = input_ids[j]  # <|eot_id|> 토큰도 포함\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "\n",
    "    # 패딩 처리\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        pad_len = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n",
    "        new_batch[\"labels\"][i].extend([-100] * pad_len)\n",
    "\n",
    "    for k in new_batch:\n",
    "        new_batch[k] = torch.tensor(new_batch[k])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e77712-7612-4c6c-a102-d0ff557d122f",
   "metadata": {},
   "source": [
    "입력으로 사용되는 라마 챗 템플릿은 아래와 같습니다.되어 반환됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094bc1d-d47e-4932-98ba-9d2f30949809",
   "metadata": {},
   "source": [
    "```python\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "시스템 프롬프트<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "유저 프롬프트<|eot_id|><|start_header_id|>assistant<|end_header|>LLM의 답변<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f99a67-f433-498f-a7db-31af0a9950aa",
   "metadata": {},
   "source": [
    "collate_fn(batch) 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 label_ids를 생성합니다. 레이블 데이터의 경우 실제 assistant 응답 부분을 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5cc839-5f65-44c8-b475-ac613fde97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 227])\n",
      "어텐션 마스크 형태: torch.Size([1, 227])\n",
      "레이블 형태: torch.Size([1, 227])\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 최대 길이 제한\n",
    "max_seq_length=8192\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1. 즉, 데이터 1개에 대해서 전처리를 진행해본다.)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7578d30e-fb04-4c6e-974d-a613e7cfa785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[128000, 128000, 128006, 9125, 128007, 198, 65895, 83628, 34804, 1495, 4791, 1355, 1498, 18359, 125201, 110513, 109670, 13, 128009, 128006, 882, 128007, 198, 44966, 29854, 10997, 45204, 54289, 25, 220, 2366, 16, 100392, 220, 18, 80816, 109509, 10013, 57575, 116604, 53400, 67890, 102130, 96451, 33931, 106313, 123503, 220, 22, 101838, 101412, 73653, 32428, 112785, 18359, 63171, 104065, 24486, 80307, 45780, 61857, 107213, 32179, 112785, 21028, 105276, 67890, 102130, 96451, 33931, 106313, 120045, 95303, 86157, 92245, 382, 59881, 12518, 512, 23421, 14700, 80705, 48167, 1161, 1604, 851, 9403, 11, 2027, 851, 9403, 11, 6412, 4257, 29643, 11, 41329, 10622, 9403, 1237, 23421, 14700, 3956, 20040, 851, 9403, 11, 2027, 1292, 16139, 11, 5699, 16139, 11, 3224, 16139, 629, 82001, 21028, 107573, 13094, 105551, 104167, 42529, 81673, 41820, 110257, 43449, 10997, 45204, 54289, 18918, 82818, 120378, 43139, 8029, 3396, 123, 120, 106064, 114839, 61938, 13, 128009, 128006, 78191, 128007, 198, 115740, 120, 29102, 114839, 25, 19638, 71514, 1161, 516, 74585, 10622, 8, 4393, 80705, 48167, 274, 13369, 3956, 281, 6328, 274, 12377, 851, 284, 281, 12377, 851, 5401, 281, 18898, 284, 364, 4991, 67003, 6, 3651, 281, 34424, 21170, 364, 39907, 28337, 3651, 274, 516, 74585, 10622, 2669, 220, 22, 3651, 29643, 29378, 493, 3236, 518, 274, 93317, 4257, 8, 284, 220, 2366, 16, 3651, 29643, 29378, 493, 33115, 518, 274, 93317, 4257, 8, 284, 220, 18, 26, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b8d7425-6d62-44af-b726-689ca29c6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids 디코딩 결과:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "당신은 text-to-sql을 수행해야 합니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "입력 텍스트: 2021년 3분기에 EU에서 판매된 지속 가능성 점수가 7점 미만인 제품을 제외한 스킨케어 제품의 평균 지속 가능성 점수를 계산하세요.\n",
      "\n",
      "DDL statements:\n",
      "CREATE TABLE skincare_sales(sale_id INT, product_id INT, sale_date DATE, sustainability_score INT);CREATE TABLE products(product_id INT, product_name TEXT, category TEXT, country TEXT);\n",
      "\n",
      "위의 테이블 명세와 사용자의 입력 텍스트를 바탕으로 SQL 쿼리를 작성합니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "쿼리 작성: SELECT AVG(s.sustainability_score) FROM skincare_sales s JOIN products p ON s.product_id = p.product_id WHERE p.category = 'skincare' AND p.country LIKE 'EU%' AND s.sustainability_score >= 7 AND DATE_PART('year', s.sale_date) = 2021 AND DATE_PART('quarter', s.sale_date) = 3;<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# 디코딩된 input_ids 출력\n",
    "decoded_text = tokenizer.decode(\n",
    "    batch[\"input_ids\"][0].tolist(),\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\ninput_ids 디코딩 결과:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5ec99e0-5360-40b2-b622-b8c7d2110454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 115740, 120, 29102, 114839, 25, 19638, 71514, 1161, 516, 74585, 10622, 8, 4393, 80705, 48167, 274, 13369, 3956, 281, 6328, 274, 12377, 851, 284, 281, 12377, 851, 5401, 281, 18898, 284, 364, 4991, 67003, 6, 3651, 281, 34424, 21170, 364, 39907, 28337, 3651, 274, 516, 74585, 10622, 2669, 220, 22, 3651, 29643, 29378, 493, 3236, 518, 274, 93317, 4257, 8, 284, 220, 2366, 16, 3651, 29643, 29378, 493, 33115, 518, 274, 93317, 4257, 8, 284, 220, 18, 26, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6938e31-3109-42b9-ae14-8895c087649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labels 디코딩 결과 (-100 제외):\n",
      "쿼리 작성: SELECT AVG(s.sustainability_score) FROM skincare_sales s JOIN products p ON s.product_id = p.product_id WHERE p.category = 'skincare' AND p.country LIKE 'EU%' AND s.sustainability_score >= 7 AND DATE_PART('year', s.sale_date) = 2021 AND DATE_PART('quarter', s.sale_date) = 3;<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# -100이 아닌 부분만 골라 디코딩\n",
    "label_ids = [token_id for token_id in batch[\"labels\"][0].tolist() if token_id != -100]\n",
    "\n",
    "decoded_labels = tokenizer.decode(\n",
    "    label_ids,\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\nlabels 디코딩 결과 (-100 제외):\")\n",
    "print(decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e849a2-066c-4453-8c85-f997cdaa3e26",
   "metadata": {},
   "source": [
    "## 5. 어텐션 마스크 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1339de-ac4c-46d7-8081-73a538a89c73",
   "metadata": {},
   "source": [
    "### 배치 크기의 의미\n",
    "\n",
    "배치 크기란 모델이 한 번에 학습하는 데이터 샘플의 수를 의미합니다. 예를 들어 배치 크기가 3이면, 모델은 세 개의 데이터 샘플을 동시에 처리합니다. 이렇게 병렬적으로 학습하면 계산 효율성이 높아지고 학습 속도가 빨라지는 이점이 있습니다.\n",
    "\n",
    "### 텍스트 데이터의 길이 문제\n",
    "\n",
    "자연어 처리에서 각 샘플(문장, 대화 등)은 길이가 다양합니다. 예를 들어 배치 크기 3인 경우:\n",
    "\n",
    "- 샘플1: \"인공지능이란 무엇인가요?\" → [101, 4089, 8024, 6356, 102] (5 토큰)\n",
    "- 샘플2: \"오늘 날씨가 정말 좋네요.\" → [101, 3157, 2533, 4120, 2642, 8730, 6824, 102] (8 토큰)\n",
    "- 샘플3: \"딥러닝 모델을 학습시키는 방법을 알려주세요.\" → [101, 2982, 3478, 4567, 2053, 8276, 5036, 2355, 4602, 7312, 102] (11 토큰)\n",
    "\n",
    "여기서 101과 102는 특수 토큰으로, 각각 문장의 시작과 끝을 표시합니다.\n",
    "\n",
    "### 패딩의 필요성\n",
    "\n",
    "신경망의 내부 연산은 고정된 크기의 입력을 요구합니다. 이 문제를 해결하기 위해 '패딩'을 사용하여 모든 샘플의 길이를 배치 내 가장 긴 샘플에 맞춥니다.\n",
    "\n",
    "위 예시에서는 가장 긴 샘플3(11 토큰)에 맞춰 다른 샘플들에 패딩(0)을 추가합니다:\n",
    "\n",
    "- 샘플1: [101, 4089, 8024, 6356, 102, 0, 0, 0, 0, 0, 0] (5 실제 + 6 패딩)\n",
    "- 샘플2: [101, 3157, 2533, 4120, 2642, 8730, 6824, 102, 0, 0, 0] (8 실제 + 3 패딩)\n",
    "- 샘플3: [101, 2982, 3478, 4567, 2053, 8276, 5036, 2355, 4602, 7312, 102] (11 실제 토큰)\n",
    "\n",
    "이렇게 하면 모든 샘플이 동일한 길이(11)를 가지게 되어 하나의 배치로 처리할 수 있습니다.\n",
    "\n",
    "### 어텐션 마스크의 필요성\n",
    "\n",
    "패딩을 추가하면 모델이 어떤 토큰이 실제 내용이고 어떤 토큰이 의미 없는 패딩인지 구분해야 합니다. 이를 위해 '어텐션 마스크'를 사용합니다:\n",
    "\n",
    "- 샘플1 마스크: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "- 샘플2 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "- 샘플3 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "트랜스포머 모델의 어텐션 메커니즘은 이 마스크를 사용하여 패딩 토큰을 무시하고 실제 의미 있는 토큰에만 집중합니다. 이렇게 하면 패딩된 부분이 모델의 예측이나 학습에 영향을 미치지 않게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c70f56b-7eac-4e3d-8635-c363539bfccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번 데이터 길이: 227\n",
      "1번 데이터 길이: 176\n",
      "\n",
      "배치 처리 후:\n",
      "입력 ID 형태: torch.Size([2, 227])\n",
      "어텐션 마스크 형태: torch.Size([2, 227])\n",
      "0번 샘플 어텐션 마스크 합계: 227\n",
      "1번 샘플 어텐션 마스크 합계: 176\n",
      "\n",
      "0번과 1번 샘플의 어텐션 마스크가 다른가요? True\n",
      "\n",
      "0번 샘플 어텐션 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1번 샘플 어텐션 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "배치 내 최대 길이: 227\n",
      "0번 샘플 어텐션 마스크 합계 (실제 토큰 수): 227\n",
      "1번 샘플 어텐션 마스크 합계 (실제 토큰 수): 176\n",
      "0번 샘플 어텐션 마스크 1의 개수: 227\n",
      "0번 샘플 어텐션 마스크 0의 개수: 0\n",
      "1번 샘플 어텐션 마스크 1의 개수: 176\n",
      "1번 샘플 어텐션 마스크 0의 개수: 51\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 최대 길이 제한\n",
    "max_seq_length = 8192\n",
    "# 0번과 1번 데이터의 길이 확인\n",
    "example0 = train_dataset[0]\n",
    "example1 = train_dataset[1]\n",
    "# 개별 길이 확인 (토큰화 후)\n",
    "tokenized0 = tokenizer(\n",
    "    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n",
    "    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\" for msg in example0[\"messages\"]]),\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    "    padding=False,\n",
    "    return_tensors=None,\n",
    ")\n",
    "tokenized1 = tokenizer(\n",
    "    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n",
    "    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\" for msg in example1[\"messages\"]]),\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    "    padding=False,\n",
    "    return_tensors=None,\n",
    ")\n",
    "print(f\"0번 데이터 길이: {len(tokenized0['input_ids'])}\")\n",
    "print(f\"1번 데이터 길이: {len(tokenized1['input_ids'])}\")\n",
    "# 배치로 처리하여 어텐션 마스크 비교\n",
    "batch = collate_fn([example0, example1])\n",
    "print(\"\\n배치 처리 후:\")\n",
    "print(f\"입력 ID 형태: {batch['input_ids'].shape}\")\n",
    "print(f\"어텐션 마스크 형태: {batch['attention_mask'].shape}\")\n",
    "# 각 샘플의 어텐션 마스크 합계 (실제 토큰 수 확인)\n",
    "print(f\"0번 샘플 어텐션 마스크 합계: {batch['attention_mask'][0].sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 합계: {batch['attention_mask'][1].sum().item()}\")\n",
    "# 0번 샘플과 1번 샘플의 어텐션 마스크가 다른지 확인\n",
    "masks_different = not torch.equal(batch['attention_mask'][0], batch['attention_mask'][1])\n",
    "print(f\"\\n0번과 1번 샘플의 어텐션 마스크가 다른가요? {masks_different}\")\n",
    "# 어텐션 마스크 패턴 시각화 (처음 20개와 마지막 20개 토큰)\n",
    "print(\"\\n0번 샘플 어텐션 마스크:\", batch['attention_mask'][0].tolist())\n",
    "print(\"1번 샘플 어텐션 마스크:\", batch['attention_mask'][1].tolist())\n",
    "# 배치 내에서 가장 긴 시퀀스 길이 구하기\n",
    "max_length_in_batch = max(len(tokenized0['input_ids']), len(tokenized1['input_ids']))\n",
    "print(f\"\\n배치 내 최대 길이: {max_length_in_batch}\")\n",
    "print(f\"0번 샘플 어텐션 마스크 합계 (실제 토큰 수): {batch['attention_mask'][0].sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 합계 (실제 토큰 수): {batch['attention_mask'][1].sum().item()}\")\n",
    "print(f\"0번 샘플 어텐션 마스크 1의 개수: {batch['attention_mask'][0].sum().item()}\")\n",
    "print(f\"0번 샘플 어텐션 마스크 0의 개수: {(batch['attention_mask'][0] == 0).sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 1의 개수: {batch['attention_mask'][1].sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 0의 개수: {(batch['attention_mask'][1] == 0).sum().item()}\")\n",
    "# 결과 검증: 긴 샘플은 모든 어텐션 마스크가 1이고, 짧은 샘플은 일부만 1이어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02034c-61ba-4fe6-84dd-408fd27d6fed",
   "metadata": {},
   "source": [
    "## 6. 전처리 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d7d24-f600-4de6-82a5-4ae4469fe260",
   "metadata": {},
   "source": [
    "**input_ids와 labels는 어떻게 생성되는가?**\n",
    "\n",
    "LLM 학습에서 `input_ids`와 `labels`는 모델의 학습 목표에 따라 생성됩니다. 시스템 프롬프트까지 포함하여 설명하겠습니다.\n",
    "\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다:\n",
    "- 시스템 프롬프트: `당신은 친절하고 도움이 되는 AI 어시스턴트입니다.`\n",
    "- 사용자 메시지: `안녕하세요, 오늘 날씨는 어떤가요?`\n",
    "- 어시스턴트 응답: `안녕하세요! 오늘 날씨는 맑고 화창합니다.`\n",
    "\n",
    "LLaMA 3에서는 다음과 같은 템플릿 구조를 사용합니다(줄바꿈 포함):\n",
    "\n",
    "```python\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 친절하고 도움이 되는 AI 어시스턴트입니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "안녕하세요, 오늘 날씨는 어떤가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "안녕하세요! 오늘 날씨는 맑고 화창합니다.<|eot_id|>\n",
    "```\n",
    "\n",
    "이 전체 텍스트는 토크나이저에 의해 정수 시퀀스로 변환해봅시다.  \n",
    "(실제와 다르고 가정하여 정수를 맵핑하겠습니다.)\n",
    "\n",
    "먼저 모든 특수 토큰들은 아래의 고유 ID를 가진다고 가정해봅시다.  \n",
    "- <|begin_of_text|> = 토큰 ID 1\n",
    "- <|start_header_id|> = 토큰 ID 2\n",
    "- <|end_header_id|> = 토큰 ID 4\n",
    "- 줄바꿈 = 토큰 ID 5\n",
    "- <|eot_id|> = 토큰 ID 10\n",
    "\n",
    "역할 토큰들은 아래의 고유 ID를 가진다고 가정해봅시다.  \n",
    "- system = 토큰 ID 3\n",
    "- user = 토큰 ID 11\n",
    "- assistant = 토큰 ID 18\n",
    "\n",
    "전체 통합된 input_ids는 다음과 같습니다:\n",
    "`input_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 11, 4, 5, 12, 13, 14, 15, 16, 17, 10, 2, 18, 4, 5, 19, 20, 21, 22, 23, 10]`\n",
    "\n",
    "각 부분을 분리하면:\n",
    "- 시스템 프롬프트 부분: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- 사용자 메시지 부분: [2, 11, 4, 5, 12, 13, 14, 15, 16, 17, 10]\n",
    "- 어시스턴트 응답 부분: [2, 18, 4, 5, 19, 20, 21, 22, 23, 10]\n",
    "\n",
    "모델이 예측해야 할 영역은 assistant의 응답 부분인 `안녕하세요! 오늘 날씨는 맑고 화창합니다.`에 해당하는 토큰들입니다. 따라서 `labels`는 다음과 같이 설정됩니다:\n",
    "\n",
    "`labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19, 20, 21, 22, 23, 10]`\n",
    "\n",
    "여기서 주목할 점:\n",
    "1. 시스템 프롬프트와 사용자 메시지에 해당하는 모든 토큰(줄바꿈 포함)은 `-100`으로 마스킹됩니다.\n",
    "2. 어시스턴트 헤더와 첫 줄바꿈 토큰도 `-100`으로 마스킹됩니다.\n",
    "3. 실제 어시스턴트 응답 내용(19-23)과 마지막 종료 태그(10)만 원래 토큰 ID를 유지합니다.\n",
    "\n",
    "이처럼 `labels`는 모델이 실제로 생성해야 할 출력 부분만을 포함하고, 나머지 부분은 `-100`으로 채워져 손실 계산에서 제외됩니다. 이를 통해 모델은 입력(시스템 프롬프트+사용자 질문)을 기반으로 적절한 응답을 생성하는 방법을 학습합니다.\n",
    "\n",
    "학습 과정에서는:\n",
    "1. 모델에 `input_ids` 전체를 입력으로 제공합니다.\n",
    "2. 모델은 각 위치에서 다음 토큰을 예측합니다.\n",
    "3. 손실 계산 시 `labels`가 `-100`이 아닌 위치에서만 오차를 계산합니다.\n",
    "4. 이를 통해 모델은 주어진 맥락(시스템 프롬프트와 사용자 질문)에 대해 적절한 응답을 생성하는 방법을 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc5ff1-ea33-4ef2-a1dc-a6efd97cbbe4",
   "metadata": {},
   "source": [
    "## 7. 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe99cff0-d149-4ed5-8e8d-b62642165aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c51a9-0a8e-4ac1-b5f9-7575b1005fb3",
   "metadata": {},
   "source": [
    "- **학습 데이터 크기**: 2500개\n",
    "\n",
    "모델 학습에 사용되는 전체 데이터 샘플의 수입니다. 이는 각 에포크마다 처리되는 전체 데이터 양을 의미합니다.\n",
    "\n",
    "- **에포크(epochs)**: 3회\n",
    "\n",
    "전체 데이터셋을 처음부터 끝까지 반복해서 학습하는 횟수입니다. 즉, 모든 학습 데이터를 3번 반복해서 모델이 학습한다는 의미입니다.\n",
    "\n",
    "- **배치 크기(batch size)**: 2\n",
    "\n",
    "한 번에 처리하는 데이터 샘플의 수입니다. 메모리 효율을 위해 전체 데이터를 작은 배치로 나누어 처리하며, 여기서는 2개씩 묶어서 처리합니다.\n",
    "\n",
    "- **누적 단계(accumulation steps)**: 2\n",
    "\n",
    "모델을 실제로 업데이트하기 전에 여러 배치의 정보를 모으는 수입니다. 여기서는 2개의 배치(총 4개의 샘플)를 처리한 후에야 실제 모델 업데이트가 일어납니다.\n",
    "\n",
    "- **에포크 1회당 업데이트 횟수**: 2500 ÷ (2 × 2) = 625회\n",
    "\n",
    "한 에포크에서 모델이 업데이트되는 횟수입니다. 전체 데이터 2500개를 유효 배치 크기 4(배치 크기 2 × 누적 단계 2)로 나누면 625번의 업데이트가 발생합니다.\n",
    "\n",
    "- **총 업데이트 계산 방법**: (데이터 크기 × 에포크) ÷ (배치 크기 × 누적 단계)\n",
    "\n",
    "학습 과정 전체에서 발생하는 모델 업데이트의 총 횟수를 계산하는 공식입니다. 전체 처리 샘플 수를 유효 배치 크기로 나눕니다.\n",
    "\n",
    "- **총 업데이트 계산 과정**: (2500 × 3) ÷ (2 × 2) = 7,500 ÷ 4 = 1,875\n",
    "\n",
    "3개의 에포크 동안 총 7,500개의 샘플이 처리되고, 유효 배치 크기인 4개의 샘플마다 한 번씩 모델이 업데이트되므로 총 1,875번의 모델 업데이트가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44eff24a-0f63-4d91-9c89-c15410fce0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 12:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.161200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.156800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.172900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.119400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a78dac-8258-41f6-9e1a-b049ca8f0ba9",
   "metadata": {},
   "source": [
    "## 8. 테스트 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e8254-77c8-4a54-9132-131c432e9505",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 `<|start_header_id|>assistant<|end_header_id|>\\n`가 부착되어서 넣는 것이 좋습니다. 그러면 모델이 조금 더 안정적으로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4e1e2b4-7181-416a-8014-1018144ce6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f56d478b-4e31-4369-9ba1-b3861a8cc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 text-to-sql을 수행해야 합니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "입력 텍스트: 텍사스와 캘리포니아 주에서 발급된 상업용 건물의 모든 건축 허가증 번호를 나열하세요.\n",
      "\n",
      "DDL statements:\n",
      "CREATE TABLE permit (id INT, state VARCHAR(20), type VARCHAR(20), permit_number INT); INSERT INTO permit (id, state, type, permit_number) VALUES (1, 'Texas', 'Commercial', 100), (2, 'Texas', 'Residential', 150), (3, 'California', 'Commercial', 80), (4, 'California', 'Residential', 200);\n",
      "\n",
      "위의 테이블 명세와 사용자의 입력 텍스트를 바탕으로 SQL 쿼리를 작성합니다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "157493fd-bab0-4d18-a2d0-e85eab42e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "쿼리 작성: SELECT permit_number FROM permit WHERE (state = 'Texas' OR state = 'California') AND type = 'Commercial';\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a825b-89c3-4288-9c97-2c3c7f01ad16",
   "metadata": {},
   "source": [
    "## 9. 파인튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c34fb6-6b3a-4d67-b9b7-4a28167f5181",
   "metadata": {},
   "source": [
    "`AutoPeftModelForCausalLM()`의 입력으로 LoRA Adapter가 저장된 체크포인트의 주소를 넣으면 LoRA Adapter가 기존의 LLM과 부착되어 로드됩니다. 이 과정은 LoRA Adapter의 가중치를 사전 학습된 언어 모델(LLM)에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n",
    "\n",
    "`peft_model_id` 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. `\"llama3-8b-text-to-sql/checkpoint-1875\"`는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서 해당 가중치를 불러옵니다.\n",
    "\n",
    "`fine_tuned_model`은 `AutoPeftModelForCausalLM.from_pretrained` 메서드를 통해 체크포인트를 로드하여 생성됩니다. 이 메서드는 LLM과 LoRA Adapter를 결합하고, 최적화된 설정으로 모델을 메모리에 로드합니다. `device_map=\"auto\"` 옵션은 모델을 자동으로 GPU에 배치합니다.\n",
    "\n",
    "`pipeline`은 Hugging Face의 고수준 유틸리티로, NLP 작업(예: 텍스트 생성, 번역, 요약 등)을 간단히 수행할 수 있게 해줍니다. 이 코드에서 사용된 `pipeline(\"text-generation\")`은 텍스트 생성 작업을 수행하기 위한 파이프라인 객체를 생성합니다. 파이프라인은 내부적으로 모델과 토크나이저를 관리하여, 입력 텍스트를 토큰화하고, 모델을 통해 생성된 결과를 다시 디코딩하여 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "\n",
    "이 코드는 미세 조정된 LLM을 로드한 뒤, 이를 이용해 텍스트 생성 작업을 간단히 수행할 수 있도록 준비하는 데 목적이 있습니다. `pipeline`을 통해 텍스트 생성 작업을 실행하면, 입력 텍스트에 기반하여 모델이 다음 토큰을 예측하고 이를 반복적으로 생성합니다. 이 과정은 사용자에게 자연스러운 텍스트를 출력하는 데 사용됩니다.데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "043328ae-7ad0-41c3-b6c6-aedab0b8ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dac6d4d-2012-4603-9a62-da33a253e29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125ed501d0964150ace7f00ad730e954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"llama3-8b-text-to-sql/checkpoint-1875\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1190ab71-0f39-44c3-9f89-55c0ab4b23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0e846a9-3b68-4ba3-846e-e8f54b4667c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cd6da67-6672-4dc4-af0e-88052b7144ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "쿼리 작성: SELECT AVG(age) FROM community_engagement;\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT AVG(age) FROM community_engagement WHERE language IS NOT NULL;\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "쿼리 작성: SELECT AVG(price) FROM skincare_sales WHERE sale_country = 'Canada' AND product_name LIKE '%organic%';\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT AVG(price) FROM skincare_sales WHERE sale_country = 'Canada' AND product_name LIKE '%organic%';\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "쿼리 작성: SELECT agency.name, COUNT(event.id) as total_events FROM agency INNER JOIN event ON agency.id = event.agency_id GROUP BY agency.name;\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT agency_id, COUNT(*) as total_events FROM event GROUP BY agency_id;\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "쿼리 작성: SELECT country, COUNT(*) as satellites_in_orbit FROM satellites_in_orbit WHERE last_update = '2022-07-01' GROUP BY country ORDER BY satellites_in_orbit DESC;\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT country, satellites FROM satellites_in_orbit WHERE last_update <= '2022-07-01' AND country IN ('China', 'India', 'Japan', 'South Korea', 'Australia', 'Indonesia', 'Malaysia', 'Thailand', 'Vietnam', 'Philippines', 'New Zealand') GROUP BY country ORDER BY satellites DESC;\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "쿼리 작성: INSERT INTO fan_demographics (group_name, location, age, ticket_price) VALUES ('Latin America', 'Latin America', 25, 50);\n",
      "    label:\n",
      "\n",
      "쿼리 작성: INSERT INTO fan_demographics (group_name, location, ticket_price, total_fans) VALUES ('Latin America', 'Latin America', 50, 200);\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84388175-c0da-432a-b4a0-5333ae1c7feb",
   "metadata": {},
   "source": [
    "## 10. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49751b0-4dc2-43a7-a771-314e012b65c1",
   "metadata": {},
   "source": [
    "이번에는 LoRA Adapter를 merge하지 않은 기본 모델로 테스트 데이터에 대해서 인퍼런스해보겠습니다.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29c4c78d-50d0-4aeb-911b-989d691a1101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bb3f4ec4c648a2b5ea73f994af034b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0adeb550-b6c2-4783-b174-344e695a3641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "입력 텍스트에서 \"언어를 사용하는 사람들의 평균 나이\"를 구하려면 다음과 같은 SQL 쿼리를 작성할 수 있습니다:\n",
      "\n",
      "```sql\n",
      "SELECT AVG(age) AS avg_age\n",
      "FROM community_engagement\n",
      "WHERE language = 'English';\n",
      "```\n",
      "\n",
      "이 쿼리는 `community_engagement` 테이블에서 `language` 열 값이 'English'인 행의 `age` 열 값을 평균으로 계산합니다.\n",
      "\n",
      "이 쿼리를 실행하면 다음과 같은 결과를 얻을 수 있습니다:\n",
      "\n",
      "```sql\n",
      "avg_age\n",
      "-------\n",
      " 45.000\n",
      "```\n",
      "\n",
      "이 결과는 테이블에 있는 영어를 사용하는 사람들의 평균 나이를 나타냅니다.\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT AVG(age) FROM community_engagement WHERE language IS NOT NULL;\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "입력 텍스트에서 제공된 정보를 바탕으로 SQL 쿼리를 작성하려면, 평균 가격을 구하려면 테이블에 저장된 모든 데이터를 사용해야 합니다. 현재 테이블에는 단 하나의 행이 있습니다. 따라서 평균을 구할 수 없습니다. 그러나 사용자가 추가 데이터를 제공하면 평균 가격을 계산할 수 있습니다.\n",
      "\n",
      "다음은 현재 데이터를 기반으로 작성한 SQL 쿼리입니다:\n",
      "\n",
      "```sql\n",
      "SELECT product_name, price, sale_country\n",
      "FROM skincare_sales;\n",
      "```\n",
      "\n",
      "이 쿼리는 `skincare_sales` 테이블의 모든 행을 반환합니다. 현재 테이블에는 하나의 행만 있으므로, 이 쿼리는 다음과 같은 결과를 제공합니다:\n",
      "\n",
      "| product_name       | price | sale_country |\n",
      "|-------------------|-------|-------------|\n",
      "| Organic Day Cream | 35.50 | Canada      |\n",
      "\n",
      "추가 데이터가 제공되면, 그 데이터를 사용하여 평균 가격을 계산할 수 있습니다. 예를 들어, 다음과 같은 쿼리를 사용하여 평균 가격을 계산할 수 있습니다:\n",
      "\n",
      "```sql\n",
      "SELECT AVG(price) AS avg_price\n",
      "FROM skincare_sales\n",
      "WHERE sale_country = 'Canada';\n",
      "```\n",
      "\n",
      "이 쿼리는 `sale_country`가 'Canada'인 행의 `price`를 평균으로 계산합니다. 현재 데이터는 하나의 행만 있으므로, 이 쿼리는 다음과 같은 결과를 제공합니다:\n",
      "\n",
      "| avg_price |\n",
      "|----------|\n",
      "|   35.50 |\n",
      "\n",
      "추가 데이터가 제공되면, 그 데이터를 사용하여 평균 가격을 계산할 수 있습니다.\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT AVG(price) FROM skincare_sales WHERE sale_country = 'Canada' AND product_name LIKE '%organic%';\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "입력 텍스트에서 \"각 정부 기관이 주최한 공공 행사의 총 개수는 얼마입니까?\"라는 질문을 해결하기 위해, 각 정부 기관이 주최한 공공 행사의 총 개수를 구하려면 다음과 같은 SQL 쿼리를 사용할 수 있습니다.\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(*) AS total_events\n",
      "FROM event;\n",
      "```\n",
      "\n",
      "이 쿼리는 `event` 테이블에서 행의 개수를 세어줍니다. 결과는 `total_events` 열에 각 정부 기관이 주최한 공공 행사의 총 개수가 출력됩니다.\n",
      "\n",
      "이 쿼리를 실행하면 다음과 같은 결과를 얻을 수 있습니다:\n",
      "\n",
      "| total_events |\n",
      "| ----------- |\n",
      "| 4           |\n",
      "\n",
      "이 결과는 `event` 테이블에 저장된 행의 총 개수인 4를 나타냅니다. 각 정부 기관이 주최한 공공 행사의 총 개수는 4개입니다.\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT agency_id, COUNT(*) as total_events FROM event GROUP BY agency_id;\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "아시아 태평양 지역 각 국가별로 2022년 7월 1일 기준으로 궤도에 있는 위성의 수를 위성의 수가 많은 순으로 나열하려면 다음과 같은 SQL 쿼리를 사용할 수 있습니다. 먼저, 필요한 데이터를 입력한 후에 쿼리를 실행하면 됩니다.\n",
      "\n",
      "1. 데이터 입력:\n",
      "```sql\n",
      "INSERT INTO satellites_in_orbit (id, country, satellites, last_update)\n",
      "VALUES\n",
      "(1, '일본', 1500, '2022-07-01'),\n",
      "(2, '중국', 1200, '2022-07-01'),\n",
      "(3, '인도', 700, '2022-07-01'),\n",
      "(4, '대한민국', 300, '2022-07-01'),\n",
      "(5, '호주', 200, '2022-07-01'),\n",
      "(6, '러시아', 150, '2022-07-01'),\n",
      "(7, '싱가포르', 50, '2022-07-01'),\n",
      "(8, '태국', 30, '2022-07-01');\n",
      "```\n",
      "\n",
      "2. 위성의 수가 많은 순으로 나열하기:\n",
      "```sql\n",
      "SELECT country, satellites\n",
      "FROM satellites_in_orbit\n",
      "ORDER BY satellites DESC;\n",
      "```\n",
      "\n",
      "위의 쿼리를 실행하면 아시아 태평양 지역 각 국가별로 2022년 7월 1일 기준으로 궤도에 있는 위성의 수를 위성의 수가 많은 순으로 나열할 수 있습니다.\n",
      "    label:\n",
      "\n",
      "쿼리 작성: SELECT country, satellites FROM satellites_in_orbit WHERE last_update <= '2022-07-01' AND country IN ('China', 'India', 'Japan', 'South Korea', 'Australia', 'Indonesia', 'Malaysia', 'Thailand', 'Vietnam', 'Philippines', 'New Zealand') GROUP BY country ORDER BY satellites DESC;\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "다음은 입력 텍스트를 기반으로 `fan_demographics` 테이블에 새로운 팬 인구 통계 그룹을 추가하는 SQL 쿼리입니다:\n",
      "\n",
      "```sql\n",
      "INSERT INTO fan_demographics (group_name, location, age, ticket_price)\n",
      "VALUES ('Latin America', 'Latin America', 200, 50.00);\n",
      "```\n",
      "\n",
      "이 쿼리는 `fan_demographics` 테이블에 새로운 행을 삽입하여 `group_name`이 'Latin America'이고 `location`이 'Latin America'이며, `age`가 200명이고 `ticket_price`가 $50인 팬 인구 통계 그룹을 추가합니다.\n",
      "    label:\n",
      "\n",
      "쿼리 작성: INSERT INTO fan_demographics (group_name, location, ticket_price, total_fans) VALUES ('Latin America', 'Latin America', 50, 200);\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
