{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe8f548-b6e0-43ae-a7ee-116b706a010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed93795-8cac-4cc6-9352-d2d207f21a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 수: 4599\n",
      "학습 데이터 수: 3219 (70.0%)\n",
      "테스트 데이터 수: 1380 (30.0%)\n"
     ]
    }
   ],
   "source": [
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "dataset = load_dataset(\"iamjoon/esg-survey-datasets\", split=\"train\")\n",
    "\n",
    "# 2. OpenAI format으로 데이터 변환을 위한 함수 \n",
    "def format_data(sample):\n",
    "    # OpenAI format으로 변환\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sample['system_prompt'],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"user_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"assistant\"]\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 3. 전체 데이터에 OpenAI 포맷 전처리 적용\n",
    "formatted_dataset = dataset.map(format_data)\n",
    "\n",
    "# 4. 데이터를 7:2 비율로 학습/테스트 분할 (HuggingFace 내장 메서드 사용)\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "\n",
    "# 5. 결과 확인\n",
    "print(f\"전체 데이터 수: {len(formatted_dataset)}\")\n",
    "print(f\"학습 데이터 수: {len(train_dataset)} ({len(train_dataset)/len(formatted_dataset)*100:.1f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_dataset)} ({len(test_dataset)/len(formatted_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8790587-b2cc-489f-b8d6-c40cc3094d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860279bd2ad341a08b76a0b3b7f41887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1380 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 저장\n",
    "test_dataset.save_to_disk(\"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2fbc9a9-d6d7-4b90-9ff4-a89eb986cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017d3e3c-2b40-425e-a35e-b1be9076056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7aac056-6f7b-4a70-aa49-6793f38e22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 08:16:52 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 08-07 08:17:00 [config.py:585] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-07 08:17:00 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-07 08:17:02 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='iamjoon/llama3-8b-esg-survey-model', speculative_config=None, tokenizer='iamjoon/llama3-8b-esg-survey-model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=iamjoon/llama3-8b-esg-survey-model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 08-07 08:17:02 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d92a17e5590>\n",
      "INFO 08-07 08:17:03 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-07 08:17:03 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-07 08:17:03 [gpu_model_runner.py:1174] Starting to load model iamjoon/llama3-8b-esg-survey-model...\n",
      "WARNING 08-07 08:17:03 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-07 08:17:03 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b7d5f6e9344650a703e0d27552c9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-07 08:17:06 [loader.py:447] Loading weights took 2.89 seconds\n",
      "INFO 08-07 08:17:07 [gpu_model_runner.py:1186] Model loading took 14.9596 GB and 3.493485 seconds\n",
      "INFO 08-07 08:17:13 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/5fbafbff57/rank_0_0 for vLLM's torch.compile\n",
      "INFO 08-07 08:17:13 [backends.py:425] Dynamo bytecode transform time: 6.84 s\n",
      "INFO 08-07 08:17:14 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "INFO 08-07 08:17:19 [monitor.py:33] torch.compile takes 6.84 s in total\n",
      "INFO 08-07 08:17:20 [kv_cache_utils.py:566] GPU KV cache size: 418,768 tokens\n",
      "INFO 08-07 08:17:20 [kv_cache_utils.py:569] Maximum concurrency for 8,192 tokens per request: 51.12x\n",
      "INFO 08-07 08:17:39 [gpu_model_runner.py:1534] Graph capturing finished in 20 secs, took 0.51 GiB\n",
      "INFO 08-07 08:17:39 [core.py:151] init engine (profile, create kv cache, warmup model) took 32.96 seconds\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"iamjoon/llama3-8b-esg-survey-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48669319-4cb8-48f6-aa41-34d7050ca097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec352455-f38d-42f9-8046-a86c2eb32644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 1380개 중 216개 샘플 유지\n",
      "1164개 샘플 배제됨\n",
      "216\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "# 긴 프롬프트 샘플 배제\n",
    "def filter_long_prompts(prompt_lst, label_lst, max_length=8000):\n",
    "    \"\"\"\n",
    "    최대 길이를 초과하는 프롬프트 샘플들을 배제하는 함수\n",
    "    \"\"\"\n",
    "    filtered_prompts = []\n",
    "    filtered_labels = []\n",
    "    filtered_indices = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompt_lst):\n",
    "        if len(prompt) <= max_length:\n",
    "            filtered_prompts.append(prompt)\n",
    "            filtered_labels.append(label_lst[i])\n",
    "            filtered_indices.append(i)\n",
    "    \n",
    "    print(f\"전체 {len(prompt_lst)}개 중 {len(filtered_prompts)}개 샘플 유지\")\n",
    "    print(f\"{len(prompt_lst) - len(filtered_prompts)}개 샘플 배제됨\")\n",
    "    \n",
    "    return filtered_prompts, filtered_labels, filtered_indices\n",
    "\n",
    "# 사용법\n",
    "filtered_prompt_lst, filtered_label_lst, valid_indices = filter_long_prompts(prompt_lst, label_lst, max_length=12645)\n",
    "print(len(filtered_prompt_lst))\n",
    "print(len(filtered_label_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7088fee6-7044-4daa-bb6b-b8989d663152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 216/216 [01:31<00:00,  2.35it/s, est. speed input: 16554.54 toks/s, output: 20.22 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# 샘플링 파라미터 설정\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"\\n<|end_of_text|>\", \"<|eot_id|>\"],  # 문자열로 중지 토큰 지정\n",
    ")\n",
    "\n",
    "# 생성\n",
    "outputs = llm.generate(filtered_prompt_lst, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4643b86-f7f4-46a5-beae-079c54f51a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs를 문자열 리스트로 파싱\n",
    "preds = [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c269c824-7ffd-46ec-9dc3-39ec9ed5f079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['답변: 1) 예', '답변: 4) 많다', '답변: 알 수 없음', '답변: 알 수 없음', '답변: 알 수 없음']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0800ad-8c2c-4e55-9156-860ff5203547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n답변: 1) 예', '\\n답변: 3) 보통이다', '\\n답변: 알 수 없음', '\\n답변: 알 수 없음', '\\n답변: 알 수 없음']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_label_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36bd70e-ff00-44ae-a953-20a999efa94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00932e07-dc88-4df3-8f7e-45f25e81d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def calculate_accuracy(labels_list, predictions_list):\n",
    "    \"\"\"\n",
    "    텍스트에서 '답변:' 다음 부분을 추출하여 일치도를 계산하는 함수\n",
    "    \"\"\"\n",
    "    def extract_answer(text):\n",
    "        match = re.search(r'답변:\\s*(.+)', text.strip())\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return \"\"\n",
    "    \n",
    "    if len(labels_list) != len(predictions_list):\n",
    "        raise ValueError(\"라벨과 예측 결과의 개수가 일치하지 않습니다.\")\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(labels_list)\n",
    "    \n",
    "    for label_text, pred_text in zip(labels_list, predictions_list):\n",
    "        label_answer = extract_answer(label_text)\n",
    "        pred_answer = extract_answer(pred_text)\n",
    "        \n",
    "        if label_answer == pred_answer:\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c447722-0993-416e-bb16-a5aaafa6ba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8240740740740741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint-800\n",
    "calculate_accuracy(filtered_label_lst, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
