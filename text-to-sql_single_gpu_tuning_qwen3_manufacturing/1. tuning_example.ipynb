{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7956796",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc338ac8-931c-4b20-9057-e987e8227b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.8.0\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.8.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.13.1.3)\n",
      "Collecting triton==3.4.0 (from torch==2.8.0)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m177.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m173.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m138.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m172.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m173.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.0.0\n",
      "\u001b[2K    Uninstalling triton-3.0.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.0.0\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.1.1053\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.1.105:━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.1.105/13\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.20.5/13\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.20.5:━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.20.5 0/13\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.1.0.1060m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.1.0.106:━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.2.106\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.2.106:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.2.106━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.0.2.54━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.0.2.54:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105 \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.1050m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.1050m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu120m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.1.1050m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.1.3.1━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.1.3.1:0m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.4.5.1070m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.4.5.107:m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.1.0.70━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.7090m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.4.01m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.4.0:━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.4.0━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [torch]m12/13\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.3 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision==0.23.0 in /usr/local/lib/python3.12/dist-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.1.2)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.8.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchvision==0.23.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchvision==0.23.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchvision==0.23.0) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchaudio==2.8.0 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio==2.8.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->torchaudio==2.8.0) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.55.2\n",
      "  Downloading transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.55.2)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.55.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.55.2) (2025.10.5)\n",
      "Downloading transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.20.3\n",
      "\u001b[2K    Uninstalling tokenizers-0.20.3:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.20.3\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.45.1\n",
      "\u001b[2K    Uninstalling transformers-4.45.1:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.45.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.21.4 transformers-4.55.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tokenizers==0.21.4 in /usr/local/lib/python3.12/dist-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers==0.21.4) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers==0.21.4) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting safetensors==0.6.2\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "Successfully installed safetensors-0.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting huggingface-hub==0.34.4\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.34.4) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.34.4) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.34.4) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.34.4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.34.4) (2025.10.5)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "Successfully installed huggingface-hub-0.34.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets==4.0.0\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.0.0) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==4.0.0) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.0.0) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.0.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.0.0) (1.16.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.0.1\n",
      "    Uninstalling datasets-3.0.1:\n",
      "      Successfully uninstalled datasets-3.0.1\n",
      "Successfully installed datasets-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting accelerate==1.10.0\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.10.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.10.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.10.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.10.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.10.0) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.10.0) (2025.10.5)\n",
      "Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.34.2\n",
      "    Uninstalling accelerate-0.34.2:\n",
      "      Successfully uninstalled accelerate-0.34.2\n",
      "Successfully installed accelerate-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting trl==0.21.0\n",
      "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.21.0) (1.10.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.21.0) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.21.0) (4.55.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (0.34.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.21.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.21.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl==0.21.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl==0.21.0) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl==0.21.0) (2025.10.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl==0.21.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.55.0->trl==0.21.0) (0.21.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl==0.21.0) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl==0.21.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl==0.21.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl==0.21.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.21.0) (1.16.0)\n",
      "Downloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
      "Installing collected packages: trl\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.11.1\n",
      "    Uninstalling trl-0.11.1:\n",
      "      Successfully uninstalled trl-0.11.1\n",
      "Successfully installed trl-0.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting peft==0.17.0\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (2.8.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (4.55.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (1.10.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.17.0) (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft==0.17.0) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.17.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.17.0) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.17.0) (2025.10.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.17.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers->peft==0.17.0) (0.21.4)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.13.0\n",
      "    Uninstalling peft-0.13.0:\n",
      "      Successfully uninstalled peft-0.13.0\n",
      "Successfully installed peft-0.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.8.0\"\n",
    "%pip install \"torchvision==0.23.0\" \n",
    "%pip install \"torchaudio==2.8.0\"\n",
    "%pip install \"transformers==4.55.2\"\n",
    "%pip install \"tokenizers==0.21.4\"\n",
    "%pip install \"safetensors==0.6.2\"\n",
    "%pip install \"huggingface-hub==0.34.4\"\n",
    "%pip install \"datasets==4.0.0\"\n",
    "%pip install \"accelerate==1.10.0\"\n",
    "%pip install \"trl==0.21.0\"\n",
    "%pip install \"peft==0.17.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a347fd7-0a13-426f-8125-59cdfc3eac62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab0492f-1159-4562-bc34-9c0967b950a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec3494",
   "metadata": {},
   "source": [
    "빠른 학습을 위해 학습 데이터와 테스트 데이터를 2:8 비율로 분할합니다. 이 값을 변경하고자 하는 분은 test_ratio의 값을 변경하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5183067f-0a44-4c1d-b5a0-b574c1238771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터 개수: 776\n",
      "Train: 621개\n",
      "Test:  155개\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/manufacturing-text-to-sql\", split=\"train\")\n",
    "\n",
    "print(f\"원본 데이터 개수: {len(dataset)}\")\n",
    "\n",
    "# 2. train/test split\n",
    "test_ratio = 0.2\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "test_size = int(len(indices) * test_ratio)\n",
    "\n",
    "test_indices = indices[:test_size]\n",
    "train_indices = indices[test_size:]\n",
    "\n",
    "# 3. OpenAI format 변환 함수\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n",
    "            {\"role\": \"user\", \"content\": sample[\"user_prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"assistant\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 4. 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_indices]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_indices]\n",
    "\n",
    "# 5. 결과 출력\n",
    "print(f\"Train: {len(train_dataset)}개\")\n",
    "print(f\"Test:  {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e17067-01c0-45e9-9aa0-b352e82a4901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"\\n당신은 SQL을 생성하는 AI 모델입니다.\\n아래는 데이터베이스 스키마(DDL)입니다.\\n\\n<SCHEMA>\\n-- 작업완료로그 테이블\\nCREATE TABLE IF NOT EXISTS `log_lottranslog` (\\n  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\\n  `LOTNO` char(20) NOT NULL DEFAULT '' COMMENT 'LOT번호',\\n  `LINENO` char(20) DEFAULT NULL COMMENT '생산라인번호',\\n  `TRANSACTIONNAME` char(50) NOT NULL DEFAULT '' COMMENT '처리명',\\n  `TIMELOGGED` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '로그입력일시',\\n  `ACTUALTIME` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '실제실행일시',\\n  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\\n  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\\n  `TRANSQTY` double NOT NULL DEFAULT 0 COMMENT '변경수량',\\n  `CURRENTQTY` double NOT NULL DEFAULT 0 COMMENT '현재수량',\\n  `NEXTQTY` double DEFAULT NULL COMMENT '변경반영된수량',\\n  `TRANSUOM` char(5) DEFAULT NULL COMMENT '측정단위',\\n  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\\n  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\\n  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\\n  `USERCODE` char(20) NOT NULL DEFAULT '' COMMENT '사용자코드',\\n  PRIMARY KEY (`TRANSLOGID`) USING BTREE\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci COMMENT='LOT 변경 로그';\\n\\n-- VIEW 종속성 오류를 극복하기 위해 임시 테이블을 생성합니다.\\nCREATE TABLE `vw_lot` (\\n    `lotno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT 'LOT번호',\\n    `lineno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT '생산라인번호',\\n    `materialcode` CHAR(30) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\\n    `materialname` CHAR(50) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재명',\\n    `customercode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '고객코드',\\n    `comcode2` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `createdate` DATETIME(3) NOT NULL COMMENT '생성일시',\\n    `duedate` DATETIME(3) NULL COMMENT '마감일시',\\n    `createqty` DOUBLE NOT NULL COMMENT '생성수량',\\n    `standarduom` CHAR(5) NULL COLLATE 'utf8_general_ci' COMMENT '표준측정단위',\\n    `ispr` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `code1` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `code2` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `order_id` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '주문ID',\\n    `confirmqty` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '수량확인여부',\\n    `lotindex` SMALLINT(6) NOT NULL COMMENT 'LOT인덱스',\\n    `warehousecode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '창고코드',\\n    `bopmaterialcode` CHAR(30) NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\\n    `processcode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '공정코드',\\n    `workstate` CHAR(10) NOT NULL COLLATE 'utf8_general_ci' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\\n    `queuedqty` DOUBLE NULL COMMENT '대기수량',\\n    `startedqty` DOUBLE NULL COMMENT '시작수량',\\n    `queuedtime` DATETIME(3) NULL COMMENT '대기일시',\\n    `startedtime` DATETIME(3) NULL COMMENT '시작일시',\\n    `consumptiondone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '소모완료여부',\\n    `datacollectdone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '데이터수집완료여부',\\n    `completedone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '완료여부'\\n) ENGINE=MyISAM;\\n\\n-- 자재정보 테이블\\nCREATE TABLE IF NOT EXISTS `udt_material` (\\n  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\\n  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\\n  `MATERIALTYPECODE` char(20) NOT NULL DEFAULT '' COMMENT '자재구분코드',\\n  `LOTQTY` double DEFAULT NULL COMMENT 'LOT수량',\\n  `STANDARDUOM` char(5) DEFAULT NULL COMMENT '표준측정단위',\\n  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\\n  `REMARK` char(50) DEFAULT NULL COMMENT '비고',\\n  PRIMARY KEY (`MATERIALCODE`) USING BTREE,\\n  KEY `fk_udt_material_materialtype` (`MATERIALTYPECODE`) USING BTREE,\\n  CONSTRAINT `fk_udt_material_materialtype` FOREIGN KEY (`MATERIALTYPECODE`) REFERENCES `udt_materialtype` (`MATERIALTYPECODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- LOT 변경 로그 테이블\\nCREATE TABLE IF NOT EXISTS `log_completelog` (\\n  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '트랜잭션아이디',\\n  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\\n  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\\n  `NEXTBOPMATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '다음공정자재코드',\\n  `NEXTPROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '다음공정코드',\\n  `MACHINECODE` char(20) DEFAULT NULL COMMENT '',\\n  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\\n  CONSTRAINT `FK_log_completelog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- 공정완료로그 테이블\\nCREATE TABLE IF NOT EXISTS `log_finishlog` (\\n  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\\n  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\\n  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\\n  `NEXTWAREHOUSECODE` char(20) NOT NULL DEFAULT '' COMMENT '이동된창고코드',\\n  `MACHINECODE` char(20) DEFAULT NULL COMMENT '장비코드',\\n  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\\n  CONSTRAINT `FK_log_finishlog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- LOT 공정 진행상황 정보 테이블\\nCREATE TABLE IF NOT EXISTS `sgt_wiplot` (\\n  `LOTNO` char(20) NOT NULL COMMENT 'LOT번호',\\n  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\\n  `LOTINDEX` smallint(6) NOT NULL DEFAULT 0 COMMENT 'LOT인덱스',\\n  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\\n  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\\n  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\\n  `WORKSTATE` char(10) NOT NULL DEFAULT '' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\\n  `QUEUEDQTY` double DEFAULT NULL COMMENT '입력수량',\\n  `STARTEDQTY` double DEFAULT NULL COMMENT '작업수량',\\n  `QUEUEDTIME` datetime(3) DEFAULT NULL COMMENT '대기일시',\\n  `STARTEDTIME` datetime(3) DEFAULT NULL COMMENT '시작일시',\\n  `CONSUMPTIONDONE` char(1) NOT NULL DEFAULT '' COMMENT '소모완료여부',\\n  `DATACOLLECTDONE` char(1) NOT NULL DEFAULT '' COMMENT '데이터수집완료여부',\\n  `COMPLETEDONE` char(1) NOT NULL DEFAULT '' COMMENT '완료여부',\\n  PRIMARY KEY (`LOTINDEX`,`LOTNO`,`LINENO`) USING BTREE,\\n  KEY `fk_sgt_sublot_lotid` (`LOTNO`,`LINENO`) USING BTREE,\\n  KEY `FK_sgt_wiplot_udt_warehouse` (`WAREHOUSECODE`) USING BTREE,\\n  CONSTRAINT `FK_sgt_wiplot_sgt_lot` FOREIGN KEY (`LOTNO`) REFERENCES `sgt_lot` (`LOTNO`),\\n  CONSTRAINT `FK_sgt_wiplot_udt_warehouse` FOREIGN KEY (`WAREHOUSECODE`) REFERENCES `udt_warehouse` (`WAREHOUSECODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- 공장생산라인정보 테이블\\nCREATE TABLE IF NOT EXISTS `udt_factoryline` (\\n  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\\n  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\\n  `FACTORYCODE` char(20) NOT NULL COMMENT '공장코드',\\n  `MESYN` int(11) NOT NULL DEFAULT 0 COMMENT '자동화여부',\\n  `AUTOINPUTMATYN` int(11) DEFAULT NULL COMMENT '자재자동공급여부',\\n  PRIMARY KEY (`LINENO`) USING BTREE,\\n  KEY `R_98` (`FACTORYCODE`) USING BTREE,\\n  CONSTRAINT `R_98` FOREIGN KEY (`FACTORYCODE`) REFERENCES `udt_factory` (`FACTORYCODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- 공정정보 테이블\\nCREATE TABLE IF NOT EXISTS `udt_process` (\\n  `PROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '공정코드',\\n  `PROCESSNAME` char(50) NOT NULL DEFAULT '' COMMENT '공정명',\\n  `COMPLETEONLY` char(1) NOT NULL DEFAULT '' COMMENT '',\\n  `LOTFLOW` char(1) NOT NULL DEFAULT '' COMMENT '',\\n  `WORKSTATIONCODE` char(20) DEFAULT NULL COMMENT '워크스테이션코드',\\n  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\\n  PRIMARY KEY (`PROCESSCODE`) USING BTREE,\\n  KEY `fk_udt_process_workstation` (`WORKSTATIONCODE`) USING BTREE,\\n  CONSTRAINT `fk_udt_process_workstation` FOREIGN KEY (`WORKSTATIONCODE`) REFERENCES `udt_workstation` (`WORKSTATIONCODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n</SCHEMA>\\n\\n규칙:\\n1. 반드시 스키마에 존재하는 테이블과 컬럼만 사용하십시오.\\n2. 답변은 SQL 쿼리만 출력하고 설명은 절대 포함하지 마십시오.\\n3. WHERE / GROUP BY / ORDER BY는 스키마 기반으로 논리적으로 작성하십시오.\\n4. 쿼리는 항상 실행 가능한 SQL이어야 합니다.\\n\"},\n",
       " {'role': 'user', 'content': '작년 포르쉐라인 총 생산량 얼마였는지'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"SELECT SUM(TRANSQTY) AS TOTAL_PRODUCTION FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' AND YEAR(ACTUALTIME) = YEAR(NOW()) - 1;\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e561d8f-7a5c-4d50-a030-a31ecbe7853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205c38e8-e863-446d-b1f6-e9c587138276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': \"\\n당신은 SQL을 생성하는 AI 모델입니다.\\n아래는 데이터베이스 스키마(DDL)입니다.\\n\\n<SCHEMA>\\n-- 작업완료로그 테이블\\nCREATE TABLE IF NOT EXISTS `log_lottranslog` (\\n  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\\n  `LOTNO` char(20) NOT NULL DEFAULT '' COMMENT 'LOT번호',\\n  `LINENO` char(20) DEFAULT NULL COMMENT '생산라인번호',\\n  `TRANSACTIONNAME` char(50) NOT NULL DEFAULT '' COMMENT '처리명',\\n  `TIMELOGGED` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '로그입력일시',\\n  `ACTUALTIME` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '실제실행일시',\\n  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\\n  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\\n  `TRANSQTY` double NOT NULL DEFAULT 0 COMMENT '변경수량',\\n  `CURRENTQTY` double NOT NULL DEFAULT 0 COMMENT '현재수량',\\n  `NEXTQTY` double DEFAULT NULL COMMENT '변경반영된수량',\\n  `TRANSUOM` char(5) DEFAULT NULL COMMENT '측정단위',\\n  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\\n  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\\n  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\\n  `USERCODE` char(20) NOT NULL DEFAULT '' COMMENT '사용자코드',\\n  PRIMARY KEY (`TRANSLOGID`) USING BTREE\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci COMMENT='LOT 변경 로그';\\n\\n-- VIEW 종속성 오류를 극복하기 위해 임시 테이블을 생성합니다.\\nCREATE TABLE `vw_lot` (\\n    `lotno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT 'LOT번호',\\n    `lineno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT '생산라인번호',\\n    `materialcode` CHAR(30) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\\n    `materialname` CHAR(50) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재명',\\n    `customercode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '고객코드',\\n    `comcode2` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `createdate` DATETIME(3) NOT NULL COMMENT '생성일시',\\n    `duedate` DATETIME(3) NULL COMMENT '마감일시',\\n    `createqty` DOUBLE NOT NULL COMMENT '생성수량',\\n    `standarduom` CHAR(5) NULL COLLATE 'utf8_general_ci' COMMENT '표준측정단위',\\n    `ispr` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `code1` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `code2` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\\n    `order_id` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '주문ID',\\n    `confirmqty` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '수량확인여부',\\n    `lotindex` SMALLINT(6) NOT NULL COMMENT 'LOT인덱스',\\n    `warehousecode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '창고코드',\\n    `bopmaterialcode` CHAR(30) NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\\n    `processcode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '공정코드',\\n    `workstate` CHAR(10) NOT NULL COLLATE 'utf8_general_ci' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\\n    `queuedqty` DOUBLE NULL COMMENT '대기수량',\\n    `startedqty` DOUBLE NULL COMMENT '시작수량',\\n    `queuedtime` DATETIME(3) NULL COMMENT '대기일시',\\n    `startedtime` DATETIME(3) NULL COMMENT '시작일시',\\n    `consumptiondone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '소모완료여부',\\n    `datacollectdone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '데이터수집완료여부',\\n    `completedone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '완료여부'\\n) ENGINE=MyISAM;\\n\\n-- 자재정보 테이블\\nCREATE TABLE IF NOT EXISTS `udt_material` (\\n  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\\n  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\\n  `MATERIALTYPECODE` char(20) NOT NULL DEFAULT '' COMMENT '자재구분코드',\\n  `LOTQTY` double DEFAULT NULL COMMENT 'LOT수량',\\n  `STANDARDUOM` char(5) DEFAULT NULL COMMENT '표준측정단위',\\n  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\\n  `REMARK` char(50) DEFAULT NULL COMMENT '비고',\\n  PRIMARY KEY (`MATERIALCODE`) USING BTREE,\\n  KEY `fk_udt_material_materialtype` (`MATERIALTYPECODE`) USING BTREE,\\n  CONSTRAINT `fk_udt_material_materialtype` FOREIGN KEY (`MATERIALTYPECODE`) REFERENCES `udt_materialtype` (`MATERIALTYPECODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- LOT 변경 로그 테이블\\nCREATE TABLE IF NOT EXISTS `log_completelog` (\\n  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '트랜잭션아이디',\\n  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\\n  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\\n  `NEXTBOPMATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '다음공정자재코드',\\n  `NEXTPROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '다음공정코드',\\n  `MACHINECODE` char(20) DEFAULT NULL COMMENT '',\\n  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\\n  CONSTRAINT `FK_log_completelog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- 공정완료로그 테이블\\nCREATE TABLE IF NOT EXISTS `log_finishlog` (\\n  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\\n  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\\n  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\\n  `NEXTWAREHOUSECODE` char(20) NOT NULL DEFAULT '' COMMENT '이동된창고코드',\\n  `MACHINECODE` char(20) DEFAULT NULL COMMENT '장비코드',\\n  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\\n  CONSTRAINT `FK_log_finishlog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- LOT 공정 진행상황 정보 테이블\\nCREATE TABLE IF NOT EXISTS `sgt_wiplot` (\\n  `LOTNO` char(20) NOT NULL COMMENT 'LOT번호',\\n  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\\n  `LOTINDEX` smallint(6) NOT NULL DEFAULT 0 COMMENT 'LOT인덱스',\\n  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\\n  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\\n  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\\n  `WORKSTATE` char(10) NOT NULL DEFAULT '' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\\n  `QUEUEDQTY` double DEFAULT NULL COMMENT '입력수량',\\n  `STARTEDQTY` double DEFAULT NULL COMMENT '작업수량',\\n  `QUEUEDTIME` datetime(3) DEFAULT NULL COMMENT '대기일시',\\n  `STARTEDTIME` datetime(3) DEFAULT NULL COMMENT '시작일시',\\n  `CONSUMPTIONDONE` char(1) NOT NULL DEFAULT '' COMMENT '소모완료여부',\\n  `DATACOLLECTDONE` char(1) NOT NULL DEFAULT '' COMMENT '데이터수집완료여부',\\n  `COMPLETEDONE` char(1) NOT NULL DEFAULT '' COMMENT '완료여부',\\n  PRIMARY KEY (`LOTINDEX`,`LOTNO`,`LINENO`) USING BTREE,\\n  KEY `fk_sgt_sublot_lotid` (`LOTNO`,`LINENO`) USING BTREE,\\n  KEY `FK_sgt_wiplot_udt_warehouse` (`WAREHOUSECODE`) USING BTREE,\\n  CONSTRAINT `FK_sgt_wiplot_sgt_lot` FOREIGN KEY (`LOTNO`) REFERENCES `sgt_lot` (`LOTNO`),\\n  CONSTRAINT `FK_sgt_wiplot_udt_warehouse` FOREIGN KEY (`WAREHOUSECODE`) REFERENCES `udt_warehouse` (`WAREHOUSECODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- 공장생산라인정보 테이블\\nCREATE TABLE IF NOT EXISTS `udt_factoryline` (\\n  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\\n  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\\n  `FACTORYCODE` char(20) NOT NULL COMMENT '공장코드',\\n  `MESYN` int(11) NOT NULL DEFAULT 0 COMMENT '자동화여부',\\n  `AUTOINPUTMATYN` int(11) DEFAULT NULL COMMENT '자재자동공급여부',\\n  PRIMARY KEY (`LINENO`) USING BTREE,\\n  KEY `R_98` (`FACTORYCODE`) USING BTREE,\\n  CONSTRAINT `R_98` FOREIGN KEY (`FACTORYCODE`) REFERENCES `udt_factory` (`FACTORYCODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n\\n-- 공정정보 테이블\\nCREATE TABLE IF NOT EXISTS `udt_process` (\\n  `PROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '공정코드',\\n  `PROCESSNAME` char(50) NOT NULL DEFAULT '' COMMENT '공정명',\\n  `COMPLETEONLY` char(1) NOT NULL DEFAULT '' COMMENT '',\\n  `LOTFLOW` char(1) NOT NULL DEFAULT '' COMMENT '',\\n  `WORKSTATIONCODE` char(20) DEFAULT NULL COMMENT '워크스테이션코드',\\n  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\\n  PRIMARY KEY (`PROCESSCODE`) USING BTREE,\\n  KEY `fk_udt_process_workstation` (`WORKSTATIONCODE`) USING BTREE,\\n  CONSTRAINT `fk_udt_process_workstation` FOREIGN KEY (`WORKSTATIONCODE`) REFERENCES `udt_workstation` (`WORKSTATIONCODE`)\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\\n</SCHEMA>\\n\\n규칙:\\n1. 반드시 스키마에 존재하는 테이블과 컬럼만 사용하십시오.\\n2. 답변은 SQL 쿼리만 출력하고 설명은 절대 포함하지 마십시오.\\n3. WHERE / GROUP BY / ORDER BY는 스키마 기반으로 논리적으로 작성하십시오.\\n4. 쿼리는 항상 실행 가능한 SQL이어야 합니다.\\n\",\n",
       "   'role': 'system'},\n",
       "  {'content': '올해 포르쉐라인 월평균 생산량 계산해줘', 'role': 'user'},\n",
       "  {'content': \"SELECT AVG(MONTHLY_PRODUCTION) AS AVG_MONTHLY_PRODUCTION FROM (SELECT DATE_FORMAT(ACTUALTIME, '%Y-%m') AS MONTH, SUM(TRANSQTY) AS MONTHLY_PRODUCTION FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' AND YEAR(ACTUALTIME) = YEAR(NOW()) GROUP BY DATE_FORMAT(ACTUALTIME, '%Y-%m')) AS MONTHLY_DATA;\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33536c0",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5f299a-e0ca-407f-ae21-0f0e5fadeac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e0dc68cd954f3dba803cb1ca81594a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0b4b4524b447d39bf18f5cf2d915e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ea9b1164c64634940646f2e091c7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f775921b18c14979847c635c721b3d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ddeeb0c923c4cea81a7d960980805b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93ebaf16944491a8e8d09843ea9d06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b66b107835e42ceba45e29650cb7bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f3ed9dce744625a8a80f025df737ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e1fc2264c045689958c00a495e15e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aacc9a9e3824c6ca644eba5da03067a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8df8462-72af-4ad4-b1ed-06a84dd6086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "\n",
      "당신은 SQL을 생성하는 AI 모델입니다.\n",
      "아래는 데이터베이스 스키마(DDL)입니다.\n",
      "\n",
      "<SCHEMA>\n",
      "-- 작업완료로그 테이블\n",
      "CREATE TABLE IF NOT EXISTS `log_lottranslog` (\n",
      "  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\n",
      "  `LOTNO` char(20) NOT NULL DEFAULT '' COMMENT 'LOT번호',\n",
      "  `LINENO` char(20) DEFAULT NULL COMMENT '생산라인번호',\n",
      "  `TRANSACTIONNAME` char(50) NOT NULL DEFAULT '' COMMENT '처리명',\n",
      "  `TIMELOGGED` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '로그입력일시',\n",
      "  `ACTUALTIME` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '실제실행일시',\n",
      "  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\n",
      "  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\n",
      "  `TRANSQTY` double NOT NULL DEFAULT 0 COMMENT '변경수량',\n",
      "  `CURRENTQTY` double NOT NULL DEFAULT 0 COMMENT '현재수량',\n",
      "  `NEXTQTY` double DEFAULT NULL COMMENT '변경반영된수량',\n",
      "  `TRANSUOM` char(5) DEFAULT NULL COMMENT '측정단위',\n",
      "  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\n",
      "  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\n",
      "  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\n",
      "  `USERCODE` char(20) NOT NULL DEFAULT '' COMMENT '사용자코드',\n",
      "  PRIMARY KEY (`TRANSLOGID`) USING BTREE\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci COMMENT='LOT 변경 로그';\n",
      "\n",
      "-- VIEW 종속성 오류를 극복하기 위해 임시 테이블을 생성합니다.\n",
      "CREATE TABLE `vw_lot` (\n",
      "    `lotno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT 'LOT번호',\n",
      "    `lineno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT '생산라인번호',\n",
      "    `materialcode` CHAR(30) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\n",
      "    `materialname` CHAR(50) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재명',\n",
      "    `customercode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '고객코드',\n",
      "    `comcode2` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `createdate` DATETIME(3) NOT NULL COMMENT '생성일시',\n",
      "    `duedate` DATETIME(3) NULL COMMENT '마감일시',\n",
      "    `createqty` DOUBLE NOT NULL COMMENT '생성수량',\n",
      "    `standarduom` CHAR(5) NULL COLLATE 'utf8_general_ci' COMMENT '표준측정단위',\n",
      "    `ispr` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `code1` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `code2` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `order_id` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '주문ID',\n",
      "    `confirmqty` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '수량확인여부',\n",
      "    `lotindex` SMALLINT(6) NOT NULL COMMENT 'LOT인덱스',\n",
      "    `warehousecode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '창고코드',\n",
      "    `bopmaterialcode` CHAR(30) NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\n",
      "    `processcode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '공정코드',\n",
      "    `workstate` CHAR(10) NOT NULL COLLATE 'utf8_general_ci' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\n",
      "    `queuedqty` DOUBLE NULL COMMENT '대기수량',\n",
      "    `startedqty` DOUBLE NULL COMMENT '시작수량',\n",
      "    `queuedtime` DATETIME(3) NULL COMMENT '대기일시',\n",
      "    `startedtime` DATETIME(3) NULL COMMENT '시작일시',\n",
      "    `consumptiondone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '소모완료여부',\n",
      "    `datacollectdone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '데이터수집완료여부',\n",
      "    `completedone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '완료여부'\n",
      ") ENGINE=MyISAM;\n",
      "\n",
      "-- 자재정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `udt_material` (\n",
      "  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\n",
      "  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\n",
      "  `MATERIALTYPECODE` char(20) NOT NULL DEFAULT '' COMMENT '자재구분코드',\n",
      "  `LOTQTY` double DEFAULT NULL COMMENT 'LOT수량',\n",
      "  `STANDARDUOM` char(5) DEFAULT NULL COMMENT '표준측정단위',\n",
      "  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\n",
      "  `REMARK` char(50) DEFAULT NULL COMMENT '비고',\n",
      "  PRIMARY KEY (`MATERIALCODE`) USING BTREE,\n",
      "  KEY `fk_udt_material_materialtype` (`MATERIALTYPECODE`) USING BTREE,\n",
      "  CONSTRAINT `fk_udt_material_materialtype` FOREIGN KEY (`MATERIALTYPECODE`) REFERENCES `udt_materialtype` (`MATERIALTYPECODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- LOT 변경 로그 테이블\n",
      "CREATE TABLE IF NOT EXISTS `log_completelog` (\n",
      "  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '트랜잭션아이디',\n",
      "  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\n",
      "  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\n",
      "  `NEXTBOPMATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '다음공정자재코드',\n",
      "  `NEXTPROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '다음공정코드',\n",
      "  `MACHINECODE` char(20) DEFAULT NULL COMMENT '',\n",
      "  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\n",
      "  CONSTRAINT `FK_log_completelog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- 공정완료로그 테이블\n",
      "CREATE TABLE IF NOT EXISTS `log_finishlog` (\n",
      "  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\n",
      "  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\n",
      "  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\n",
      "  `NEXTWAREHOUSECODE` char(20) NOT NULL DEFAULT '' COMMENT '이동된창고코드',\n",
      "  `MACHINECODE` char(20) DEFAULT NULL COMMENT '장비코드',\n",
      "  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\n",
      "  CONSTRAINT `FK_log_finishlog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- LOT 공정 진행상황 정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `sgt_wiplot` (\n",
      "  `LOTNO` char(20) NOT NULL COMMENT 'LOT번호',\n",
      "  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\n",
      "  `LOTINDEX` smallint(6) NOT NULL DEFAULT 0 COMMENT 'LOT인덱스',\n",
      "  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\n",
      "  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\n",
      "  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\n",
      "  `WORKSTATE` char(10) NOT NULL DEFAULT '' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\n",
      "  `QUEUEDQTY` double DEFAULT NULL COMMENT '입력수량',\n",
      "  `STARTEDQTY` double DEFAULT NULL COMMENT '작업수량',\n",
      "  `QUEUEDTIME` datetime(3) DEFAULT NULL COMMENT '대기일시',\n",
      "  `STARTEDTIME` datetime(3) DEFAULT NULL COMMENT '시작일시',\n",
      "  `CONSUMPTIONDONE` char(1) NOT NULL DEFAULT '' COMMENT '소모완료여부',\n",
      "  `DATACOLLECTDONE` char(1) NOT NULL DEFAULT '' COMMENT '데이터수집완료여부',\n",
      "  `COMPLETEDONE` char(1) NOT NULL DEFAULT '' COMMENT '완료여부',\n",
      "  PRIMARY KEY (`LOTINDEX`,`LOTNO`,`LINENO`) USING BTREE,\n",
      "  KEY `fk_sgt_sublot_lotid` (`LOTNO`,`LINENO`) USING BTREE,\n",
      "  KEY `FK_sgt_wiplot_udt_warehouse` (`WAREHOUSECODE`) USING BTREE,\n",
      "  CONSTRAINT `FK_sgt_wiplot_sgt_lot` FOREIGN KEY (`LOTNO`) REFERENCES `sgt_lot` (`LOTNO`),\n",
      "  CONSTRAINT `FK_sgt_wiplot_udt_warehouse` FOREIGN KEY (`WAREHOUSECODE`) REFERENCES `udt_warehouse` (`WAREHOUSECODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- 공장생산라인정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `udt_factoryline` (\n",
      "  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\n",
      "  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\n",
      "  `FACTORYCODE` char(20) NOT NULL COMMENT '공장코드',\n",
      "  `MESYN` int(11) NOT NULL DEFAULT 0 COMMENT '자동화여부',\n",
      "  `AUTOINPUTMATYN` int(11) DEFAULT NULL COMMENT '자재자동공급여부',\n",
      "  PRIMARY KEY (`LINENO`) USING BTREE,\n",
      "  KEY `R_98` (`FACTORYCODE`) USING BTREE,\n",
      "  CONSTRAINT `R_98` FOREIGN KEY (`FACTORYCODE`) REFERENCES `udt_factory` (`FACTORYCODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- 공정정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `udt_process` (\n",
      "  `PROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '공정코드',\n",
      "  `PROCESSNAME` char(50) NOT NULL DEFAULT '' COMMENT '공정명',\n",
      "  `COMPLETEONLY` char(1) NOT NULL DEFAULT '' COMMENT '',\n",
      "  `LOTFLOW` char(1) NOT NULL DEFAULT '' COMMENT '',\n",
      "  `WORKSTATIONCODE` char(20) DEFAULT NULL COMMENT '워크스테이션코드',\n",
      "  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\n",
      "  PRIMARY KEY (`PROCESSCODE`) USING BTREE,\n",
      "  KEY `fk_udt_process_workstation` (`WORKSTATIONCODE`) USING BTREE,\n",
      "  CONSTRAINT `fk_udt_process_workstation` FOREIGN KEY (`WORKSTATIONCODE`) REFERENCES `udt_workstation` (`WORKSTATIONCODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "</SCHEMA>\n",
      "\n",
      "규칙:\n",
      "1. 반드시 스키마에 존재하는 테이블과 컬럼만 사용하십시오.\n",
      "2. 답변은 SQL 쿼리만 출력하고 설명은 절대 포함하지 마십시오.\n",
      "3. WHERE / GROUP BY / ORDER BY는 스키마 기반으로 논리적으로 작성하십시오.\n",
      "4. 쿼리는 항상 실행 가능한 SQL이어야 합니다.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "올해 포르쉐라인 월평균 생산량 계산해줘<|im_end|>\n",
      "<|im_start|>assistant\n",
      "SELECT AVG(MONTHLY_PRODUCTION) AS AVG_MONTHLY_PRODUCTION FROM (SELECT DATE_FORMAT(ACTUALTIME, '%Y-%m') AS MONTH, SUM(TRANSQTY) AS MONTHLY_PRODUCTION FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' AND YEAR(ACTUALTIME) = YEAR(NOW()) GROUP BY DATE_FORMAT(ACTUALTIME, '%Y-%m')) AS MONTHLY_DATA;<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72437ff3",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb39bf90-042a-470f-8ded-63dba005466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880250c",
   "metadata": {},
   "source": [
    "`lora_alpha`: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "\n",
    "`lora_dropout`: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. `0.1`로 설정하면 학습 중 10%의 뉴런이 비활성화됩니다.\n",
    "\n",
    "`r`: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "\n",
    "`bias`: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. `\"none\"`으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. `\"all\"` 또는 `\"lora_only\"`와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "\n",
    "`target_modules`: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 `\"q_proj\"`와 `\"v_proj\"`를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "\n",
    "`task_type`: LoRA가 적용되는 작업 유형을 지정합니다. `\"CAUSAL_LM\"`은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 `\"SEQ2SEQ_LM\"`(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d57675fd-4374-4cd6-b8bb-950bf216cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen3-4b-text-to-sql-ko\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=5,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None,\n",
    "    max_length=max_seq_length,              # 최대 시퀀스 길이 추가\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b457fc",
   "metadata": {},
   "source": [
    "`output_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\n",
    "`num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니다.\n",
    "\n",
    "`per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있습니다.\n",
    "\n",
    "`gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 2로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 유용합니다.\n",
    "\n",
    "`gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질 수 있습니다.\n",
    "\n",
    "`optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적화기를 사용합니다.\n",
    "\n",
    "`logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마다 로그를 기록합니다.\n",
    "\n",
    "`save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스텝마다 모델이 저장됩니다.\n",
    "\n",
    "`save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50 스텝마다 모델을 저장합니다.\n",
    "\n",
    "`bf16`: bfloat16 정밀도를 사용하도록 설정합니다. bfloat16은 FP32와 유사한 범위를 제공하면서 메모리와 계산 효율성을 높입니다.\n",
    "\n",
    "`learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 사용하여 안정적인 학습을 유도합니다.\n",
    "\n",
    "`max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 조정하여 폭발적 그래디언트를 방지합니다.\n",
    "\n",
    "`warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다. 학습의 안정성을 높이기 위해 사용됩니다.\n",
    "\n",
    "`lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"constant\"`는 학습률을 일정하게 유지합니다.\n",
    "\n",
    "`push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "\n",
    "`remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다. True로 설정하면 메모리를 절약할 수 있습니다.\n",
    "\n",
    "`dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset: True`로 설정하여 데이터셋 준비 단계를 건너뜹니다.\n",
    "\n",
    "`report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11e4e1",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adbcf4b1-ab29-451d-b628-21466d16519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eccfa92",
   "metadata": {},
   "source": [
    "`collate_fn(batch)` 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 attention_mask를 생성합니다. 이후 레이블 데이터를 초기화한 다음, 특정 토큰 패턴(<|im_start|>assistant 이후부터 <|im_end|>까지)을 찾아 해당 범위에 레이블을 설정합니다. 이 범위를 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다.\n",
    "\n",
    "최종적으로, 배치 내 모든 샘플의 길이를 동일하게 맞추기 위해 패딩 작업을 수행합니다. 이 과정에서 입력 데이터에는 패딩 토큰 ID를 추가하고, 어텐션 마스크에는 0을 추가하며, 레이블에는 -100을 추가합니다. 모든 데이터는 PyTorch 텐서로 변환되어 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82c8810f-2613-4d6b-8dcb-985ba50991f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 2877])\n",
      "어텐션 마스크 형태: torch.Size([1, 2877])\n",
      "레이블 형태: torch.Size([1, 2877])\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이\n",
    "max_seq_length=8192\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d50fbb8d-1265-4c9a-b56e-2803f09bd810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 271, 64795, 82528, 33704, 7870, 17877, 51917, 42905, 15235, 54070, 142713, 78952, 624, 52959, 53442, 16560, 54248, 132947, 12802, 24897, 79207, 44680, 92120, 125544, 7, 58781, 8, 78952, 382, 27, 3540, 35839, 397, 313, 139495, 130973, 63256, 81650, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 839, 91849, 1458, 839, 63, 2399, 220, 1565, 43103, 7243, 915, 63, 79663, 7, 17, 15, 8, 4183, 1770, 11955, 220, 15, 50505, 364, 81650, 915, 751, 220, 1565, 15745, 8996, 63, 1161, 7, 17, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 15745, 72048, 751, 220, 1565, 63127, 63362, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 76435, 85057, 50340, 31328, 72048, 751, 220, 1565, 48773, 46159, 7535, 63, 1161, 7, 20, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 131342, 79632, 751, 220, 1565, 18129, 7243, 45522, 63, 8874, 7, 18, 8, 4183, 1770, 11955, 364, 15, 15, 15, 15, 12, 15, 15, 12, 15, 15, 220, 15, 15, 25, 15, 15, 25, 15, 15, 13, 15, 15, 15, 6, 50505, 364, 81650, 43866, 28754, 32077, 29326, 751, 220, 1565, 6823, 16383, 18129, 63, 8874, 7, 18, 8, 4183, 1770, 11955, 364, 15, 15, 15, 15, 12, 15, 15, 12, 15, 15, 220, 15, 15, 25, 15, 15, 25, 15, 15, 13, 15, 15, 15, 6, 50505, 364, 125086, 37087, 21530, 44680, 51275, 32077, 29326, 751, 220, 1565, 44, 44407, 14866, 63, 1161, 7, 18, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 25715, 57132, 168, 77047, 751, 220, 1565, 44, 44407, 7535, 63, 1161, 7, 20, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 25715, 57132, 79632, 751, 220, 1565, 43103, 48, 22098, 63, 1990, 4183, 1770, 11955, 220, 15, 50505, 364, 126667, 65306, 23259, 131837, 751, 220, 1565, 44690, 48, 22098, 63, 1990, 4183, 1770, 11955, 220, 15, 50505, 364, 34759, 79879, 23259, 131837, 751, 220, 1565, 70450, 48, 22098, 63, 1990, 11955, 1770, 50505, 364, 126667, 65306, 126641, 125144, 52300, 23259, 131837, 751, 220, 1565, 43103, 52, 1898, 63, 1161, 7, 20, 8, 11955, 1770, 50505, 364, 132612, 29281, 125068, 80901, 751, 220, 1565, 7663, 96394, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 130095, 34395, 168, 77047, 751, 220, 1565, 33, 3067, 44, 44407, 14866, 63, 1161, 7, 18, 15, 8, 11955, 1770, 50505, 364, 25715, 57132, 168, 77047, 751, 220, 1565, 53436, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 78125, 29281, 168, 77047, 751, 220, 1565, 6448, 14866, 63, 1161, 7, 17, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 55054, 26699, 25715, 168, 77047, 751, 220, 37467, 12013, 28654, 43103, 7243, 915, 32881, 62051, 21111, 6593, 198, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 50505, 1131, 15745, 87737, 71015, 48606, 2357, 313, 40656, 98358, 126299, 32831, 73077, 97929, 18411, 134312, 117, 97834, 66425, 130039, 16235, 226, 29326, 10764, 72509, 12802, 135032, 17877, 51917, 60838, 624, 22599, 14363, 1565, 34431, 91849, 63, 2399, 262, 1565, 9184, 2152, 63, 23997, 7, 17, 15, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 15745, 72048, 751, 262, 1565, 79983, 63, 23997, 7, 17, 15, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 76435, 85057, 50340, 31328, 72048, 751, 262, 1565, 8243, 1851, 63, 23997, 7, 18, 15, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 25715, 57132, 168, 77047, 751, 262, 1565, 8243, 606, 63, 23997, 7, 20, 15, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 25715, 57132, 79632, 751, 262, 1565, 11049, 1851, 63, 23997, 7, 17, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 34395, 127906, 168, 77047, 751, 262, 1565, 874, 1851, 17, 63, 23997, 7, 17, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 6870, 262, 1565, 7120, 349, 63, 39362, 44618, 7, 18, 8, 4183, 1770, 50505, 364, 76435, 32831, 32077, 29326, 751, 262, 1565, 78133, 349, 63, 39362, 44618, 7, 18, 8, 1770, 50505, 364, 125544, 129567, 32077, 29326, 751, 262, 1565, 3182, 29303, 63, 56272, 4183, 1770, 50505, 364, 76435, 32831, 23259, 131837, 751, 262, 1565, 20270, 84, 316, 63, 23997, 7, 20, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 126414, 129044, 132612, 29281, 125068, 80901, 751, 262, 1565, 285, 649, 63, 23997, 7, 16, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 6870, 262, 1565, 1851, 16, 63, 23997, 7, 20, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 6870, 262, 1565, 1851, 17, 63, 23997, 7, 20, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 6870, 262, 1565, 1358, 842, 63, 23997, 7, 17, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 54330, 51588, 915, 751, 262, 1565, 13800, 29303, 63, 23997, 7, 16, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 23259, 131837, 33593, 66261, 57026, 63089, 751, 262, 1565, 9184, 1252, 63, 49197, 3221, 7, 21, 8, 4183, 1770, 50505, 364, 15745, 31328, 144727, 24897, 751, 262, 1565, 45546, 1851, 63, 23997, 7, 17, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 130095, 34395, 168, 77047, 751, 262, 1565, 65, 453, 8243, 1851, 63, 23997, 7, 18, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 25715, 57132, 168, 77047, 751, 262, 1565, 4630, 1851, 63, 23997, 7, 17, 15, 8, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 78125, 29281, 168, 77047, 751, 262, 1565, 1778, 2454, 63, 23997, 7, 16, 15, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 85251, 124528, 55902, 86372, 88259, 79716, 7, 5595, 69167, 11, 51437, 1718, 11, 24344, 38879, 11, 49671, 11, 17542, 16921, 262, 1565, 66547, 29303, 63, 56272, 1770, 50505, 364, 66845, 20487, 23259, 131837, 751, 262, 1565, 46723, 29303, 63, 56272, 1770, 50505, 364, 29326, 67511, 23259, 131837, 751, 262, 1565, 66547, 1678, 63, 39362, 44618, 7, 18, 8, 1770, 50505, 364, 66845, 20487, 32077, 29326, 751, 262, 1565, 46723, 1678, 63, 39362, 44618, 7, 18, 8, 1770, 50505, 364, 29326, 67511, 32077, 29326, 751, 262, 1565, 66676, 560, 10438, 63, 23997, 7, 16, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 43590, 129439, 130973, 63256, 57026, 63089, 751, 262, 1565, 691, 17384, 10438, 63, 23997, 7, 16, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 167, 50362, 23259, 126886, 130973, 63256, 57026, 63089, 751, 262, 1565, 34735, 603, 63, 23997, 7, 16, 8, 4183, 1770, 62508, 2336, 364, 4762, 23, 39177, 43514, 6, 50505, 364, 130973, 63256, 57026, 63089, 1248, 8, 48360, 28, 5050, 1637, 1402, 401, 313, 64577, 57132, 126800, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 661, 83, 33884, 63, 2399, 220, 1565, 44, 44407, 14866, 63, 1161, 7, 18, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 25715, 57132, 168, 77047, 751, 220, 1565, 44, 44407, 7535, 63, 1161, 7, 20, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 25715, 57132, 79632, 751, 220, 1565, 44, 44407, 9502, 14866, 63, 1161, 7, 17, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 25715, 57132, 88259, 79716, 168, 77047, 751, 220, 1565, 15745, 48, 22098, 63, 1990, 11955, 1770, 50505, 364, 15745, 23259, 131837, 751, 220, 1565, 784, 38635, 52, 1898, 63, 1161, 7, 20, 8, 11955, 1770, 50505, 364, 126414, 129044, 132612, 29281, 125068, 80901, 751, 220, 1565, 45433, 63, 1161, 7, 16, 15, 15, 8, 11955, 1770, 50505, 364, 125624, 79632, 751, 220, 1565, 787, 24033, 63, 1161, 7, 20, 15, 8, 11955, 1770, 50505, 364, 70582, 34395, 751, 220, 37467, 12013, 28654, 44, 44407, 14866, 32881, 62051, 21111, 6593, 345, 220, 12013, 1565, 41718, 71939, 83, 33884, 33884, 1313, 63, 28654, 44, 44407, 9502, 14866, 32881, 62051, 21111, 6593, 345, 220, 93763, 1565, 41718, 71939, 83, 33884, 33884, 1313, 63, 81356, 12013, 28654, 44, 44407, 9502, 14866, 32881, 78656, 1565, 661, 83, 33884, 1313, 63, 28654, 44, 44407, 9502, 14866, 24183, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 401, 313, 53360, 87737, 71015, 48606, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 839, 2965, 10819, 50571, 63, 2399, 220, 1565, 43103, 7243, 915, 63, 79663, 7, 17, 15, 8, 4183, 1770, 11955, 220, 15, 50505, 364, 28626, 136499, 144791, 92031, 52959, 12802, 89235, 751, 220, 1565, 5595, 52, 44618, 63, 1990, 4183, 1770, 11955, 220, 15, 50505, 364, 66845, 20487, 134745, 751, 220, 1565, 9117, 32329, 784, 5660, 63, 1990, 4183, 1770, 11955, 220, 15, 50505, 364, 131342, 134745, 751, 220, 1565, 70450, 33, 3067, 44, 44407, 14866, 63, 1161, 7, 18, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 13146, 48431, 78125, 29281, 25715, 57132, 168, 77047, 751, 220, 1565, 45, 3257, 4239, 1285, 5246, 14866, 63, 1161, 7, 17, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 13146, 48431, 78125, 29281, 168, 77047, 751, 220, 1565, 44, 41234, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 6870, 220, 37467, 12013, 28654, 43103, 7243, 915, 32881, 62051, 21111, 6593, 345, 220, 93763, 1565, 26998, 5224, 2965, 10819, 50571, 5224, 91849, 1458, 839, 63, 81356, 12013, 28654, 43103, 7243, 915, 32881, 78656, 1565, 839, 91849, 1458, 839, 63, 28654, 43103, 7243, 915, 24183, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 401, 313, 125466, 29281, 130973, 63256, 81650, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 839, 42980, 839, 63, 2399, 220, 1565, 43103, 7243, 915, 63, 79663, 7, 17, 15, 8, 4183, 1770, 11955, 220, 15, 50505, 364, 81650, 915, 751, 220, 1565, 5595, 52, 44618, 63, 1990, 4183, 1770, 11955, 220, 15, 50505, 364, 66845, 20487, 134745, 751, 220, 1565, 9117, 32329, 784, 5660, 63, 1990, 4183, 1770, 11955, 220, 15, 50505, 364, 131342, 134745, 751, 220, 1565, 70450, 7663, 96394, 14866, 63, 1161, 7, 17, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 12802, 57089, 52300, 130095, 34395, 168, 77047, 751, 220, 1565, 44, 41234, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 40853, 70582, 168, 77047, 751, 220, 37467, 12013, 28654, 43103, 7243, 915, 32881, 62051, 21111, 6593, 345, 220, 93763, 1565, 26998, 5224, 42980, 839, 5224, 91849, 1458, 839, 63, 81356, 12013, 28654, 43103, 7243, 915, 32881, 78656, 1565, 839, 91849, 1458, 839, 63, 28654, 43103, 7243, 915, 24183, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 401, 313, 53360, 125466, 29281, 132876, 55902, 130803, 60039, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 1991, 83, 1670, 72, 4469, 63, 2399, 220, 1565, 15745, 8996, 63, 1161, 7, 17, 15, 8, 4183, 1770, 50505, 364, 15745, 72048, 751, 220, 1565, 63127, 63362, 63, 1161, 7, 17, 15, 8, 4183, 1770, 50505, 364, 76435, 85057, 50340, 31328, 72048, 751, 220, 1565, 15745, 44331, 63, 2613, 396, 7, 21, 8, 4183, 1770, 11955, 220, 15, 50505, 364, 15745, 31328, 144727, 24897, 751, 220, 1565, 7663, 96394, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 130095, 34395, 168, 77047, 751, 220, 1565, 33, 3067, 44, 44407, 14866, 63, 1161, 7, 18, 15, 8, 11955, 1770, 50505, 364, 25715, 57132, 168, 77047, 751, 220, 1565, 53436, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 78125, 29281, 168, 77047, 751, 220, 1565, 18470, 24628, 63, 1161, 7, 16, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 85251, 124528, 55902, 86372, 88259, 79716, 7, 5595, 69167, 11, 51437, 1718, 11, 24344, 38879, 11, 49671, 11, 17542, 16921, 220, 1565, 5595, 69167, 48, 22098, 63, 1990, 11955, 1770, 50505, 364, 43866, 28754, 23259, 131837, 751, 220, 1565, 22564, 1479, 48, 22098, 63, 1990, 11955, 1770, 50505, 364, 67511, 124517, 23259, 131837, 751, 220, 1565, 5595, 69167, 18129, 63, 8874, 7, 18, 8, 11955, 1770, 50505, 364, 66845, 20487, 32077, 29326, 751, 220, 1565, 22564, 1479, 18129, 63, 8874, 7, 18, 8, 11955, 1770, 50505, 364, 29326, 67511, 32077, 29326, 751, 220, 1565, 5790, 27377, 6578, 70396, 63, 1161, 7, 16, 8, 4183, 1770, 11955, 3355, 50505, 364, 43590, 129439, 130973, 63256, 57026, 63089, 751, 220, 1565, 47892, 1706, 1930, 3965, 70396, 63, 1161, 7, 16, 8, 4183, 1770, 11955, 3355, 50505, 364, 167, 50362, 23259, 126886, 130973, 63256, 57026, 63089, 751, 220, 1565, 8696, 48751, 5225, 63, 1161, 7, 16, 8, 4183, 1770, 11955, 3355, 50505, 364, 130973, 63256, 57026, 63089, 751, 220, 37467, 12013, 28654, 15745, 44331, 40181, 15745, 8996, 40181, 63127, 63362, 32881, 62051, 21111, 6593, 345, 220, 12013, 1565, 41718, 643, 5178, 5228, 9184, 91849, 307, 63, 28654, 15745, 8996, 40181, 63127, 63362, 32881, 62051, 21111, 6593, 345, 220, 12013, 1565, 26998, 643, 5178, 1670, 72, 4469, 71939, 83, 62, 45546, 63, 28654, 7663, 96394, 14866, 32881, 62051, 21111, 6593, 345, 220, 93763, 1565, 26998, 643, 5178, 1670, 72, 4469, 643, 5178, 91849, 63, 81356, 12013, 28654, 15745, 8996, 32881, 78656, 1565, 1991, 83, 91849, 63, 28654, 15745, 8996, 89746, 220, 93763, 1565, 26998, 643, 5178, 1670, 72, 4469, 71939, 83, 62, 45546, 63, 81356, 12013, 28654, 7663, 96394, 14866, 32881, 78656, 1565, 661, 83, 62, 45546, 63, 28654, 7663, 96394, 14866, 24183, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 401, 313, 125466, 40853, 76435, 85057, 50340, 31328, 126800, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 661, 83, 24269, 1056, 63, 2399, 220, 1565, 63127, 63362, 63, 1161, 7, 17, 15, 8, 4183, 1770, 50505, 364, 76435, 85057, 50340, 31328, 72048, 751, 220, 1565, 45433, 63, 1161, 7, 16, 15, 15, 8, 11955, 1770, 50505, 364, 125624, 79632, 751, 220, 1565, 58710, 7516, 14866, 63, 1161, 7, 17, 15, 8, 4183, 1770, 50505, 364, 78125, 40853, 168, 77047, 751, 220, 1565, 87711, 24273, 63, 526, 7, 16, 16, 8, 4183, 1770, 11955, 220, 15, 50505, 364, 143994, 56290, 57026, 63089, 751, 220, 1565, 67260, 29421, 56889, 24273, 63, 526, 7, 16, 16, 8, 11955, 1770, 50505, 364, 25715, 57132, 143994, 78125, 128911, 57026, 63089, 751, 220, 37467, 12013, 28654, 63127, 63362, 32881, 62051, 21111, 6593, 345, 220, 12013, 1565, 49, 62, 24, 23, 63, 28654, 58710, 7516, 14866, 32881, 62051, 21111, 6593, 345, 220, 93763, 1565, 49, 62, 24, 23, 63, 81356, 12013, 28654, 58710, 7516, 14866, 32881, 78656, 1565, 661, 83, 24269, 63, 28654, 58710, 7516, 14866, 24183, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 401, 313, 125466, 29281, 126800, 10764, 72509, 12802, 135032, 198, 22599, 14363, 11551, 4183, 34839, 1565, 661, 83, 11305, 63, 2399, 220, 1565, 53436, 14866, 63, 1161, 7, 17, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 78125, 29281, 168, 77047, 751, 220, 1565, 53436, 7535, 63, 1161, 7, 20, 15, 8, 4183, 1770, 11955, 3355, 50505, 364, 78125, 29281, 79632, 751, 220, 1565, 81969, 31092, 63, 1161, 7, 16, 8, 4183, 1770, 11955, 3355, 50505, 6870, 220, 1565, 1593, 10808, 9441, 63, 1161, 7, 16, 8, 4183, 1770, 11955, 3355, 50505, 6870, 220, 1565, 18470, 784, 3495, 14866, 63, 1161, 7, 17, 15, 8, 11955, 1770, 50505, 364, 130109, 81133, 24897, 130229, 12802, 92031, 168, 77047, 751, 220, 1565, 45433, 63, 1161, 7, 16, 15, 15, 8, 11955, 1770, 50505, 364, 125624, 79632, 751, 220, 37467, 12013, 28654, 53436, 14866, 32881, 62051, 21111, 6593, 345, 220, 12013, 1565, 41718, 71939, 83, 11305, 11498, 20155, 63, 28654, 18470, 784, 3495, 14866, 32881, 62051, 21111, 6593, 345, 220, 93763, 1565, 41718, 71939, 83, 11305, 11498, 20155, 63, 81356, 12013, 28654, 18470, 784, 3495, 14866, 32881, 78656, 1565, 661, 83, 11498, 20155, 63, 28654, 18470, 784, 3495, 14866, 24183, 8, 48360, 28, 641, 82240, 11955, 97701, 22264, 23, 62508, 2336, 22264, 23, 39177, 43514, 280, 522, 3540, 35839, 1339, 131005, 135092, 510, 16, 13, 141762, 79207, 44680, 92120, 125544, 19391, 134015, 42905, 10764, 72509, 12802, 135032, 53680, 89095, 105, 125894, 72553, 40720, 16186, 139713, 624, 17, 13, 143604, 33704, 7870, 3315, 123, 120, 28002, 72553, 61126, 126204, 133828, 33704, 18585, 230, 66845, 133970, 87425, 95577, 139713, 624, 18, 13, 5288, 608, 26870, 7710, 608, 15520, 7710, 16560, 79207, 44680, 92120, 125544, 54116, 126641, 42039, 127041, 120, 28002, 128552, 140174, 16186, 139713, 624, 19, 13, 3315, 123, 120, 132920, 142655, 85788, 143964, 7870, 12802, 137571, 130650, 624, 151645, 198, 151644, 872, 198, 130137, 33883, 98869, 125548, 144838, 50340, 31328, 85413, 242, 126742, 139746, 141091, 131837, 94203, 85057, 33883, 144131, 151645, 198, 151644, 77091, 198, 4858, 70414, 3189, 711, 3617, 8932, 5756, 47546, 8, 5752, 70414, 48023, 8932, 5756, 47546, 4295, 320, 4858, 28543, 14960, 4346, 1162, 16383, 18129, 11, 7677, 56, 11069, 76, 863, 5752, 57980, 11, 30735, 7, 43103, 48, 22098, 8, 5752, 57980, 8932, 5756, 47546, 4295, 8081, 4606, 51, 43103, 7243, 5288, 64789, 63362, 284, 364, 2668, 15, 15, 15, 15, 6, 3567, 93923, 7535, 284, 364, 16750, 16444, 6, 3567, 50905, 4346, 1162, 16383, 18129, 8, 284, 50905, 8204, 3307, 2140, 26870, 7710, 28543, 14960, 4346, 1162, 16383, 18129, 11, 7677, 56, 11069, 76, 3789, 5752, 57980, 8932, 7896, 26, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c10d8b0-3604-45b8-95c1-c91c02599db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 4858, 70414, 3189, 711, 3617, 8932, 5756, 47546, 8, 5752, 70414, 48023, 8932, 5756, 47546, 4295, 320, 4858, 28543, 14960, 4346, 1162, 16383, 18129, 11, 7677, 56, 11069, 76, 863, 5752, 57980, 11, 30735, 7, 43103, 48, 22098, 8, 5752, 57980, 8932, 5756, 47546, 4295, 8081, 4606, 51, 43103, 7243, 5288, 64789, 63362, 284, 364, 2668, 15, 15, 15, 15, 6, 3567, 93923, 7535, 284, 364, 16750, 16444, 6, 3567, 50905, 4346, 1162, 16383, 18129, 8, 284, 50905, 8204, 3307, 2140, 26870, 7710, 28543, 14960, 4346, 1162, 16383, 18129, 11, 7677, 56, 11069, 76, 3789, 5752, 57980, 8932, 7896, 26, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf6d01",
   "metadata": {},
   "source": [
    "LLM 학습에서 `input_ids`와 `labels`는 모델의 학습 목표에 따라 생성됩니다. 이를 예시 문장과 정수 인코딩을 통해 상세히 설명하겠습니다.\n",
    "\n",
    "\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다.  \n",
    "사용자가 **\"안녕하세요, 오늘 날씨는 어떤가요?\"**라고 물었고,  \n",
    "모델은 **\"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"**라고 응답해야 한다고 합시다.\n",
    "\n",
    "이 데이터를 학습하기 위해 먼저 전체 대화 데이터를 정수로 인코딩합니다.  \n",
    "토크나이저는 대화의 구조를 구분하기 위해 `<|im_start|>`와 `<|im_end|>`을 사용하여 정수 시퀀스를 생성한다고 가정해봅시다.  \n",
    "(실제로는 LLM 템플릿이 이보다는 복잡함을 기억하고 혼동하지 맙시다.)\n",
    "정수 시퀀스는 다음과 같이 구성될 수 있습니다.  \n",
    "\n",
    "```\n",
    "<|im_start|>user 안녕하세요, 오늘 날씨는 어떤가요?<|im_end|>  \n",
    "<|im_start|>assistant 안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|>\n",
    "```\n",
    "\n",
    "이를 정수로 변환하면 다음과 같습니다.  \n",
    "`input_ids = [1001, 2001, 3001, 4001, 5001, 6001, 7001, 1002, 1001, 8001, 9001, 1003, 2002]`  \n",
    "모델이 예측해야 하는 부분은 `assistant`의 응답인 \"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"입니다. 따라서, `labels`는 다음과 같이 설정됩니다.\n",
    "\n",
    "`labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, 8001, 9001, 1003, 2002]`  \n",
    "\n",
    "이처럼 `labels`는 모델의 출력이 필요한 영역만을 포함하고, 나머지 부분은 `-100`으로 채워져 모델이 실제로 예측하고 오차를 계산해야 하는 대상(학습 대상)에서 제외됩니다. 이를 통해 모델은 불필요한 영역을 학습하지 않고, 필요한 응답 영역에만 집중할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff0f9e",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a511a4b7-d2f8-4942-a501-7d7a4e4cfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edb9fbff-19ef-4b4b-8381-11df04aa96a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='780' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [780/780 49:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.418100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.116100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.085500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc989b25",
   "metadata": {},
   "source": [
    "## 6. 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a7972",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 '<|im_start|>assistant'가 부착되어서 넣는 것이 좋습니다. 그래야만 모델이 바로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b58447a2-f506-4529-ac01-a8c280cb76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed095c91-c0ab-4b6c-b60c-cf8c8314db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "\n",
      "당신은 SQL을 생성하는 AI 모델입니다.\n",
      "아래는 데이터베이스 스키마(DDL)입니다.\n",
      "\n",
      "<SCHEMA>\n",
      "-- 작업완료로그 테이블\n",
      "CREATE TABLE IF NOT EXISTS `log_lottranslog` (\n",
      "  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\n",
      "  `LOTNO` char(20) NOT NULL DEFAULT '' COMMENT 'LOT번호',\n",
      "  `LINENO` char(20) DEFAULT NULL COMMENT '생산라인번호',\n",
      "  `TRANSACTIONNAME` char(50) NOT NULL DEFAULT '' COMMENT '처리명',\n",
      "  `TIMELOGGED` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '로그입력일시',\n",
      "  `ACTUALTIME` datetime(3) NOT NULL DEFAULT '0000-00-00 00:00:00.000' COMMENT '실제실행일시',\n",
      "  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\n",
      "  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\n",
      "  `TRANSQTY` double NOT NULL DEFAULT 0 COMMENT '변경수량',\n",
      "  `CURRENTQTY` double NOT NULL DEFAULT 0 COMMENT '현재수량',\n",
      "  `NEXTQTY` double DEFAULT NULL COMMENT '변경반영된수량',\n",
      "  `TRANSUOM` char(5) DEFAULT NULL COMMENT '측정단위',\n",
      "  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\n",
      "  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\n",
      "  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\n",
      "  `USERCODE` char(20) NOT NULL DEFAULT '' COMMENT '사용자코드',\n",
      "  PRIMARY KEY (`TRANSLOGID`) USING BTREE\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci COMMENT='LOT 변경 로그';\n",
      "\n",
      "-- VIEW 종속성 오류를 극복하기 위해 임시 테이블을 생성합니다.\n",
      "CREATE TABLE `vw_lot` (\n",
      "    `lotno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT 'LOT번호',\n",
      "    `lineno` CHAR(20) NOT NULL COLLATE 'utf8_general_ci' COMMENT '생산라인번호',\n",
      "    `materialcode` CHAR(30) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\n",
      "    `materialname` CHAR(50) NOT NULL COLLATE 'utf8_general_ci' COMMENT '자재명',\n",
      "    `customercode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '고객코드',\n",
      "    `comcode2` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `createdate` DATETIME(3) NOT NULL COMMENT '생성일시',\n",
      "    `duedate` DATETIME(3) NULL COMMENT '마감일시',\n",
      "    `createqty` DOUBLE NOT NULL COMMENT '생성수량',\n",
      "    `standarduom` CHAR(5) NULL COLLATE 'utf8_general_ci' COMMENT '표준측정단위',\n",
      "    `ispr` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `code1` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `code2` CHAR(50) NULL COLLATE 'utf8_general_ci' COMMENT '',\n",
      "    `order_id` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '주문ID',\n",
      "    `confirmqty` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '수량확인여부',\n",
      "    `lotindex` SMALLINT(6) NOT NULL COMMENT 'LOT인덱스',\n",
      "    `warehousecode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '창고코드',\n",
      "    `bopmaterialcode` CHAR(30) NULL COLLATE 'utf8_general_ci' COMMENT '자재코드',\n",
      "    `processcode` CHAR(20) NULL COLLATE 'utf8_general_ci' COMMENT '공정코드',\n",
      "    `workstate` CHAR(10) NOT NULL COLLATE 'utf8_general_ci' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\n",
      "    `queuedqty` DOUBLE NULL COMMENT '대기수량',\n",
      "    `startedqty` DOUBLE NULL COMMENT '시작수량',\n",
      "    `queuedtime` DATETIME(3) NULL COMMENT '대기일시',\n",
      "    `startedtime` DATETIME(3) NULL COMMENT '시작일시',\n",
      "    `consumptiondone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '소모완료여부',\n",
      "    `datacollectdone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '데이터수집완료여부',\n",
      "    `completedone` CHAR(1) NOT NULL COLLATE 'utf8_general_ci' COMMENT '완료여부'\n",
      ") ENGINE=MyISAM;\n",
      "\n",
      "-- 자재정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `udt_material` (\n",
      "  `MATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '자재코드',\n",
      "  `MATERIALNAME` char(50) NOT NULL DEFAULT '' COMMENT '자재명',\n",
      "  `MATERIALTYPECODE` char(20) NOT NULL DEFAULT '' COMMENT '자재구분코드',\n",
      "  `LOTQTY` double DEFAULT NULL COMMENT 'LOT수량',\n",
      "  `STANDARDUOM` char(5) DEFAULT NULL COMMENT '표준측정단위',\n",
      "  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\n",
      "  `REMARK` char(50) DEFAULT NULL COMMENT '비고',\n",
      "  PRIMARY KEY (`MATERIALCODE`) USING BTREE,\n",
      "  KEY `fk_udt_material_materialtype` (`MATERIALTYPECODE`) USING BTREE,\n",
      "  CONSTRAINT `fk_udt_material_materialtype` FOREIGN KEY (`MATERIALTYPECODE`) REFERENCES `udt_materialtype` (`MATERIALTYPECODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- LOT 변경 로그 테이블\n",
      "CREATE TABLE IF NOT EXISTS `log_completelog` (\n",
      "  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '트랜잭션아이디',\n",
      "  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\n",
      "  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\n",
      "  `NEXTBOPMATERIALCODE` char(30) NOT NULL DEFAULT '' COMMENT '다음공정자재코드',\n",
      "  `NEXTPROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '다음공정코드',\n",
      "  `MACHINECODE` char(20) DEFAULT NULL COMMENT '',\n",
      "  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\n",
      "  CONSTRAINT `FK_log_completelog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- 공정완료로그 테이블\n",
      "CREATE TABLE IF NOT EXISTS `log_finishlog` (\n",
      "  `TRANSLOGID` bigint(20) NOT NULL DEFAULT 0 COMMENT '로그ID',\n",
      "  `QUEUETIME` double NOT NULL DEFAULT 0 COMMENT '대기시간',\n",
      "  `PROCESSTIME` double NOT NULL DEFAULT 0 COMMENT '처리시간',\n",
      "  `NEXTWAREHOUSECODE` char(20) NOT NULL DEFAULT '' COMMENT '이동된창고코드',\n",
      "  `MACHINECODE` char(20) DEFAULT NULL COMMENT '장비코드',\n",
      "  PRIMARY KEY (`TRANSLOGID`) USING BTREE,\n",
      "  CONSTRAINT `FK_log_finishlog_log_lottranslog` FOREIGN KEY (`TRANSLOGID`) REFERENCES `log_lottranslog` (`TRANSLOGID`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- LOT 공정 진행상황 정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `sgt_wiplot` (\n",
      "  `LOTNO` char(20) NOT NULL COMMENT 'LOT번호',\n",
      "  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\n",
      "  `LOTINDEX` smallint(6) NOT NULL DEFAULT 0 COMMENT 'LOT인덱스',\n",
      "  `WAREHOUSECODE` char(20) DEFAULT NULL COMMENT '창고코드',\n",
      "  `BOPMATERIALCODE` char(30) DEFAULT NULL COMMENT '자재코드',\n",
      "  `PROCESSCODE` char(20) DEFAULT NULL COMMENT '공정코드',\n",
      "  `WORKSTATE` char(10) NOT NULL DEFAULT '' COMMENT '진행상태구분(QUEUED, PROCESSING, FINISHED, FAILED, HOLD)',\n",
      "  `QUEUEDQTY` double DEFAULT NULL COMMENT '입력수량',\n",
      "  `STARTEDQTY` double DEFAULT NULL COMMENT '작업수량',\n",
      "  `QUEUEDTIME` datetime(3) DEFAULT NULL COMMENT '대기일시',\n",
      "  `STARTEDTIME` datetime(3) DEFAULT NULL COMMENT '시작일시',\n",
      "  `CONSUMPTIONDONE` char(1) NOT NULL DEFAULT '' COMMENT '소모완료여부',\n",
      "  `DATACOLLECTDONE` char(1) NOT NULL DEFAULT '' COMMENT '데이터수집완료여부',\n",
      "  `COMPLETEDONE` char(1) NOT NULL DEFAULT '' COMMENT '완료여부',\n",
      "  PRIMARY KEY (`LOTINDEX`,`LOTNO`,`LINENO`) USING BTREE,\n",
      "  KEY `fk_sgt_sublot_lotid` (`LOTNO`,`LINENO`) USING BTREE,\n",
      "  KEY `FK_sgt_wiplot_udt_warehouse` (`WAREHOUSECODE`) USING BTREE,\n",
      "  CONSTRAINT `FK_sgt_wiplot_sgt_lot` FOREIGN KEY (`LOTNO`) REFERENCES `sgt_lot` (`LOTNO`),\n",
      "  CONSTRAINT `FK_sgt_wiplot_udt_warehouse` FOREIGN KEY (`WAREHOUSECODE`) REFERENCES `udt_warehouse` (`WAREHOUSECODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- 공장생산라인정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `udt_factoryline` (\n",
      "  `LINENO` char(20) NOT NULL COMMENT '생산라인번호',\n",
      "  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\n",
      "  `FACTORYCODE` char(20) NOT NULL COMMENT '공장코드',\n",
      "  `MESYN` int(11) NOT NULL DEFAULT 0 COMMENT '자동화여부',\n",
      "  `AUTOINPUTMATYN` int(11) DEFAULT NULL COMMENT '자재자동공급여부',\n",
      "  PRIMARY KEY (`LINENO`) USING BTREE,\n",
      "  KEY `R_98` (`FACTORYCODE`) USING BTREE,\n",
      "  CONSTRAINT `R_98` FOREIGN KEY (`FACTORYCODE`) REFERENCES `udt_factory` (`FACTORYCODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "\n",
      "-- 공정정보 테이블\n",
      "CREATE TABLE IF NOT EXISTS `udt_process` (\n",
      "  `PROCESSCODE` char(20) NOT NULL DEFAULT '' COMMENT '공정코드',\n",
      "  `PROCESSNAME` char(50) NOT NULL DEFAULT '' COMMENT '공정명',\n",
      "  `COMPLETEONLY` char(1) NOT NULL DEFAULT '' COMMENT '',\n",
      "  `LOTFLOW` char(1) NOT NULL DEFAULT '' COMMENT '',\n",
      "  `WORKSTATIONCODE` char(20) DEFAULT NULL COMMENT '워크스테이션코드',\n",
      "  `DESCRIPTION` char(100) DEFAULT NULL COMMENT '설명',\n",
      "  PRIMARY KEY (`PROCESSCODE`) USING BTREE,\n",
      "  KEY `fk_udt_process_workstation` (`WORKSTATIONCODE`) USING BTREE,\n",
      "  CONSTRAINT `fk_udt_process_workstation` FOREIGN KEY (`WORKSTATIONCODE`) REFERENCES `udt_workstation` (`WORKSTATIONCODE`)\n",
      ") ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci;\n",
      "</SCHEMA>\n",
      "\n",
      "규칙:\n",
      "1. 반드시 스키마에 존재하는 테이블과 컬럼만 사용하십시오.\n",
      "2. 답변은 SQL 쿼리만 출력하고 설명은 절대 포함하지 마십시오.\n",
      "3. WHERE / GROUP BY / ORDER BY는 스키마 기반으로 논리적으로 작성하십시오.\n",
      "4. 쿼리는 항상 실행 가능한 SQL이어야 합니다.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "포르쉐라인에서 이번 달 공정별 평균 cycle time 알려줘.<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bc52e99-eb23-46d9-a85f-85ef08d3e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT f.PROCESSCODE, AVG(TIMESTAMPDIFF(SECOND, c.create_time, f.ACTUALTIME)) AS AVG_CYCLE_SEC FROM LOG_LOTTRANSLOG f LEFT JOIN (SELECT LOTNO, MIN(ACTUALTIME) AS create_time FROM LOG_LOTTRANSLOG WHERE TRANSACTIONNAME='CREATE' GROUP BY LOTNO) c ON c.LOTNO = f.LOTNO WHERE f.LINENO='ML0000' AND f.TRANSACTIONNAME='FINISH' AND DATE_FORMAT(f.ACTUALTIME, '%Y%m') = DATE_FORMAT(CURDATE(), '%Y%m') GROUP BY f.PROCESSCODE;<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3522a-b7c9-4406-8df4-9d989044b873",
   "metadata": {},
   "source": [
    "## 7. 파인 튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfe38b",
   "metadata": {},
   "source": [
    "`AutoPeftModelForCausalLM()`의 입력으로 LoRA Adapter가 저장된 체크포인트의 주소를 넣으면 LoRA Adapter가 기존의 LLM과 부착되어 로드됩니다. 이 과정은 LoRA Adapter의 가중치를 사전 학습된 언어 모델(LLM)에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n",
    "\n",
    "`peft_model_id` 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. `\"qwen2-7b-rag-ko/checkpoint-285\"`는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서 해당 가중치를 불러옵니다.\n",
    "\n",
    "`fine_tuned_model`은 `AutoPeftModelForCausalLM.from_pretrained` 메서드를 통해 체크포인트를 로드하여 생성됩니다. 이 메서드는 LLM과 LoRA Adapter를 결합하고, 최적화된 설정으로 모델을 메모리에 로드합니다. `device_map=\"auto\"` 옵션은 모델을 자동으로 GPU에 배치합니다.\n",
    "\n",
    "`pipeline`은 Hugging Face의 고수준 유틸리티로, NLP 작업(예: 텍스트 생성, 번역, 요약 등)을 간단히 수행할 수 있게 해줍니다. 이 코드에서 사용된 `pipeline(\"text-generation\")`은 텍스트 생성 작업을 수행하기 위한 파이프라인 객체를 생성합니다. 파이프라인은 내부적으로 모델과 토크나이저를 관리하여, 입력 텍스트를 토큰화하고, 모델을 통해 생성된 결과를 다시 디코딩하여 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "\n",
    "이 코드는 미세 조정된 LLM을 로드한 뒤, 이를 이용해 텍스트 생성 작업을 간단히 수행할 수 있도록 준비하는 데 목적이 있습니다. `pipeline`을 통해 텍스트 생성 작업을 실행하면, 입력 텍스트에 기반하여 모델이 다음 토큰을 예측하고 이를 반복적으로 생성합니다. 이 과정은 사용자에게 자연스러운 텍스트를 출력하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15a4262b-186b-4abf-b6cf-65d22f3c10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27d9d395-a3c2-47b7-b135-34f4f78f2ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed1181d119f4263b5575a9c5498c63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen3-4b-text-to-sql-ko/checkpoint-450\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b6fbdd3-a8db-48dc-92fa-fb51b20f2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fad932e6-1c1a-4bd8-b1b5-c877a575cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c41ebf47-6905-45ad-b685-a1751613caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "WITH MONTHLY AS (SELECT DATE_FORMAT(ACTUALTIME, '%Y%m') AS YM, SUM(TRANSQTY) AS TOTAL_QTY FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='FINISH' AND DATE_FORMAT(ACTUALTIME, '%Y')=DATE_FORMAT(CURDATE(), '%Y') GROUP BY DATE_FORMAT(ACTUALTIME, '%Y%m')) SELECT YM, TOTAL_QTY, (SELECT SUM(CREATEQTY) FROM VW_LOT WHERE WORKSTATE='FAILED' AND DATE_FORMAT(CREATEDATE, '%Y')=DATE_FORMAT(CURDATE(), '%Y') AND DATE_FORMAT(CREATEDATE, '%Y%m')=YM) AS FAIL_QTY, ROUND((SELECT SUM(CREATEQTY) FROM VW_LOT WHERE WORKSTATE='FAILED' AND DATE_FORMAT(CREATEDATE, '%Y')=DATE_FORMAT(CURDATE(), '%Y') AND DATE_FORMAT(CREATEDATE, '%Y%m')=YM) / NULLIF(TOTAL_QTY, 0) * 1000000, 2) AS DEFECT_RATE_PPM FROM MONTHLY ORDER BY YM;\n",
      "    label:\n",
      "\n",
      "SELECT DATE_FORMAT(A.ACTUALTIME,'%Y%m') AS ym, ROUND((SUM(COALESCE(fl.fail_qty,0))/NULLIF(SUM(A.TRANSQTY),0))*1000000,2) AS ppm FROM LOG_LOTTRANSLOG A LEFT JOIN (SELECT LOTNO, MATERIALCODE, SUM(QUEUEDQTY) AS fail_qty FROM VW_LOT WHERE WORKSTATE='FAILED' AND DATE_FORMAT(CREATEDATE,'%Y')=DATE_FORMAT(CURDATE(),'%Y') GROUP BY LOTNO, MATERIALCODE) fl ON A.LOTNO=fl.LOTNO AND A.MATERIALCODE=fl.MATERIALCODE WHERE A.TRANSACTIONNAME='FINISH' AND DATE_FORMAT(A.ACTUALTIME,'%Y')=DATE_FORMAT(CURDATE(),'%Y') GROUP BY DATE_FORMAT(A.ACTUALTIME,'%Y%m') ORDER BY ym;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' AND ACTUALTIME >= '2025-03-16 00:00:00.000' AND ACTUALTIME < '2025-04-01 00:00:00.000';\n",
      "    label:\n",
      "\n",
      "SELECT SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' AND DATE(ACTUALTIME) BETWEEN '2025-03-16' AND '2025-03-31';<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT DATE_FORMAT(ll.ACTUALTIME, '%Y%m%d') AS WORKDATE, SUM(ll.TRANSQTY) AS WIP_INPUT_QTY FROM log_lottranslog ll WHERE ll.LINENO = 'ML0000' AND ll.TRANSQTY > 0 AND DATE(ll.ACTUALTIME) BETWEEN DATE_SUB(CURDATE(), INTERVAL 29 DAY) AND CURDATE() GROUP BY DATE_FORMAT(ll.ACTUALTIME, '%Y%m%d') ORDER BY WORKDATE;\n",
      "    label:\n",
      "\n",
      "WITH PARAM AS ( SELECT CURDATE() AS TARGET_DATE ) SELECT DATE_FORMAT(l.ACTUALTIME, '%Y%m%d') AS WORKDATE, SUM(l.TRANSQTY) AS WIP_INPUT_QTY FROM log_lottranslog l, PARAM WHERE l.LINENO = 'ML0000' AND l.TRANSACTIONNAME = 'WIP_START' AND DATE(l.ACTUALTIME) BETWEEN DATE_SUB(PARAM.TARGET_DATE, INTERVAL 29 DAY) AND PARAM.TARGET_DATE GROUP BY DATE_FORMAT(l.ACTUALTIME, '%Y%m%d') ORDER BY WORKDATE;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "WITH TMP AS (SELECT M.MATERIALCODE, L.LINENO, SUM(L.TRANSQTY) / (2 * 8) AS WORKER_PROD FROM LOG_LOTTRANSLOG L INNER JOIN UDT_MATERIAL M ON M.MATERIALCODE = L.MATERIALCODE WHERE L.TRANSACTIONNAME = 'FINISH' AND L.LINENO = 'ML0000' GROUP BY M.MATERIALCODE, L.LINENO) SELECT MATERIALCODE, LINENO, MAX(WORKER_PROD) AS MAX_PROD FROM TMP GROUP BY MATERIALCODE, LINENO ORDER BY MAX_PROD DESC;\n",
      "    label:\n",
      "\n",
      "SELECT MATERIALCODE, SUM(TRANSQTY) / (2 * 8) AS WORKER_PROD FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' GROUP BY MATERIALCODE ORDER BY WORKER_PROD DESC;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "WITH FD AS (SELECT LOTNO, MATERIALCODE, SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='FINISH' AND DATE_FORMAT(ACTUALTIME,'%Y%m')='202505' GROUP BY LOTNO, MATERIALCODE), SD AS (SELECT LOTNO, MATERIALCODE, SUM(TRANSQTY) AS STARTSUM FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='START' AND DATE_FORMAT(ACTUALTIME,'%Y%m')='202505' GROUP BY LOTNO, MATERIALCODE) SELECT FD.MATERIALCODE, ROUND((SUM(FD.FINISHSUM)/NULLIF(SUM(SD.STARTSUM),0))*100,2) AS YIELD_PCT FROM FD INNER JOIN SD ON FD.LOTNO=SD.LOTNO AND FD.MATERIALCODE=SD.MATERIALCODE GROUP BY FD.MATERIALCODE ORDER BY YIELD_PCT DESC LIMIT 5;\n",
      "    label:\n",
      "\n",
      "WITH FD AS (SELECT LOTNO, MATERIALCODE, SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='FINISH' AND DATE_FORMAT(ACTUALTIME,'%Y%m')='202505' GROUP BY LOTNO, MATERIALCODE), SD AS (SELECT LOTNO, MATERIALCODE, SUM(TRANSQTY) AS STARTSUM FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='START' AND DATE_FORMAT(ACTUALTIME,'%Y%m')='202505' GROUP BY LOTNO, MATERIALCODE), MAT_YIELD AS (SELECT FD.MATERIALCODE, SUM(FD.FINISHSUM) AS TOTAL_FINISH_QTY, SUM(SD.STARTSUM) AS TOTAL_START_QTY, ROUND((SUM(FD.FINISHSUM)/NULLIF(SUM(SD.STARTSUM),0))*100,2) AS YIELD_PCT FROM FD INNER JOIN SD ON FD.LOTNO=SD.LOTNO AND FD.MATERIALCODE=SD.MATERIALCODE GROUP BY FD.MATERIALCODE) SELECT MATERIALCODE, TOTAL_FINISH_QTY, TOTAL_START_QTY, YIELD_PCT FROM MAT_YIELD ORDER BY YIELD_PCT DESC LIMIT 5;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "WITH PARAM AS (SELECT CURDATE() AS TARGET_DATE), FQ AS (SELECT LOTNO, DATE(ACTUALTIME) AS WORKDATE, SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG, PARAM WHERE TRANSACTIONNAME='FINISH' AND LINENO='ML0000' AND DATE(ACTUALTIME) BETWEEN DATE_SUB(PARAM.TARGET_DATE,INTERVAL 1 MONTH) AND PARAM.TARGET_DATE GROUP BY LOTNO, DATE(ACTUALTIME)), SQ AS (SELECT LOTNO, DATE(ACTUALTIME) AS WORKDATE, SUM(TRANSQTY) AS STARTSUM FROM LOG_LOTTRANSLOG, PARAM WHERE TRANSACTIONNAME='START' AND LINENO='ML0000' AND DATE(ACTUALTIME) BETWEEN DATE_SUB(PARAM.TARGET_DATE,INTERVAL 1 MONTH) AND PARAM.TARGET_DATE GROUP BY LOTNO, DATE(ACTUALTIME)) SELECT DATE_FORMAT(F.WORKDATE,'%Y%m') AS WORKMONTH, ROUND(AVG(TIMESTAMPDIFF(SECOND,SQ.WORKDATE,F.WORKDATE)),2) AS AVG_CYCLE_SEC, STDDEV(ROUND(TIMESTAMPDIFF(SECOND,SQ.WORKDATE,F.WORKDATE),2)) AS STD_CYCLE_SEC FROM FQ F INNER JOIN SQ SQ ON F.LOTNO=SQ.LOTNO AND F.WORKDATE=SQ.WORKDATE GROUP BY F.WORKDATE;\n",
      "    label:\n",
      "\n",
      "SELECT DATE(f.ACTUALTIME) AS WORKDATE, AVG(TIMESTAMPDIFF(SECOND,c.create_time,f.ACTUALTIME)) AS AVG_CYCLE_SEC FROM LOG_LOTTRANSLOG f LEFT JOIN (SELECT LOTNO,MIN(ACTUALTIME) AS create_time FROM LOG_LOTTRANSLOG WHERE TRANSACTIONNAME='CREATE' GROUP BY LOTNO) c ON c.LOTNO=f.LOTNO WHERE f.LINENO='ML0000' AND f.TRANSACTIONNAME='FINISH' AND DATE(f.ACTUALTIME) >= DATE_SUB(CURDATE(),INTERVAL 1 MONTH) GROUP BY DATE(f.ACTUALTIME) ORDER BY WORKDATE;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT fd.ymd AS '작업일자', SUM(fd.prod_qty) AS '총 생산수량', SUM(COALESCE(fl.fail_qty,0)) AS '총 불량수량', ROUND((SUM(COALESCE(fl.fail_qty,0))/NULLIF(SUM(fd.prod_qty),0))*1000000,2) AS '불량율(ppm)' FROM ( SELECT A.LOTNO, A.MATERIALCODE, DATE_FORMAT(A.ACTUALTIME,'%Y%m%d') AS ymd, SUM(A.TRANSQTY) AS prod_qty FROM LOG_LOTTRANSLOG A WHERE A.TRANSACTIONNAME='FINISH' AND A.LINENO='ML0000' AND DATE_FORMAT(A.ACTUALTIME,'%Y%m') = DATE_FORMAT(CURDATE(),'%Y%m') GROUP BY A.LOTNO, A.MATERIALCODE, DATE_FORMAT(A.ACTUALTIME,'%Y%m%d') ) fd LEFT JOIN ( SELECT LOTNO, MATERIALCODE, SUM(QUEUEDQTY) AS fail_qty FROM VW_LOT WHERE WORKSTATE='FAILED' AND DATE_FORMAT(CREATEDATE,'%Y%m') = DATE_FORMAT(CURDATE(),'%Y%m') GROUP BY LOTNO, MATERIALCODE ) fl ON fl.LOTNO=fd.LOTNO AND fl.MATERIALCODE=fd.MATERIALCODE GROUP BY fd.ymd ORDER BY SUM(COALESCE(fl.fail_qty,0)) DESC LIMIT 1;\n",
      "    label:\n",
      "\n",
      "SELECT fd.ymd AS '작업일자', SUM(fd.day_prod_qty) AS '총 생산수량', SUM(COALESCE(fl.fail_qty,0)) AS '총 불량수량', ROUND((SUM(COALESCE(fl.fail_qty,0))/NULLIF(SUM(fd.day_prod_qty),0))*100,2) AS '불량율 퍼센트(%)' FROM (SELECT A.LOTNO, A.MATERIALCODE, DATE_FORMAT(A.ACTUALTIME,'%Y%m%d') AS ymd, SUM(A.TRANSQTY) AS day_prod_qty FROM LOG_LOTTRANSLOG A WHERE A.TRANSACTIONNAME='FINISH' AND DATE_FORMAT(A.ACTUALTIME,'%Y%m')=DATE_FORMAT(CURDATE(),'%Y%m') GROUP BY A.LOTNO, A.MATERIALCODE, DATE_FORMAT(A.ACTUALTIME,'%Y%m%d')) fd LEFT JOIN (SELECT A.LOTNO, A.MATERIALCODE, SUM(A.QUEUEDQTY) AS fail_qty FROM VW_LOT A INNER JOIN UDT_MATERIAL SC ON A.MATERIALCODE=SC.MATERIALCODE WHERE A.WORKSTATE='FAILED' AND DATE_FORMAT(A.CREATEDATE,'%Y%m')=DATE_FORMAT(CURDATE(),'%Y%m') GROUP BY A.LOTNO, A.MATERIALCODE) fl ON fd.LOTNO=fl.LOTNO AND fd.MATERIALCODE=fl.MATERIALCODE GROUP BY fd.ymd ORDER BY 3 DESC LIMIT 1;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT DATE_FORMAT(sw.STARTEDTIME, '%Y%m%d') AS WORKDATE, HOUR(sw.STARTEDTIME) AS HOUR_OF_DAY, SUM(sw.STARTEDQTY) AS WIP_START_QTY FROM sgt_wiplot sw WHERE sw.LINENO = 'ML0000' AND sw.WORKSTATE = 'PROCESSING' AND DATE(sw.STARTEDTIME) = '2025-05-20' GROUP BY DATE_FORMAT(sw.STARTEDTIME, '%Y%m%d'), HOUR(sw.STARTEDTIME) ORDER BY HOUR_OF_DAY;\n",
      "    label:\n",
      "\n",
      "SELECT HOUR(sw.STARTEDTIME) AS WORK_HOUR, COUNT(DISTINCT sw.LOTNO) AS LOT_CNT, SUM(sw.STARTEDQTY) AS WIP_QTY FROM sgt_wiplot sw WHERE sw.LINENO = 'ML0000' AND sw.WORKSTATE = 'PROCESSING' AND DATE(sw.STARTEDTIME) = '2025-05-20' GROUP BY HOUR(sw.STARTEDTIME) ORDER BY WORK_HOUR;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "WITH CYCLE_DATA AS (SELECT f.LOTNO, TIMESTAMPDIFF(SECOND,c.create_time,f.ACTUALTIME) AS CYCLE_SEC FROM LOG_LOTTRANSLOG f LEFT JOIN (SELECT LOTNO,MIN(ACTUALTIME) AS create_time FROM LOG_LOTTRANSLOG WHERE TRANSACTIONNAME='CREATE' GROUP BY LOTNO) c ON c.LOTNO=f.LOTNO WHERE f.LINENO='ML0000' AND f.TRANSACTIONNAME='FINISH') SELECT CYCLE_SEC, COUNT(*) AS cnt FROM CYCLE_DATA GROUP BY CYCLE_SEC ORDER BY CYCLE_SEC;\n",
      "    label:\n",
      "\n",
      "WITH CYCLE_DATA AS (SELECT TIMESTAMPDIFF(SECOND,c.create_time,f.ACTUALTIME) AS CYCLE_SEC FROM LOG_LOTTRANSLOG f LEFT JOIN (SELECT LOTNO,MIN(ACTUALTIME) AS create_time FROM LOG_LOTTRANSLOG WHERE TRANSACTIONNAME='CREATE' GROUP BY LOTNO) c ON c.LOTNO=f.LOTNO WHERE f.LINENO='ML0000' AND f.TRANSACTIONNAME='FINISH') SELECT CASE WHEN CYCLE_SEC < 60 THEN '0-60s' WHEN CYCLE_SEC < 120 THEN '60-120s' WHEN CYCLE_SEC < 180 THEN '120-180s' WHEN CYCLE_SEC < 300 THEN '180-300s' WHEN CYCLE_SEC < 600 THEN '300-600s' ELSE '600s+' END AS CYCLE_RANGE, COUNT(*) AS LOT_COUNT FROM CYCLE_DATA GROUP BY CYCLE_RANGE ORDER BY CASE CYCLE_RANGE WHEN '0-60s' THEN 1 WHEN '60-120s' THEN 2 WHEN '120-180s' THEN 3 WHEN '180-300s' THEN 4 WHEN '300-600s' THEN 5 ELSE 6 END;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "WITH PARAM AS (SELECT CURDATE() AS TARGET_DATE), TMP AS (SELECT LOTNO, SUM(QUEUEDQTY) AS WIP_QTY FROM sgt_wiplot WHERE LINENO='ML0000' AND WORKSTATE='PROCESSING' AND DATE_FORMAT(QUEUEDTIME,'%Y%m%d') BETWEEN DATE_SUB((SELECT TARGET_DATE FROM PARAM),INTERVAL 1 DAY) AND (SELECT TARGET_DATE FROM PARAM) GROUP BY LOTNO), NOW AS (SELECT SUM(WIP_QTY) AS NOW_WIP FROM TMP), THEN AS (SELECT SUM(WIP_QTY) AS THEN_WIP FROM TMP) SELECT NOW.NOW_WIP AS NOW_WIP_QTY, THEN.THEN_WIP AS THEN_WIP_QTY FROM NOW CROSS JOIN THEN;\n",
      "    label:\n",
      "\n",
      "WITH PARAM AS ( SELECT NOW() AS NOW_TS ) SELECT CASE WHEN TIMESTAMPDIFF(HOUR, sw.STARTEDTIME, (SELECT NOW_TS FROM PARAM)) <= 24 THEN 'LAST_24H' ELSE 'OLDER' END AS AGE_BUCKET, SUM(sw.STARTEDQTY) AS WIP_QTY FROM sgt_wiplot sw, PARAM WHERE sw.LINENO = 'ML0000' AND sw.WORKSTATE = 'PROCESSING' AND sw.STARTEDTIME IS NOT NULL GROUP BY CASE WHEN TIMESTAMPDIFF(HOUR, sw.STARTEDTIME, (SELECT NOW_TS FROM PARAM)) <= 24 THEN 'LAST_24H' ELSE 'OLDER' END;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[0:10], label_lst[0:10]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77bc0b-135e-44bc-aa6f-6e105c9dcbc1",
   "metadata": {},
   "source": [
    "## 8. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7539771-6a09-4669-b828-697ff508d389",
   "metadata": {},
   "source": [
    "이번에는 LoRA Adapter를 merge하지 않은 기본 모델로 테스트 데이터에 대해서 인퍼런스해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "918de7bf-b690-460e-95af-46d74c189cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d1675acd5c4d74afaf394842b848a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "base_model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa95b03c-71e8-4453-8d97-31255c38b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "-- 요청하신 '올해 월별 전체 불량률(ppm)' 추이를 보여주는 쿼리는, 현재 제공된 스키마에 불량률 관련 데이터(예: 불량 수, 총 생산 수량, 불량 발생일시 등)가 포함되어 있지 않기 때문에, 불량률을 계산할 수 없습니다.\n",
      "\n",
      "불량률(ppm)을 계산하기 위해서는 다음과 같은 정보가 필요합니다:\n",
      "- 불량 발생 수\n",
      "- 총 생산 수량\n",
      "- 생산 일시 또는 월별 분류\n",
      "\n",
      "이러한 데이터는 제공된 테이블들(`log_lottranslog`, `vw_lot`, `udt_material`, `udt_process`, `udt_factoryline` 등)에 포함되어 있지 않으며, 불량 관련 로그나 품질 데이터 테이블이 없습니다.\n",
      "\n",
      "따라서, 요청하신 쿼리는 **데이터가 부족하여 생성할 수 없습니다**.\n",
      "    label:\n",
      "\n",
      "SELECT DATE_FORMAT(A.ACTUALTIME,'%Y%m') AS ym, ROUND((SUM(COALESCE(fl.fail_qty,0))/NULLIF(SUM(A.TRANSQTY),0))*1000000,2) AS ppm FROM LOG_LOTTRANSLOG A LEFT JOIN (SELECT LOTNO, MATERIALCODE, SUM(QUEUEDQTY) AS fail_qty FROM VW_LOT WHERE WORKSTATE='FAILED' AND DATE_FORMAT(CREATEDATE,'%Y')=DATE_FORMAT(CURDATE(),'%Y') GROUP BY LOTNO, MATERIALCODE) fl ON A.LOTNO=fl.LOTNO AND A.MATERIALCODE=fl.MATERIALCODE WHERE A.TRANSACTIONNAME='FINISH' AND DATE_FORMAT(A.ACTUALTIME,'%Y')=DATE_FORMAT(CURDATE(),'%Y') GROUP BY DATE_FORMAT(A.ACTUALTIME,'%Y%m') ORDER BY ym;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    SUM(l.createqty) AS cumulative_production\n",
      "FROM \n",
      "    vw_lot l\n",
      "    JOIN udt_factoryline f ON l.lineno = f.lineno\n",
      "WHERE \n",
      "    f.lineno = 'PORCHE' \n",
      "    AND l.createdate >= '2025-03-16 00:00:00.000'\n",
      "    AND l.createdate < '2025-03-32 00:00:00.000';\n",
      "    label:\n",
      "\n",
      "SELECT SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' AND DATE(ACTUALTIME) BETWEEN '2025-03-16' AND '2025-03-31';<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    DATE(TIMELOGGED) AS 일자,\n",
      "    SUM(TRANSQTY) AS WIP투입수량\n",
      "FROM \n",
      "    log_lottranslog\n",
      "WHERE \n",
      "    LINENO = 'ML0000'\n",
      "    AND TIMELOGGED >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)\n",
      "GROUP BY \n",
      "    DATE(TIMELOGGED)\n",
      "ORDER BY \n",
      "    일자 DESC;\n",
      "    label:\n",
      "\n",
      "WITH PARAM AS ( SELECT CURDATE() AS TARGET_DATE ) SELECT DATE_FORMAT(l.ACTUALTIME, '%Y%m%d') AS WORKDATE, SUM(l.TRANSQTY) AS WIP_INPUT_QTY FROM log_lottranslog l, PARAM WHERE l.LINENO = 'ML0000' AND l.TRANSACTIONNAME = 'WIP_START' AND DATE(l.ACTUALTIME) BETWEEN DATE_SUB(PARAM.TARGET_DATE, INTERVAL 29 DAY) AND PARAM.TARGET_DATE GROUP BY DATE_FORMAT(l.ACTUALTIME, '%Y%m%d') ORDER BY WORKDATE;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    m.MATERIALCODE,\n",
      "    m.MATERIALNAME,\n",
      "    SUM(l.TRANSQTY) AS total_trans_qty,\n",
      "    SUM(CASE WHEN l.TRANSACTIONNAME LIKE '작업%' THEN l.TRANSQTY ELSE 0 END) AS work_trans_qty,\n",
      "    COUNT(l.TRANSLOGID) AS total_transactions\n",
      "FROM \n",
      "    udt_material m\n",
      "LEFT JOIN \n",
      "    log_lottranslog l ON m.MATERIALCODE = l.MATERIALCODE\n",
      "GROUP BY \n",
      "    m.MATERIALCODE, m.MATERIALNAME\n",
      "ORDER BY \n",
      "    total_trans_qty DESC;\n",
      "    label:\n",
      "\n",
      "SELECT MATERIALCODE, SUM(TRANSQTY) / (2 * 8) AS WORKER_PROD FROM LOG_LOTTRANSLOG WHERE LINENO = 'ML0000' AND TRANSACTIONNAME = 'FINISH' GROUP BY MATERIALCODE ORDER BY WORKER_PROD DESC;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    m.MATERIALCODE,\n",
      "    m.MATERIALNAME,\n",
      "    SUM(l.TRANSQTY) AS total_trans_qty,\n",
      "    SUM(l.CURRENTQTY) AS total_current_qty,\n",
      "    SUM(l.NEXTQTY) AS total_next_qty\n",
      "FROM \n",
      "    log_lottranslog l\n",
      "JOIN \n",
      "    udt_material m ON l.MATERIALCODE = m.MATERIALCODE\n",
      "JOIN \n",
      "    udt_factoryline f ON l.LINENO = f.LINENO\n",
      "WHERE \n",
      "    f.LINENO LIKE 'POR%' \n",
      "    AND DATE_FORMAT(l.TIMELOGGED, '%Y-%m') = '2025-05'\n",
      "GROUP BY \n",
      "    m.MATERIALCODE, m.MATERIALNAME\n",
      "ORDER BY \n",
      "    (SUM(l.NEXTQTY) - SUM(l.CURRENTQTY)) / NULLIF(SUM(l.CURRENTQTY), 0) DESC\n",
      "LIMIT 5;\n",
      "    label:\n",
      "\n",
      "WITH FD AS (SELECT LOTNO, MATERIALCODE, SUM(TRANSQTY) AS FINISHSUM FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='FINISH' AND DATE_FORMAT(ACTUALTIME,'%Y%m')='202505' GROUP BY LOTNO, MATERIALCODE), SD AS (SELECT LOTNO, MATERIALCODE, SUM(TRANSQTY) AS STARTSUM FROM LOG_LOTTRANSLOG WHERE LINENO='ML0000' AND TRANSACTIONNAME='START' AND DATE_FORMAT(ACTUALTIME,'%Y%m')='202505' GROUP BY LOTNO, MATERIALCODE), MAT_YIELD AS (SELECT FD.MATERIALCODE, SUM(FD.FINISHSUM) AS TOTAL_FINISH_QTY, SUM(SD.STARTSUM) AS TOTAL_START_QTY, ROUND((SUM(FD.FINISHSUM)/NULLIF(SUM(SD.STARTSUM),0))*100,2) AS YIELD_PCT FROM FD INNER JOIN SD ON FD.LOTNO=SD.LOTNO AND FD.MATERIALCODE=SD.MATERIALCODE GROUP BY FD.MATERIALCODE) SELECT MATERIALCODE, TOTAL_FINISH_QTY, TOTAL_START_QTY, YIELD_PCT FROM MAT_YIELD ORDER BY YIELD_PCT DESC LIMIT 5;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    l.TRANSACTIONNAME,\n",
      "    AVG(l.ACTUALTIME - l.TIMELOGGED) AS AVG_CYCLE_TIME\n",
      "FROM \n",
      "    log_lottranslog l\n",
      "WHERE \n",
      "    l.ACTUALTIME >= DATE_SUB(CURDATE(), INTERVAL 1 MONTH)\n",
      "GROUP BY \n",
      "    l.TRANSACTIONNAME\n",
      "ORDER BY \n",
      "    AVG_CYCLE_TIME DESC;\n",
      "    label:\n",
      "\n",
      "SELECT DATE(f.ACTUALTIME) AS WORKDATE, AVG(TIMESTAMPDIFF(SECOND,c.create_time,f.ACTUALTIME)) AS AVG_CYCLE_SEC FROM LOG_LOTTRANSLOG f LEFT JOIN (SELECT LOTNO,MIN(ACTUALTIME) AS create_time FROM LOG_LOTTRANSLOG WHERE TRANSACTIONNAME='CREATE' GROUP BY LOTNO) c ON c.LOTNO=f.LOTNO WHERE f.LINENO='ML0000' AND f.TRANSACTIONNAME='FINISH' AND DATE(f.ACTUALTIME) >= DATE_SUB(CURDATE(),INTERVAL 1 MONTH) GROUP BY DATE(f.ACTUALTIME) ORDER BY WORKDATE;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    DATE(TIMELOGGED) AS defect_day\n",
      "FROM \n",
      "    log_lottranslog\n",
      "WHERE \n",
      "    TRANSACTIONNAME LIKE '%불량%'\n",
      "GROUP BY \n",
      "    DATE(TIMELOGGED)\n",
      "ORDER BY \n",
      "    COUNT(*) DESC\n",
      "LIMIT 1;\n",
      "    label:\n",
      "\n",
      "SELECT fd.ymd AS '작업일자', SUM(fd.day_prod_qty) AS '총 생산수량', SUM(COALESCE(fl.fail_qty,0)) AS '총 불량수량', ROUND((SUM(COALESCE(fl.fail_qty,0))/NULLIF(SUM(fd.day_prod_qty),0))*100,2) AS '불량율 퍼센트(%)' FROM (SELECT A.LOTNO, A.MATERIALCODE, DATE_FORMAT(A.ACTUALTIME,'%Y%m%d') AS ymd, SUM(A.TRANSQTY) AS day_prod_qty FROM LOG_LOTTRANSLOG A WHERE A.TRANSACTIONNAME='FINISH' AND DATE_FORMAT(A.ACTUALTIME,'%Y%m')=DATE_FORMAT(CURDATE(),'%Y%m') GROUP BY A.LOTNO, A.MATERIALCODE, DATE_FORMAT(A.ACTUALTIME,'%Y%m%d')) fd LEFT JOIN (SELECT A.LOTNO, A.MATERIALCODE, SUM(A.QUEUEDQTY) AS fail_qty FROM VW_LOT A INNER JOIN UDT_MATERIAL SC ON A.MATERIALCODE=SC.MATERIALCODE WHERE A.WORKSTATE='FAILED' AND DATE_FORMAT(A.CREATEDATE,'%Y%m')=DATE_FORMAT(CURDATE(),'%Y%m') GROUP BY A.LOTNO, A.MATERIALCODE) fl ON fd.LOTNO=fl.LOTNO AND fd.MATERIALCODE=fl.MATERIALCODE GROUP BY fd.ymd ORDER BY 3 DESC LIMIT 1;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    HOUR(t.STARTEDTIME) AS hour,\n",
      "    SUM(t.STARTEDQTY) AS started_qty\n",
      "FROM \n",
      "    sgt_wiplot t\n",
      "JOIN \n",
      "    udt_factoryline f ON t.LINENO = f.LINENO\n",
      "WHERE \n",
      "    f.LINENO LIKE '%포르쉐%'\n",
      "    AND t.STARTEDTIME >= '2025-05-20 00:00:00'\n",
      "    AND t.STARTEDTIME < '2025-05-21 00:00:00'\n",
      "GROUP BY \n",
      "    HOUR(t.STARTEDTIME)\n",
      "ORDER BY \n",
      "    hour;\n",
      "    label:\n",
      "\n",
      "SELECT HOUR(sw.STARTEDTIME) AS WORK_HOUR, COUNT(DISTINCT sw.LOTNO) AS LOT_CNT, SUM(sw.STARTEDQTY) AS WIP_QTY FROM sgt_wiplot sw WHERE sw.LINENO = 'ML0000' AND sw.WORKSTATE = 'PROCESSING' AND DATE(sw.STARTEDTIME) = '2025-05-20' GROUP BY HOUR(sw.STARTEDTIME) ORDER BY WORK_HOUR;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "SELECT \n",
      "    l.LOTNO,\n",
      "    l.LINENO,\n",
      "    l.TRANSACTIONNAME,\n",
      "    l.ACTUALTIME,\n",
      "    l.MATERIALCODE,\n",
      "    l.MATERIALNAME,\n",
      "    l.TRANSQTY,\n",
      "    l.CURRENTQTY,\n",
      "    l.NEXTQTY,\n",
      "    l.TRANSUOM,\n",
      "    l.WAREHOUSECODE,\n",
      "    l.BOPMATERIALCODE,\n",
      "    l.PROCESSCODE,\n",
      "    l.USERCODE,\n",
      "    (julianday(l.ACTUALTIME) - julianday(l.TIMELOGGED)) * 24 * 60 AS cycle_time_minutes\n",
      "FROM \n",
      "    log_lottranslog l\n",
      "JOIN \n",
      "    udt_factoryline f ON l.LINENO = f.LINENO\n",
      "WHERE \n",
      "    f.LINENO LIKE 'PORCHE%' \n",
      "    AND l.TRANSACTIONNAME = 'LOT PROCESSING'\n",
      "ORDER BY \n",
      "    cycle_time_minutes DESC;\n",
      "    label:\n",
      "\n",
      "WITH CYCLE_DATA AS (SELECT TIMESTAMPDIFF(SECOND,c.create_time,f.ACTUALTIME) AS CYCLE_SEC FROM LOG_LOTTRANSLOG f LEFT JOIN (SELECT LOTNO,MIN(ACTUALTIME) AS create_time FROM LOG_LOTTRANSLOG WHERE TRANSACTIONNAME='CREATE' GROUP BY LOTNO) c ON c.LOTNO=f.LOTNO WHERE f.LINENO='ML0000' AND f.TRANSACTIONNAME='FINISH') SELECT CASE WHEN CYCLE_SEC < 60 THEN '0-60s' WHEN CYCLE_SEC < 120 THEN '60-120s' WHEN CYCLE_SEC < 180 THEN '120-180s' WHEN CYCLE_SEC < 300 THEN '180-300s' WHEN CYCLE_SEC < 600 THEN '300-600s' ELSE '600s+' END AS CYCLE_RANGE, COUNT(*) AS LOT_COUNT FROM CYCLE_DATA GROUP BY CYCLE_RANGE ORDER BY CASE CYCLE_RANGE WHEN '0-60s' THEN 1 WHEN '60-120s' THEN 2 WHEN '120-180s' THEN 3 WHEN '180-300s' THEN 4 WHEN '300-600s' THEN 5 ELSE 6 END;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "SELECT \n",
      "    SUM(CASE WHEN startedtime >= DATE_SUB(NOW(), INTERVAL 24 HOUR) THEN startedqty ELSE 0 END) AS recent_24h_started_qty,\n",
      "    SUM(CASE WHEN startedtime < DATE_SUB(NOW(), INTERVAL 24 HOUR) THEN startedqty ELSE 0 END) AS previous_24h_started_qty\n",
      "FROM \n",
      "    sgt_wiplot\n",
      "WHERE \n",
      "    lineno = 'PORCHE' \n",
      "    AND workstate = 'PROCESSING';\n",
      "    label:\n",
      "\n",
      "WITH PARAM AS ( SELECT NOW() AS NOW_TS ) SELECT CASE WHEN TIMESTAMPDIFF(HOUR, sw.STARTEDTIME, (SELECT NOW_TS FROM PARAM)) <= 24 THEN 'LAST_24H' ELSE 'OLDER' END AS AGE_BUCKET, SUM(sw.STARTEDQTY) AS WIP_QTY FROM sgt_wiplot sw, PARAM WHERE sw.LINENO = 'ML0000' AND sw.WORKSTATE = 'PROCESSING' AND sw.STARTEDTIME IS NOT NULL GROUP BY CASE WHEN TIMESTAMPDIFF(HOUR, sw.STARTEDTIME, (SELECT NOW_TS FROM PARAM)) <= 24 THEN 'LAST_24H' ELSE 'OLDER' END;<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[0:10], label_lst[0:10]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
