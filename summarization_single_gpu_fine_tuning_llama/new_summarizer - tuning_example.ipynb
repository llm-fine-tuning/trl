{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b152fb-bcb6-40c7-8c93-32c90a55218e",
   "metadata": {},
   "source": [
    "이 실습은 Runpod에서 A100 SXM GPU 1개로 진행되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591659b-83e1-45d7-b166-2c85a2f74367",
   "metadata": {},
   "source": [
    "## 1. 필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00257c8-0292-4e9b-97e6-58e510517249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m171.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m189.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m183.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0 typing-extensions-4.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.8.93)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.0/334.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typeguard, tqdm, shtab, safetensors, requests, regex, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.30.2 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 regex-2024.11.6 requests-2.32.3 rich-14.0.0 safetensors-0.5.3 shtab-1.7.2 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.45.1 trl-0.11.1 typeguard-4.4.2 tyro-0.9.18 tzdata-2025.2 xxhash-3.5.0 yarl-1.19.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.4.0\"\n",
    "%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91394a-4b7e-415f-8a41-0ee783d41dc3",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fc9e67-2865-46c8-9d5d-99b5a227a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989235b0-b263-4b6a-8784-bd02b66bebe2",
   "metadata": {},
   "source": [
    "1. 허깅페이스 허브에서 데이터셋 로드\n",
    "- `load_dataset()` 함수를 호출하여 \"iamjoon/finance_news_summarizer\" 데이터셋의 \"train\" 분할을 로드합니다. 이 함수는 허깅페이스 datasets 라이브러리에서 제공하며, 지정된 데이터셋을 메모리로 불러옵니다.\n",
    "\n",
    "2. 데이터셋 크기 출력\n",
    "- `print(\"전체 데이터 크기:\", len(dataset))` 코드로 데이터셋의 총 샘플 수를 계산하여 콘솔에 출력합니다. 이는 로드된 데이터셋의 규모를 확인하는 단계입니다.\n",
    "\n",
    "3. train/test 분할 비율 설정\n",
    "- `test_ratio = 0.5`로 테스트 분할 비율을 50%로 설정합니다. 이는 전체 데이터 중 절반을 테스트용으로, 나머지 절반을 학습용으로 사용하겠다는 의미입니다.\n",
    "- 빈 리스트 `train_data`와 `test_data`를 초기화하여 각각 훈련 및 테스트 데이터의 인덱스를 저장할 준비를 합니다.\n",
    "\n",
    "4. 전체 데이터의 인덱스를 train/test로 분할\n",
    "- `data_indices = list(range(len(dataset)))` 코드로 0부터 데이터셋 길이-1까지의 인덱스 리스트를 생성합니다.\n",
    "- `test_size = int(len(data_indices) * test_ratio)`로 테스트 데이터셋의 샘플 수를 계산합니다.\n",
    "- `test_data = data_indices[:test_size]`로 인덱스 리스트의 앞부분을 테스트 데이터로 할당합니다.\n",
    "- `train_data = data_indices[test_size:]`로 인덱스 리스트의 뒷부분을 학습 데이터로 할당합니다.\n",
    "\n",
    "5. OpenAI format으로 데이터 변환을 위한 함수\n",
    "- `format_data()` 함수를 정의하여 각 샘플을 OpenAI API와 호환되는 메시지 형식으로 변환합니다.\n",
    "- 이 함수는 입력된 샘플에서 \"system_prompt\", \"user_prompt\", \"assistant\" 필드를 추출하여 \"messages\" 배열에 적절한 역할(role)과 함께 구성합니다.\n",
    "- 반환되는 형식은 OpenAI의 챗봇 API와 호환되는 형태로, 시스템 메시지, 사용자 메시지, 어시스턴트 응답을 포함합니다.\n",
    "\n",
    "6. 분할된 데이터를 OpenAI format으로 변환\n",
    "- `train_dataset = [format_data(dataset[i]) for i in train_data]` 코드로 학습용 인덱스에 해당하는 데이터를 OpenAI 형식으로 변환합니다.\n",
    "- `test_dataset = [format_data(dataset[i]) for i in test_data]` 코드로 테스트용 인덱스에 해당하는 데이터를 OpenAI 형식으로 변환합니다.\n",
    "- 리스트 컴프리헨션 방식을 사용하여 각 인덱스에 해당하는 데이터를 효율적으로 변환합니다.\n",
    "\n",
    "7. 최종 데이터셋 크기 출력\n",
    "- `print(f\"\\\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")` 코드로 변환된 학습 및 테스트 데이터셋의 크기를 출력합니다.\n",
    "- 이를 통해 데이터 분할과 변환 과정이 올바르게 완료되었는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef323ce1-aeb4-446d-a331-6468e83d42c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa52dad3d454806af856d8dabbea8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beeca299f9d407183355e9b6932ec97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8c68af4b214b00b233be36732f15b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 991\n",
      "\n",
      "전체 데이터 분할 결과: Train 496개, Test 495개\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/finance_news_summarizer\", split=\"train\")\n",
    "\n",
    "# 2. 전체 데이터 크기만 출력\n",
    "print(\"전체 데이터 크기:\", len(dataset))\n",
    "\n",
    "# 3. train/test 분할 비율 설정 (0.5면 5:5로 분할)\n",
    "test_ratio = 0.5\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 4. 전체 데이터의 인덱스를 train/test로 분할\n",
    "data_indices = list(range(len(dataset)))\n",
    "test_size = int(len(data_indices) * test_ratio)\n",
    "\n",
    "test_data = data_indices[:test_size]\n",
    "train_data = data_indices[test_size:]\n",
    "\n",
    "# 5. OpenAI format으로 데이터 변환을 위한 함수\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sample[\"system_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"user_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(sample[\"assistant\"])\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 6. 분할된 데이터를 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "# 7. 최종 데이터셋 크기 출력\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4390cd7e-a31c-414a-97d2-a4bd3cc83a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}'},\n",
       " {'role': 'user',\n",
       "  'content': '중소기업 하반기 경기전망 작년보다 악화…원자잿값 상승 우려\\n서울 연합뉴스 신선미 기자 중소기업의 올해 하반기 경기전망 지수가 지난해 같은 기간보다 하락한 것으로 나타났다. 5일 중소기업중앙회에 따르면 지난달 15∼24일 중소기업 500곳을 대상으로 실시한 중소기업 경영애로 및 2022년 하반기 경기전망조사 결과 하반기 경기전망지수 SBHI 는 87.6으로 지난해 하반기 91.6 보다 4.0포인트 p 하락했다. 이 지수가 100 이상이면 경기가 개선될 것으로 보는 응답자가 더 많고 100 미만이면 그 반대라는 의미다. 중기중앙회 중기중앙회 제공 하반기 SBHI를 업종별로 보면 제조업의 경우 펄프·종이 및 종이제품업 54.2 섬유제품업 54.2 전기장비업 68.2 은 경기가 악화될 것으로 내다봤고 기타 운송장비업 127.3 가죽·가방 및 신발업 104.6 은 경기가 호전될 것으로 전망했다. 서비스업에서는 부동산업 및 임대업 60.0 도매 및 소매업 84.0 은 경기가 악화될 것으로 봤지만 예술·스포츠 및 여가 관련 서비스업 112.0 은 업황 개선을 전망했다. 하반기 예상되는 애로 요인 복수응답 은 원자재 가격 상승 58.8% 내수 부진 31.2% 인력 수급난 29.8% 금리상승 28.4% 최저임금 상승 19.4% 등의 순이었다. 또 상반기 겪은 애로 요인으로는 원자재가격 상승 62.6% 내수부진 35.2% 인력 수급난 29.8% 금리상승 25.2% 최저임금 상승 22.8% 등의 순으로 응답률이 높았다. 소상공인·중소기업의 경기 개선을 위해 필요한 정부 정책 복수응답 으로는 세금 및 각종 부담금 인하 61.4% 금융지원 45.0% 인력난 해소 34.6% 원자재 수급 안정화 28.6% 근로시간 유연화 20.0% 순으로 꼽혔다. 코로나19 이전 수준의 경영실적 회복 예상 시기에 대해서는 응답자의 27.0%가 2024년 이후 라고 답했고 이어 2023년 상반기 와 2023년 하반기 각 23.0% 2022년 하반기 14.8% 등이었다.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'negative_keywords': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'positive_impact_stocks': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'positive_keywords': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'reason_for_negative_impact': '펄프·종이 및 종이제품업, 섬유제품업, 전기장비업, 부동산업 및 임대업, 도매 및 소매업은 하반기 경기 전망에서 경기가 악화될 것으로 예측되고 있다.', 'reason_for_positive_impact': '기타 운송장비업, 가죽·가방 및 신발업, 예술·스포츠 및 여가 관련 서비스업은 하반기 경기 전망에서 경기가 호전될 것으로 예측되고 있다.', 'summary': '중소기업중앙회 조사 결과, 올해 하반기 중소기업 경기 전망 지수가 작년보다 하락했다. 제조업에서는 펄프, 종이, 섬유, 전기장비업이 경기 악화를 예상했고, 운송장비와 가죽, 신발업이 호전을 전망했다. 원자재 가격 상승과 내수 부진 등이 주요 애로 요인으로, 경기 개선을 위해 세금 및 부담금 인하와 금융지원 등이 필요하다고 응답됐다.'}\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의의 345번 데이터 출력\n",
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc8f819-89cc-445a-999d-a6299a08d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1769a733-1ac3-411c-9fcd-0220aedd042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}',\n",
       "   'role': 'system'},\n",
       "  {'content': '이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\\n금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a52be8-3af6-44d8-bf39-5dc14319dc78",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212cd2a7-ac0b-4c8c-8926-c1d5d80f2171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d373265329134b2395eda7bdba78d094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400109d9655c44e3af59172b9c887e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152f5b676cf74024aebb0cfedbbd369d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a36ecadb68439697aac5d45be94807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204852c7aa8b4f50b1f9718e5653b2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438376e63be14ca0b72a68802db6d439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea3e382702445208538e170d38979cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a550caed98bf40d1a1c746b97cf15e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db08beff7b754e9195fba696263855db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fd0359cf9242b9861089c06c90b864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa31582533f4a728003880dfe6e550c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6e96a3-0d81-4e76-a385-d1c6e8146dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n",
      "금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801263cf-ac85-4e26-9fc6-dbd057552905",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2266ebe9-d3ce-4fed-956b-6a830acfe0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a625541-bf3d-49fc-9b91-0c49193b37dc",
   "metadata": {},
   "source": [
    "`lora_alpha`: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "\n",
    "`lora_dropout`: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. 0.1로 설정하면 학습 중 10%의 뉴런이 비활성화됩니다.\n",
    "\n",
    "`r`: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "\n",
    "`bias`: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. \"none\"으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. \"all\" 또는 \"lora_only\"와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "\n",
    "`target_modules`: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 \"q_proj\"와 \"v_proj\"를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "\n",
    "`task_type:` LoRA가 적용되는 작업 유형을 지정합니다. \"CAUSAL_LM\"은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 \"SEQ2SEQ_LM\"(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ae4257-f495-4a5a-a7e6-852455deb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"llama3-8b-summarizer-ko\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da90d2d-9e29-43e7-9c04-27e63f82d9a1",
   "metadata": {},
   "source": [
    "`utput_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\n",
    "`num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니다.\n",
    "\n",
    "`per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있습니다.\n",
    "\n",
    "`gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 2로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 유용합니다.\n",
    "\n",
    "`gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질 수 있습니다.\n",
    "\n",
    "`optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적화기를 사용합니다.\n",
    "\n",
    "`logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마다 로그를 기록합니다.\n",
    "\n",
    "`save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스텝마다 모델이 저장됩니다.\n",
    "\n",
    "`save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50 스텝마다 모델을 저장합니다.\n",
    "\n",
    "`bf16`: bfloat16 정밀도를 사용하도록 설정합니다. bfloat16은 FP32와 유사한 범위를 제공하면서 메모리와 계산 효율성을 높입니다.\n",
    "\n",
    "`learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 사용하여 안정적인 학습을 유도합니다.\n",
    "\n",
    "`max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 조정하여 폭발적 그래디언트를 방지합니다.\n",
    "\n",
    "`warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다. 학습의 안정성을 높이기 위해 사용됩니다.\n",
    "\n",
    "`lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"constant\"`는 학습률을 일정하게 유지합니다.\n",
    "\n",
    "`push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "\n",
    "`remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다. True로 설정하면 메모리를 절약할 수 있습니다.\n",
    "\n",
    "`dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset: True`로 설정하여 데이터셋 준비 단계를 건너뜁니다.\n",
    "\n",
    "`report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05ef38-fd56-4def-8569-9512bf56b81f",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0c36ca-5d28-4bdf-b27f-ca7ab63b4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "\n",
    "    for example in batch:\n",
    "        messages = example[\"messages\"]\n",
    "\n",
    "        # LLaMA 3 채팅 템플릿 적용 (시작 토큰 포함)\n",
    "        prompt = \"<|begin_of_text|>\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"].strip()\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}<|eot_id|>\"\n",
    "\n",
    "        # 마지막 assistant 메시지는 응답으로 간주하고 레이블에 포함\n",
    "        text = prompt.strip()\n",
    "\n",
    "        # 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "        # assistant 응답의 시작 위치 찾기\n",
    "        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n",
    "\n",
    "        # 레이블 범위 지정\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_tokens):\n",
    "            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n",
    "                start = i + len(assistant_tokens)\n",
    "                end = start\n",
    "                while end <= len(input_ids) - len(eot_tokens):\n",
    "                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start, end):\n",
    "                    labels[j] = input_ids[j]\n",
    "                for j in range(end, end + len(eot_tokens)):\n",
    "                    labels[j] = input_ids[j]  # <|eot_id|> 토큰도 포함\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "\n",
    "    # 패딩 처리\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        pad_len = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n",
    "        new_batch[\"labels\"][i].extend([-100] * pad_len)\n",
    "\n",
    "    for k in new_batch:\n",
    "        new_batch[k] = torch.tensor(new_batch[k])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e77712-7612-4c6c-a102-d0ff557d122f",
   "metadata": {},
   "source": [
    "입력으로 사용되는 라마 챗 템플릿은 아래와 같습니다.되어 반환됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094bc1d-d47e-4932-98ba-9d2f30949809",
   "metadata": {},
   "source": [
    "```python\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "시스템 프롬프트<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "유저 프롬프트<|eot_id|><|start_header_id|>assistant<|end_header|>LLM의 답변<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f99a67-f433-498f-a7db-31af0a9950aa",
   "metadata": {},
   "source": [
    "collate_fn(batch) 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 label_ids를 생성합니다. 레이블 데이터의 경우 실제 assistant 응답 부분을 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5cc839-5f65-44c8-b475-ac613fde97c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 2884])\n",
      "어텐션 마스크 형태: torch.Size([1, 2884])\n",
      "레이블 형태: torch.Size([1, 2884])\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 최대 길이 제한\n",
    "max_seq_length=8192\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1. 즉, 데이터 1개에 대해서 전처리를 진행해본다.)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7578d30e-fb04-4c6e-974d-a613e7cfa785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[128000, 128006, 9125, 128007, 198, 65895, 83628, 34804, 56773, 125441, 111068, 25941, 123103, 99458, 88708, 19954, 126652, 56773, 16969, 111068, 25941, 117469, 106478, 102517, 44005, 104193, 123061, 111068, 25941, 106478, 102517, 21121, 80052, 627, 103097, 109521, 111964, 127491, 122625, 119097, 108859, 101480, 93917, 101868, 56069, 13094, 168, 235, 105, 21028, 11240, 106612, 77437, 43139, 114839, 16582, 119978, 627, 65993, 108, 103386, 36092, 112, 102260, 109055, 19954, 105642, 103386, 36092, 112, 102260, 105880, 103607, 34609, 113348, 45618, 49085, 88525, 96677, 119978, 13, 127063, 11240, 56069, 113890, 18359, 62085, 99742, 102893, 105365, 102467, 112215, 124005, 13, 106725, 27796, 56773, 21028, 16582, 119978, 627, 54059, 54542, 11240, 57575, 106603, 907, 16969, 67890, 30426, 115790, 19954, 95713, 61938, 13, 67890, 56154, 115790, 18359, 106725, 103607, 22035, 100711, 119978, 13, 95713, 67890, 30426, 115790, 19954, 106725, 103607, 104834, 24486, 907, 18918, 104965, 103430, 76242, 96, 34609, 119978, 627, 34983, 65895, 115790, 13094, 47782, 115300, 122292, 81021, 55055, 108520, 122292, 84734, 17835, 103607, 32179, 90759, 109670, 13, 105813, 21028, 17835, 364, 123467, 6, 120908, 103607, 108503, 16969, 96270, 124005, 382, 73653, 103168, 95713, 111068, 122625, 103966, 30381, 99458, 88708, 7, 127702, 125543, 105797, 102662, 119222, 51796, 109745, 11, 103966, 30381, 99458, 88708, 7, 127702, 8, 81673, 111304, 103045, 78453, 101106, 13094, 108838, 111068, 25941, 33177, 50152, 102772, 116100, 81673, 109583, 114839, 61938, 382, 109659, 104449, 512, 5018, 285, 31641, 54356, 794, 3641, 345, 1, 1743, 794, 330, 58126, 21121, 102772, 95713, 111068, 120155, 87097, 103168, 97237, 87097, 103168, 123926, 114839, 16582, 119978, 64259, 73653, 103168, 95713, 111068, 122625, 103966, 30381, 99458, 88708, 7, 127702, 8, 115467, 78453, 101106, 109791, 109745, 11, 103966, 30381, 99458, 88708, 7, 127702, 8, 54780, 111304, 103045, 78453, 101106, 13094, 108838, 111068, 25941, 33177, 50152, 102772, 116100, 81673, 109583, 114839, 61938, 382, 109659, 104449, 512, 5018, 285, 31641, 54356, 794, 3082, 345, 1, 31587, 37888, 533, 1284, 26246, 794, 4482, 101508, 13094, 168, 235, 105, 81021, 55055, 84734, 21028, 106612, 87472, 17835, 23955, 111068, 122625, 41871, 235, 30381, 103684, 126652, 109720, 104349, 43139, 58935, 30381, 107205, 99458, 88708, 106001, 87134, 18359, 114839, 16582, 119978, 13, 106943, 26799, 17835, 103607, 109745, 110192, 80732, 43139, 103607, 22035, 100711, 119978, 13, 99458, 88708, 80732, 43139, 58935, 30381, 107205, 62398, 84391, 126546, 103607, 34609, 119978, 13, 111068, 25941, 123103, 58935, 30381, 48936, 29833, 65621, 127923, 24486, 115382, 101886, 94801, 43139, 103607, 34609, 119978, 13, 63207, 103168, 11, 119097, 88525, 110661, 115300, 122292, 84734, 17835, 114839, 16582, 119978, 1210, 1282, 1, 20489, 5595, 55260, 37888, 533, 794, 330, 82001, 21028, 99458, 88708, 102823, 95713, 111068, 25941, 123103, 41871, 235, 30381, 103684, 126652, 84696, 18359, 111590, 58935, 30381, 24486, 111436, 18918, 84618, 109509, 113631, 114839, 16582, 119978, 761, 1, 31587, 52454, 794, 4482, 18202, 235, 30381, 103684, 126652, 109720, 111590, 58935, 30381, 107205, 99458, 88708, 102823, 119097, 101528, 33390, 84618, 109509, 41871, 235, 30381, 103684, 126652, 56773, 103170, 106589, 93292, 20565, 120789, 101954, 120138, 24486, 104167, 56154, 108652, 103430, 30446, 105880, 56069, 13094, 168, 235, 105, 81021, 55055, 84734, 106612, 87472, 17835, 114839, 16582, 119978, 13, 113094, 80732, 11, 127798, 80732, 120908, 109580, 110097, 61938, 13, 107067, 100660, 104167, 56154, 112887, 108785, 27797, 61938, 13, 47782, 115300, 122292, 84734, 17835, 114839, 100660, 115106, 1210, 1282, 1, 43324, 37888, 533, 1284, 26246, 794, 4482, 101508, 13094, 168, 235, 105, 81021, 55055, 84734, 21028, 106612, 87472, 17835, 23955, 111068, 122625, 41871, 235, 30381, 103684, 126652, 109720, 104349, 43139, 58935, 30381, 107205, 99458, 88708, 105880, 114839, 16582, 119978, 13, 106943, 26799, 17835, 103607, 109745, 110192, 80732, 43139, 103607, 22035, 100711, 119978, 13, 99458, 88708, 80732, 43139, 58935, 30381, 107205, 62398, 84391, 126546, 103607, 34609, 119978, 13, 111068, 25941, 123103, 58935, 30381, 48936, 29833, 65621, 127923, 24486, 115382, 101886, 94801, 43139, 103607, 34609, 119978, 13, 63207, 103168, 11, 119097, 88525, 110661, 115300, 122292, 84734, 17835, 114839, 16582, 119978, 1210, 1282, 1, 20489, 5595, 54965, 37888, 533, 794, 330, 82001, 21028, 99458, 88708, 102823, 95713, 111068, 25941, 123103, 41871, 235, 30381, 103684, 126652, 84696, 18359, 111590, 58935, 30381, 24486, 111436, 18918, 84618, 109509, 113631, 114839, 16582, 119978, 761, 1, 43324, 52454, 794, 4482, 64189, 30381, 103684, 126652, 109720, 111590, 58935, 30381, 107205, 99458, 88708, 102823, 119097, 101528, 33390, 84618, 109509, 86503, 30381, 103684, 126652, 56773, 103170, 106589, 93292, 20565, 120789, 101954, 120138, 24486, 104167, 56154, 108652, 103430, 30446, 105880, 56069, 13094, 168, 235, 105, 81021, 55055, 84734, 106612, 87472, 17835, 114839, 16582, 119978, 13, 113094, 80732, 11, 127798, 80732, 120908, 109580, 110097, 61938, 13, 107067, 100660, 104167, 56154, 112887, 108785, 27797, 61938, 13, 47782, 115300, 122292, 84734, 17835, 114839, 100660, 115106, 1210, 1282, 1, 1743, 794, 330, 58126, 21121, 102772, 95713, 111068, 120155, 87097, 103168, 97237, 87097, 103168, 123926, 114839, 16582, 119978, 9388, 128009, 128006, 882, 128007, 198, 13094, 98934, 102335, 103236, 30446, 56154, 19954, 44215, 35495, 41953, 1981, 100981, 29102, 24486, 101603, 101096, 65677, 38187, 58083, 113110, 126015, 104019, 110513, 198, 101136, 103655, 55421, 41953, 84618, 66965, 56154, 12432, 105131, 102997, 62841, 101003, 58189, 33931, 58083, 115777, 104019, 1981, 114039, 103168, 87097, 32428, 102517, 62060, 71682, 110513, 107545, 103168, 101532, 55430, 106359, 101254, 101136, 29102, 110834, 1981, 107752, 82233, 104019, 108289, 58083, 113110, 126015, 102786, 110208, 66965, 116604, 101834, 101103, 1981, 60861, 101151, 101482, 101193, 96677, 103304, 23955, 98934, 102335, 104193, 123061, 103655, 104841, 55421, 41953, 13, 105429, 108785, 52688, 112003, 21121, 26799, 23955, 98934, 102335, 104193, 123061, 103655, 104841, 55421, 124788, 101003, 58189, 33931, 104019, 107545, 22035, 57575, 103123, 21121, 29833, 108964, 33931, 103686, 42771, 18918, 107472, 101480, 29102, 24486, 101603, 101096, 103686, 115096, 65677, 38187, 34983, 115087, 107387, 220, 20, 33177, 103153, 64189, 101528, 13, 23955, 104684, 103551, 111097, 102517, 107152, 64189, 109126, 55421, 29102, 101136, 57002, 66338, 71682, 106304, 423, 14899, 220, 18, 101353, 101015, 66610, 60798, 20565, 127245, 107205, 103659, 106725, 83719, 38187, 33931, 58083, 113110, 126015, 78102, 423, 14899, 115839, 63171, 104065, 109231, 19954, 102597, 29833, 36811, 20565, 122862, 48936, 29833, 65621, 63207, 118009, 58083, 115777, 104019, 19954, 106603, 102517, 101709, 101327, 66406, 3396, 109864, 104684, 105771, 49085, 127264, 101528, 13, 23955, 102467, 124788, 23955, 106223, 106010, 72043, 89359, 50467, 58189, 84618, 83628, 101136, 123061, 109567, 62841, 57575, 105069, 102423, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 106287, 66406, 101090, 26799, 12432, 110863, 21028, 105131, 102997, 62841, 57575, 101003, 58189, 33931, 58083, 115777, 19954, 106603, 102517, 24486, 125718, 18359, 89946, 56773, 21121, 82818, 103272, 13447, 13, 84618, 66965, 117396, 29833, 83628, 117211, 13094, 47782, 21121, 109644, 101003, 58189, 33931, 58083, 115777, 20565, 107120, 114213, 82068, 109816, 125959, 102612, 103684, 58083, 115777, 112373, 107022, 101015, 101266, 124983, 104019, 48936, 108289, 20565, 91786, 49208, 108, 107779, 80816, 24486, 111850, 101555, 21028, 101003, 58189, 111490, 103686, 42771, 44005, 62398, 104790, 103123, 21121, 29833, 108964, 33931, 103686, 42771, 18918, 107472, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 106593, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 16969, 65677, 38187, 83290, 56773, 21121, 82818, 103272, 13447, 101254, 108537, 13, 121856, 23955, 102467, 124788, 84618, 66965, 117396, 84618, 66965, 109126, 97096, 101066, 78102, 119262, 33931, 103213, 44966, 18359, 110155, 127002, 21028, 65677, 101136, 18359, 66610, 104684, 101360, 112795, 45618, 101711, 101136, 29102, 69508, 59134, 104303, 45618, 66610, 104684, 19954, 123851, 38389, 103618, 113610, 48936, 29833, 91786, 13, 103294, 65677, 101136, 103678, 27797, 118408, 33390, 57575, 36609, 101015, 67945, 71023, 34804, 126761, 104182, 107545, 103168, 24486, 95303, 102156, 13094, 106359, 101360, 119864, 67945, 71023, 34804, 108360, 125189, 101508, 13094, 121751, 113890, 29515, 62060, 71023, 78102, 86503, 58189, 86157, 107022, 102757, 19954, 104441, 101711, 33308, 120, 119567, 116492, 19954, 107138, 103655, 102893, 126652, 84696, 121969, 49208, 108, 84618, 66965, 115296, 65677, 101136, 93917, 104684, 14260, 94772, 27797, 57002, 103966, 24140, 33931, 43139, 107545, 103168, 87097, 32428, 102517, 17835, 112521, 101464, 24486, 62060, 71682, 20565, 108289, 108907, 101254, 109012, 13, 23955, 102467, 124788, 220, 2366, 15, 100392, 101327, 102757, 124141, 126906, 121296, 103185, 113360, 102249, 124141, 777, 113610, 122964, 84618, 66965, 109126, 80307, 107235, 101164, 30446, 20565, 103686, 67945, 65219, 104448, 84618, 66965, 109126, 126902, 97096, 101066, 13094, 112024, 57002, 72043, 101353, 106910, 116686, 72043, 44690, 102193, 84618, 66965, 117396, 29833, 74623, 100551, 63375, 101003, 58189, 33931, 106460, 17835, 19954, 105164, 33390, 24486, 82818, 91786, 49208, 108, 121066, 220, 21, 100551, 111323, 84618, 66965, 109126, 80307, 107235, 101164, 30446, 20565, 220, 2366, 15, 100392, 101003, 58189, 33931, 46810, 21121, 122964, 106287, 101838, 220, 6083, 18287, 117615, 59134, 62841, 116429, 65677, 101136, 93917, 104684, 84618, 101868, 13094, 127992, 115809, 57390, 116039, 91786, 101254, 109012, 13, 106237, 104448, 23955, 102467, 124788, 65677, 50643, 104182, 64432, 24140, 103684, 116492, 18359, 127271, 34983, 101003, 58189, 33931, 123706, 116273, 107573, 54289, 18918, 125744, 101360, 75086, 57002, 65677, 101136, 66610, 104684, 119623, 49085, 106327, 126403, 106313, 109070, 34983, 56773, 21121, 82818, 103272, 13447, 49208, 108, 69508, 103684, 62060, 71023, 102657, 103686, 112037, 106593, 62060, 55430, 55430, 109682, 101482, 101193, 101003, 57002, 102249, 26799, 65677, 101136, 111302, 78102, 103686, 42771, 120908, 110155, 63207, 110534, 54542, 86503, 109126, 18918, 65677, 50643, 104182, 59134, 66338, 48936, 29833, 123644, 107779, 80816, 24486, 111850, 101555, 21028, 101003, 58189, 33931, 103686, 42771, 20565, 108289, 108907, 101254, 102258, 93917, 101528, 13, 49508, 102275, 61394, 23955, 102467, 124788, 36609, 101015, 67945, 71023, 18359, 96270, 30381, 104182, 104019, 101360, 104423, 101272, 103402, 94, 24140, 122298, 116688, 103686, 112037, 44005, 103659, 49085, 104441, 101711, 34983, 104685, 105771, 103153, 64189, 101528, 13, 108154, 84618, 66965, 115296, 36609, 101015, 67945, 71023, 34804, 107545, 103168, 101532, 55430, 20565, 106359, 44005, 101254, 101136, 29102, 109231, 13094, 127002, 18359, 103213, 22035, 101360, 112795, 104193, 29102, 59134, 104303, 45618, 103521, 66965, 115602, 102678, 16582, 113191, 101834, 101103, 20565, 91786, 49208, 108, 107545, 103168, 101532, 55430, 19954, 102597, 101254, 101136, 29102, 62060, 71023, 107545, 102662, 45618, 103213, 109627, 59134, 66338, 122298, 29854, 19954, 107625, 16969, 62060, 71023, 107545, 102662, 93851, 101066, 13094, 37155, 111283, 113191, 29833, 123644, 125718, 18359, 89946, 56773, 30426, 21121, 82818, 103272, 13447, 101254, 109012, 13, 23955, 102467, 124788, 23955, 104684, 103551, 127245, 53400, 423, 14899, 220, 18, 101353, 101015, 66610, 60798, 111323, 103055, 101136, 125935, 83719, 38187, 33931, 58083, 113110, 126015, 78102, 423, 14899, 115839, 116464, 57575, 63171, 104065, 107205, 109231, 19954, 102597, 29833, 36811, 20565, 122862, 48936, 29833, 36439, 34609, 117622, 58083, 115777, 104019, 19954, 64432, 13447, 101327, 66406, 3396, 109864, 55430, 106103, 82818, 103272, 13447, 49208, 108, 125578, 104423, 101272, 103402, 94, 24140, 122298, 29854, 103686, 112037, 18359, 106958, 101412, 54542, 57519, 105115, 18359, 64432, 24140, 104182, 66980, 34983, 62060, 111270, 112037, 65895, 101136, 18359, 107779, 80816, 101709, 103607, 102365, 48936, 108289, 20565, 91786, 101254, 101012, 100, 44852, 247, 102563, 13, 23955, 102467, 124788, 119864, 67945, 71023, 13094, 103966, 30381, 107022, 102757, 19954, 105613, 101711, 119222, 51796, 108438, 84618, 83628, 102612, 56154, 101824, 33229, 102252, 101106, 106064, 102258, 57390, 34983, 109720, 124859, 104064, 29854, 101528, 13, 108154, 84618, 66965, 117396, 104219, 93292, 220, 605, 100392, 63375, 102678, 101136, 29102, 55216, 93917, 101824, 44215, 108955, 106213, 57390, 17835, 29515, 62060, 71023, 78102, 86503, 58189, 86157, 107022, 102757, 18359, 122169, 43139, 119864, 67945, 71023, 18359, 103686, 67945, 34983, 119929, 102772, 101254, 101314, 101096, 100981, 65677, 86157, 18359, 84415, 54780, 102893, 73444, 112039, 113295, 27796, 113469, 86503, 58189, 86157, 112994, 16582, 105316, 19954, 102597, 101834, 101103, 20565, 118957, 106313, 18359, 125714, 34983, 62060, 71023, 107545, 102662, 45618, 110038, 42771, 101438, 13094, 116548, 104965, 100981, 59134, 66338, 122298, 29854, 46810, 55430, 17835, 84618, 83628, 102612, 111636, 107973, 62060, 71023, 107545, 102662, 111323, 102772, 103213, 109627, 101327, 27797, 82001, 102005, 124476, 84618, 64189, 18918, 56773, 21121, 104182, 106313, 109070, 48936, 108289, 20565, 91786, 101254, 108537, 13, 121856, 23955, 102467, 124788, 84618, 66965, 56154, 101266, 124983, 119864, 58126, 83628, 106213, 56154, 101824, 33229, 102252, 101106, 106064, 102258, 57390, 101360, 119262, 116492, 115809, 57390, 19954, 62060, 71682, 34983, 62060, 111270, 112037, 65895, 101136, 69508, 103607, 102365, 109018, 110671, 115954, 56773, 30426, 21121, 82818, 103272, 13447, 49208, 108, 104193, 103655, 55421, 34804, 107036, 29515, 62060, 71023, 19954, 102597, 115888, 33931, 116090, 18918, 125744, 44005, 78102, 119864, 67945, 71023, 62085, 225, 250, 18918, 106313, 109070, 101360, 55925, 99901, 18918, 82818, 120378, 43139, 107022, 101015, 81673, 119864, 58126, 83628, 106213, 56154, 101824, 33229, 102252, 106974, 55170, 108860, 105633, 102611, 117615, 96677, 103304, 48936, 119623, 23955, 105771, 109012, 13, 23955, 102467, 124788, 124141, 777, 109682, 113052, 99458, 64356, 78102, 19954, 62060, 71682, 24486, 107545, 103168, 101532, 55430, 109682, 109018, 125718, 18359, 103153, 64189, 101528, 13, 108154, 84618, 66965, 114333, 65677, 50643, 107065, 72043, 32428, 118711, 126709, 54059, 120916, 78102, 104965, 100981, 93917, 30381, 109682, 113052, 18359, 120952, 34983, 84656, 30426, 104182, 102888, 100981, 82068, 46230, 97, 66406, 19954, 72747, 24486, 103213, 55430, 20565, 66610, 109509, 48918, 101096, 19954, 107067, 113047, 48936, 29833, 123644, 103607, 110616, 103684, 109682, 18359, 121121, 30446, 102423, 13447, 49208, 108, 104350, 34983, 220, 23, 123096, 127798, 102517, 104193, 29102, 32428, 16582, 36811, 89359, 103131, 107065, 101272, 82068, 100994, 30426, 20565, 127245, 65219, 117622, 116534, 103603, 102258, 57390, 120908, 110155, 101327, 27797, 123360, 74623, 101151, 53400, 116534, 21028, 104193, 29102, 64189, 102997, 13094, 44215, 103655, 113191, 29833, 123644, 110187, 125718, 18359, 89946, 56773, 30426, 21121, 82818, 103272, 13447, 101254, 102258, 93917, 101528, 13, 106237, 104448, 23955, 102467, 124788, 119929, 106359, 101136, 106446, 13094, 122862, 44005, 83719, 38187, 33931, 58083, 113110, 126015, 34804, 107545, 103168, 101532, 109627, 59134, 66338, 86503, 102997, 18359, 84656, 30426, 104182, 109720, 58126, 115087, 29833, 65621, 102027, 101838, 13094, 36439, 102077, 104193, 44690, 101661, 57002, 104193, 123061, 125399, 19954, 95713, 88525, 51796, 54059, 102786, 110208, 66965, 116604, 19954, 102597, 101834, 101103, 20565, 65621, 124859, 112024, 23955, 51440, 101203, 104193, 103655, 55421, 34804, 104193, 123061, 82001, 114080, 62841, 81673, 106999, 104193, 123061, 44690, 71682, 26799, 109969, 108964, 63171, 35495, 18918, 106958, 58083, 113110, 126015, 114942, 27796, 101327, 102546, 107545, 103168, 101532, 55430, 127287, 45618, 61816, 102477, 111411, 125744, 104193, 29102, 105178, 30381, 67236, 101577, 103603, 104193, 29102, 100994, 30426, 56773, 21121, 103123, 106734, 122733, 74623, 101151, 101482, 101193, 18359, 96677, 103304, 72043, 19954, 91786, 13, 106603, 103236, 30446, 56154, 12432, 127737, 49085, 74623, 101151, 101482, 101193, 96677, 103304, 57519, 102704, 116534, 19954, 102597, 114942, 101412, 102735, 94, 78102, 43139, 59777, 34983, 102786, 110208, 66965, 116604, 20565, 113610, 88525, 51796, 108438, 65677, 50643, 104182, 104019, 18918, 102258, 57390, 34983, 56773, 30426, 106647, 103153, 64189, 30446, 102423, 13447, 101254, 109012, 13, 23955, 102467, 124788, 84618, 66965, 101096, 101015, 44215, 108955, 29854, 102258, 117216, 107472, 111850, 38187, 107123, 57390, 78102, 126950, 82068, 109682, 18359, 49508, 110833, 22035, 51796, 103373, 105453, 118183, 49085, 116283, 35859, 104828, 13, 108154, 105638, 22035, 117419, 57519, 66338, 45618, 124784, 107625, 13094, 34983, 104193, 123061, 101096, 54780, 75086, 101136, 123061, 101096, 21028, 44215, 101015, 20565, 108785, 101438, 32179, 119073, 103924, 13, 125578, 84618, 66965, 117396, 102558, 227, 102953, 82233, 81673, 21028, 44215, 108955, 106213, 57390, 17835, 84618, 101109, 107022, 102757, 107988, 123851, 108699, 19954, 72747, 34983, 36439, 34609, 117622, 116453, 102132, 41953, 58189, 116688, 97096, 108214, 48936, 29833, 123644, 109682, 16582, 125684, 49208, 108, 105638, 22035, 117419, 57519, 66338, 58935, 42529, 18918, 125714, 34983, 24839, 116, 101090, 101824, 86503, 24140, 101096, 100981, 21028, 115483, 82001, 84618, 66965, 101096, 102517, 107545, 102662, 96451, 107022, 100981, 21028, 50152, 104193, 123061, 101096, 54780, 78453, 101106, 53400, 115888, 19954, 122115, 16969, 104193, 123061, 82001, 19954, 103686, 124784, 103521, 21028, 16582, 125684, 13, 103294, 123102, 102464, 71023, 45618, 109018, 104193, 103655, 123645, 123102, 103315, 29726, 126709, 18918, 120952, 83290, 84618, 66965, 115296, 106460, 17835, 115790, 18359, 61816, 44690, 48936, 29833, 123644, 110671, 113026, 125684, 101254, 108537, 13, 110154, 43139, 23955, 102467, 124788, 104193, 123061, 30426, 41953, 116492, 13094, 103123, 117534, 19954, 74623, 101151, 119222, 120903, 111590, 96717, 57002, 65219, 117622, 122352, 92143, 102735, 94, 18359, 120693, 58083, 115777, 104019, 81673, 104193, 123061, 44690, 71682, 26799, 126110, 19954, 104441, 101711, 34983, 56773, 30426, 106647, 103153, 64189, 30446, 102423, 13447, 49208, 108, 104193, 103655, 55421, 49085, 84618, 66965, 101096, 101015, 81673, 122352, 105711, 101709, 101228, 102233, 116429, 104414, 101096, 64189, 112343, 44215, 108955, 29854, 102258, 117216, 106958, 106434, 111850, 38187, 18918, 74623, 101151, 101360, 62085, 248, 101, 33931, 63171, 35495, 18918, 107472, 102058, 29854, 49085, 67890, 102130, 48936, 72208, 23955, 105771, 109012, 13, 128009, 128006, 78191, 128007, 198, 13922, 285, 31641, 54356, 1232, 3082, 11, 364, 43324, 37888, 533, 1284, 26246, 1232, 2570, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 518, 364, 101436, 30446, 56154, 4181, 364, 43324, 52454, 1232, 2570, 101314, 58189, 33931, 58083, 115777, 518, 364, 29102, 113110, 126015, 518, 364, 35495, 82001, 102005, 65677, 86157, 518, 364, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 4181, 364, 31587, 37888, 533, 1284, 26246, 1232, 10277, 364, 31587, 52454, 1232, 10277, 364, 20489, 5595, 54965, 37888, 533, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 101003, 58189, 33931, 58083, 115777, 104019, 18918, 102258, 93917, 108859, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 102258, 117216, 67890, 30426, 24486, 110005, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 81673, 103236, 30446, 56154, 114026, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 103924, 13, 125578, 11, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 101824, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 65677, 38187, 20565, 126999, 65219, 104448, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 109720, 29833, 103924, 16045, 364, 20489, 5595, 55260, 37888, 533, 1232, 9158, 364, 1743, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 103236, 30446, 56154, 81673, 84618, 83628, 66965, 52688, 101136, 123061, 62841, 111636, 116464, 43139, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 18918, 103153, 64189, 108859, 11, 101003, 58189, 33931, 58083, 115777, 81673, 107545, 103168, 101532, 55430, 62060, 71023, 19954, 102597, 56773, 21028, 18918, 102258, 93917, 101528, 13, 127063, 95713, 104193, 123061, 56154, 106001, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 91786, 3238, 92, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8d7425-6d62-44af-b726-689ca29c6d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_ids 디코딩 결과:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n",
      "금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# 디코딩된 input_ids 출력\n",
    "decoded_text = tokenizer.decode(\n",
    "    batch[\"input_ids\"][0].tolist(),\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\ninput_ids 디코딩 결과:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ec99e0-5360-40b2-b622-b8c7d2110454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 13922, 285, 31641, 54356, 1232, 3082, 11, 364, 43324, 37888, 533, 1284, 26246, 1232, 2570, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 518, 364, 101436, 30446, 56154, 4181, 364, 43324, 52454, 1232, 2570, 101314, 58189, 33931, 58083, 115777, 518, 364, 29102, 113110, 126015, 518, 364, 35495, 82001, 102005, 65677, 86157, 518, 364, 58126, 83628, 66965, 52688, 101136, 123061, 127702, 4181, 364, 31587, 37888, 533, 1284, 26246, 1232, 10277, 364, 31587, 52454, 1232, 10277, 364, 20489, 5595, 54965, 37888, 533, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 101003, 58189, 33931, 58083, 115777, 104019, 18918, 102258, 93917, 108859, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 102258, 117216, 67890, 30426, 24486, 110005, 84618, 83628, 66965, 52688, 101136, 123061, 127702, 81673, 103236, 30446, 56154, 114026, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 103924, 13, 125578, 11, 101254, 82001, 102005, 65677, 86157, 103686, 67945, 101824, 101480, 29102, 24486, 101603, 101096, 103686, 41953, 65677, 38187, 20565, 126999, 65219, 104448, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 109720, 29833, 103924, 16045, 364, 20489, 5595, 55260, 37888, 533, 1232, 9158, 364, 1743, 1232, 364, 101136, 123061, 103655, 104841, 55421, 114784, 103236, 30446, 56154, 81673, 84618, 83628, 66965, 52688, 101136, 123061, 62841, 111636, 116464, 43139, 101480, 29102, 24486, 101603, 101096, 65677, 38187, 81673, 58083, 113110, 126015, 104019, 18918, 103153, 64189, 108859, 11, 101003, 58189, 33931, 58083, 115777, 81673, 107545, 103168, 101532, 55430, 62060, 71023, 19954, 102597, 56773, 21028, 18918, 102258, 93917, 101528, 13, 127063, 95713, 104193, 123061, 56154, 106001, 29833, 108964, 33931, 19954, 86503, 30381, 103684, 126652, 101412, 119866, 29833, 91786, 3238, 92, 128009]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6938e31-3109-42b9-ae14-8895c087649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "labels 디코딩 결과 (-100 제외):\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# -100이 아닌 부분만 골라 디코딩\n",
    "label_ids = [token_id for token_id in batch[\"labels\"][0].tolist() if token_id != -100]\n",
    "\n",
    "decoded_labels = tokenizer.decode(\n",
    "    label_ids,\n",
    "    skip_special_tokens=False,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(\"\\nlabels 디코딩 결과 (-100 제외):\")\n",
    "print(decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e849a2-066c-4453-8c85-f997cdaa3e26",
   "metadata": {},
   "source": [
    "## 5. 어텐션 마스크 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1339de-ac4c-46d7-8081-73a538a89c73",
   "metadata": {},
   "source": [
    "### 배치 크기의 의미\n",
    "\n",
    "배치 크기란 모델이 한 번에 학습하는 데이터 샘플의 수를 의미합니다. 예를 들어 배치 크기가 3이면, 모델은 세 개의 데이터 샘플을 동시에 처리합니다. 이렇게 병렬적으로 학습하면 계산 효율성이 높아지고 학습 속도가 빨라지는 이점이 있습니다.\n",
    "\n",
    "### 텍스트 데이터의 길이 문제\n",
    "\n",
    "자연어 처리에서 각 샘플(문장, 대화 등)은 길이가 다양합니다. 예를 들어 배치 크기 3인 경우:\n",
    "\n",
    "- 샘플1: \"인공지능이란 무엇인가요?\" → [101, 4089, 8024, 6356, 102] (5 토큰)\n",
    "- 샘플2: \"오늘 날씨가 정말 좋네요.\" → [101, 3157, 2533, 4120, 2642, 8730, 6824, 102] (8 토큰)\n",
    "- 샘플3: \"딥러닝 모델을 학습시키는 방법을 알려주세요.\" → [101, 2982, 3478, 4567, 2053, 8276, 5036, 2355, 4602, 7312, 102] (11 토큰)\n",
    "\n",
    "여기서 101과 102는 특수 토큰으로, 각각 문장의 시작과 끝을 표시합니다.\n",
    "\n",
    "### 패딩의 필요성\n",
    "\n",
    "신경망의 내부 연산은 고정된 크기의 입력을 요구합니다. 이 문제를 해결하기 위해 '패딩'을 사용하여 모든 샘플의 길이를 배치 내 가장 긴 샘플에 맞춥니다.\n",
    "\n",
    "위 예시에서는 가장 긴 샘플3(11 토큰)에 맞춰 다른 샘플들에 패딩(0)을 추가합니다:\n",
    "\n",
    "- 샘플1: [101, 4089, 8024, 6356, 102, 0, 0, 0, 0, 0, 0] (5 실제 + 6 패딩)\n",
    "- 샘플2: [101, 3157, 2533, 4120, 2642, 8730, 6824, 102, 0, 0, 0] (8 실제 + 3 패딩)\n",
    "- 샘플3: [101, 2982, 3478, 4567, 2053, 8276, 5036, 2355, 4602, 7312, 102] (11 실제 토큰)\n",
    "\n",
    "이렇게 하면 모든 샘플이 동일한 길이(11)를 가지게 되어 하나의 배치로 처리할 수 있습니다.\n",
    "\n",
    "### 어텐션 마스크의 필요성\n",
    "\n",
    "패딩을 추가하면 모델이 어떤 토큰이 실제 내용이고 어떤 토큰이 의미 없는 패딩인지 구분해야 합니다. 이를 위해 '어텐션 마스크'를 사용합니다:\n",
    "\n",
    "- 샘플1 마스크: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "- 샘플2 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
    "- 샘플3 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "트랜스포머 모델의 어텐션 메커니즘은 이 마스크를 사용하여 패딩 토큰을 무시하고 실제 의미 있는 토큰에만 집중합니다. 이렇게 하면 패딩된 부분이 모델의 예측이나 학습에 영향을 미치지 않게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c70f56b-7eac-4e3d-8635-c363539bfccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번 데이터 길이: 2884\n",
      "1번 데이터 길이: 974\n",
      "\n",
      "배치 처리 후:\n",
      "입력 ID 형태: torch.Size([2, 2884])\n",
      "어텐션 마스크 형태: torch.Size([2, 2884])\n",
      "0번 샘플 어텐션 마스크 합계: 2884\n",
      "1번 샘플 어텐션 마스크 합계: 974\n",
      "\n",
      "0번과 1번 샘플의 어텐션 마스크가 다른가요? True\n",
      "\n",
      "0번 샘플 어텐션 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1번 샘플 어텐션 마스크: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "배치 내 최대 길이: 2884\n",
      "0번 샘플 어텐션 마스크 합계 (실제 토큰 수): 2884\n",
      "1번 샘플 어텐션 마스크 합계 (실제 토큰 수): 974\n",
      "0번 샘플 어텐션 마스크 1의 개수: 2884\n",
      "0번 샘플 어텐션 마스크 0의 개수: 0\n",
      "1번 샘플 어텐션 마스크 1의 개수: 974\n",
      "1번 샘플 어텐션 마스크 0의 개수: 1910\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 최대 길이 제한\n",
    "max_seq_length = 8192\n",
    "# 0번과 1번 데이터의 길이 확인\n",
    "example0 = train_dataset[0]\n",
    "example1 = train_dataset[1]\n",
    "# 개별 길이 확인 (토큰화 후)\n",
    "tokenized0 = tokenizer(\n",
    "    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n",
    "    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\" for msg in example0[\"messages\"]]),\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    "    padding=False,\n",
    "    return_tensors=None,\n",
    ")\n",
    "tokenized1 = tokenizer(\n",
    "    # 전체 처리 과정과 동일하게 전체 대화를 토큰화\n",
    "    \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n{msg['content'].strip()}<|eot_id|>\" for msg in example1[\"messages\"]]),\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    "    padding=False,\n",
    "    return_tensors=None,\n",
    ")\n",
    "print(f\"0번 데이터 길이: {len(tokenized0['input_ids'])}\")\n",
    "print(f\"1번 데이터 길이: {len(tokenized1['input_ids'])}\")\n",
    "# 배치로 처리하여 어텐션 마스크 비교\n",
    "batch = collate_fn([example0, example1])\n",
    "print(\"\\n배치 처리 후:\")\n",
    "print(f\"입력 ID 형태: {batch['input_ids'].shape}\")\n",
    "print(f\"어텐션 마스크 형태: {batch['attention_mask'].shape}\")\n",
    "# 각 샘플의 어텐션 마스크 합계 (실제 토큰 수 확인)\n",
    "print(f\"0번 샘플 어텐션 마스크 합계: {batch['attention_mask'][0].sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 합계: {batch['attention_mask'][1].sum().item()}\")\n",
    "# 0번 샘플과 1번 샘플의 어텐션 마스크가 다른지 확인\n",
    "masks_different = not torch.equal(batch['attention_mask'][0], batch['attention_mask'][1])\n",
    "print(f\"\\n0번과 1번 샘플의 어텐션 마스크가 다른가요? {masks_different}\")\n",
    "# 어텐션 마스크 패턴 시각화 (처음 20개와 마지막 20개 토큰)\n",
    "print(\"\\n0번 샘플 어텐션 마스크:\", batch['attention_mask'][0].tolist())\n",
    "print(\"1번 샘플 어텐션 마스크:\", batch['attention_mask'][1].tolist())\n",
    "# 배치 내에서 가장 긴 시퀀스 길이 구하기\n",
    "max_length_in_batch = max(len(tokenized0['input_ids']), len(tokenized1['input_ids']))\n",
    "print(f\"\\n배치 내 최대 길이: {max_length_in_batch}\")\n",
    "print(f\"0번 샘플 어텐션 마스크 합계 (실제 토큰 수): {batch['attention_mask'][0].sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 합계 (실제 토큰 수): {batch['attention_mask'][1].sum().item()}\")\n",
    "print(f\"0번 샘플 어텐션 마스크 1의 개수: {batch['attention_mask'][0].sum().item()}\")\n",
    "print(f\"0번 샘플 어텐션 마스크 0의 개수: {(batch['attention_mask'][0] == 0).sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 1의 개수: {batch['attention_mask'][1].sum().item()}\")\n",
    "print(f\"1번 샘플 어텐션 마스크 0의 개수: {(batch['attention_mask'][1] == 0).sum().item()}\")\n",
    "# 결과 검증: 긴 샘플은 모든 어텐션 마스크가 1이고, 짧은 샘플은 일부만 1이어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02034c-61ba-4fe6-84dd-408fd27d6fed",
   "metadata": {},
   "source": [
    "## 6. 전처리 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d7d24-f600-4de6-82a5-4ae4469fe260",
   "metadata": {},
   "source": [
    "**input_ids와 labels는 어떻게 생성되는가?**\n",
    "\n",
    "LLM 학습에서 `input_ids`와 `labels`는 모델의 학습 목표에 따라 생성됩니다. 시스템 프롬프트까지 포함하여 설명하겠습니다.\n",
    "\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다:\n",
    "- 시스템 프롬프트: `당신은 친절하고 도움이 되는 AI 어시스턴트입니다.`\n",
    "- 사용자 메시지: `안녕하세요, 오늘 날씨는 어떤가요?`\n",
    "- 어시스턴트 응답: `안녕하세요! 오늘 날씨는 맑고 화창합니다.`\n",
    "\n",
    "LLaMA 3에서는 다음과 같은 템플릿 구조를 사용합니다(줄바꿈 포함):\n",
    "\n",
    "```python\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 친절하고 도움이 되는 AI 어시스턴트입니다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "안녕하세요, 오늘 날씨는 어떤가요?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "안녕하세요! 오늘 날씨는 맑고 화창합니다.<|eot_id|>\n",
    "```\n",
    "\n",
    "이 전체 텍스트는 토크나이저에 의해 정수 시퀀스로 변환해봅시다.  \n",
    "(실제와 다르고 가정하여 정수를 맵핑하겠습니다.)\n",
    "\n",
    "먼저 모든 특수 토큰들은 아래의 고유 ID를 가진다고 가정해봅시다.  \n",
    "- <|begin_of_text|> = 토큰 ID 1\n",
    "- <|start_header_id|> = 토큰 ID 2\n",
    "- <|end_header_id|> = 토큰 ID 4\n",
    "- 줄바꿈 = 토큰 ID 5\n",
    "- <|eot_id|> = 토큰 ID 10\n",
    "\n",
    "역할 토큰들은 아래의 고유 ID를 가진다고 가정해봅시다.  \n",
    "- system = 토큰 ID 3\n",
    "- user = 토큰 ID 11\n",
    "- assistant = 토큰 ID 18\n",
    "\n",
    "전체 통합된 input_ids는 다음과 같습니다:\n",
    "`input_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 11, 4, 5, 12, 13, 14, 15, 16, 17, 10, 2, 18, 4, 5, 19, 20, 21, 22, 23, 10]`\n",
    "\n",
    "각 부분을 분리하면:\n",
    "- 시스템 프롬프트 부분: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- 사용자 메시지 부분: [2, 11, 4, 5, 12, 13, 14, 15, 16, 17, 10]\n",
    "- 어시스턴트 응답 부분: [2, 18, 4, 5, 19, 20, 21, 22, 23, 10]\n",
    "\n",
    "모델이 예측해야 할 영역은 assistant의 응답 부분인 `안녕하세요! 오늘 날씨는 맑고 화창합니다.`에 해당하는 토큰들입니다. 따라서 `labels`는 다음과 같이 설정됩니다:\n",
    "\n",
    "`labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19, 20, 21, 22, 23, 10]`\n",
    "\n",
    "여기서 주목할 점:\n",
    "1. 시스템 프롬프트와 사용자 메시지에 해당하는 모든 토큰(줄바꿈 포함)은 `-100`으로 마스킹됩니다.\n",
    "2. 어시스턴트 헤더와 첫 줄바꿈 토큰도 `-100`으로 마스킹됩니다.\n",
    "3. 실제 어시스턴트 응답 내용(19-23)과 마지막 종료 태그(10)만 원래 토큰 ID를 유지합니다.\n",
    "\n",
    "이처럼 `labels`는 모델이 실제로 생성해야 할 출력 부분만을 포함하고, 나머지 부분은 `-100`으로 채워져 손실 계산에서 제외됩니다. 이를 통해 모델은 입력(시스템 프롬프트+사용자 질문)을 기반으로 적절한 응답을 생성하는 방법을 학습합니다.\n",
    "\n",
    "학습 과정에서는:\n",
    "1. 모델에 `input_ids` 전체를 입력으로 제공합니다.\n",
    "2. 모델은 각 위치에서 다음 토큰을 예측합니다.\n",
    "3. 손실 계산 시 `labels`가 `-100`이 아닌 위치에서만 오차를 계산합니다.\n",
    "4. 이를 통해 모델은 주어진 맥락(시스템 프롬프트와 사용자 질문)에 대해 적절한 응답을 생성하는 방법을 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc5ff1-ea33-4ef2-a1dc-a6efd97cbbe4",
   "metadata": {},
   "source": [
    "## 7. 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe99cff0-d149-4ed5-8e8d-b62642165aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493c51a9-0a8e-4ac1-b5f9-7575b1005fb3",
   "metadata": {},
   "source": [
    "- **학습 데이터 크기**: 496개\n",
    "\n",
    "모델 학습에 사용되는 전체 데이터 샘플의 수입니다. 이는 각 에포크마다 처리되는 전체 데이터 양을 의미합니다.\n",
    "\n",
    "- **에포크(epochs)**: 3회\n",
    "\n",
    "전체 데이터셋을 처음부터 끝까지 반복해서 학습하는 횟수입니다. 즉, 모든 학습 데이터를 3번 반복해서 모델이 학습한다는 의미입니다.\n",
    "\n",
    "- **배치 크기(batch size)**: 2\n",
    "\n",
    "한 번에 처리하는 데이터 샘플의 수입니다. 메모리 효율을 위해 전체 데이터를 작은 배치로 나누어 처리하며, 여기서는 2개씩 묶어서 처리합니다.\n",
    "\n",
    "- **누적 단계(accumulation steps)**: 2\n",
    "\n",
    "모델을 실제로 업데이트하기 전에 여러 배치의 정보를 모으는 수입니다. 여기서는 2개의 배치(총 4개의 샘플)를 처리한 후에야 실제 모델 업데이트가 일어납니다.\n",
    "\n",
    "- **에포크 1회당 업데이트 횟수**: 496 ÷ (2 × 2) = 124회\n",
    "\n",
    "한 에포크에서 모델이 업데이트되는 횟수입니다. 전체 데이터 496개를 유효 배치 크기 4(배치 크기 2 × 누적 단계 2)로 나누면 124번의 업데이트가 발생합니다.\n",
    "\n",
    "- **총 업데이트 계산 방법**: (데이터 크기 × 에포크) ÷ (배치 크기 × 누적 단계)\n",
    "\n",
    "학습 과정 전체에서 발생하는 모델 업데이트의 총 횟수를 계산하는 공식입니다. 전체 처리 샘플 수를 유효 배치 크기로 나눕니다.\n",
    "\n",
    "- **총 업데이트 계산 과정**: (496 × 3) ÷ (2 × 2) = 1,488 ÷ 4 = 372\n",
    "\n",
    "3개의 에포크 동안 총 1,488개의 샘플이 처리되고, 유효 배치 크기인 4개의 샘플마다 한 번씩 모델이 업데이트되므로 총 372번의 모델 업데이트가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44eff24a-0f63-4d91-9c89-c15410fce0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 16:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.849300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.594100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.421700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.507600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.403400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a78dac-8258-41f6-9e1a-b049ca8f0ba9",
   "metadata": {},
   "source": [
    "## 8. 테스트 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e8254-77c8-4a54-9132-131c432e9505",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 `<|start_header_id|>assistant<|end_header_id|>\\n`가 부착되어서 넣는 것이 좋습니다. 그러면 모델이 조금 더 안정적으로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e1e2b4-7181-416a-8014-1018144ce6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f56d478b-4e31-4369-9ba1-b3861a8cc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "HUG 국민·우리은행 등 표준 PF 주관금융사로 재선정\n",
      "주택도시보증공사 HUG 는 주택사업자의 원활한 자금조달을 지원하는 표준 프로젝트 파이낸싱 PF 과 후분양 표준 PF 보증의 주관금융사를 새로 선정했다고 4일 밝혔다. PF 보증이란 주택 건설 사업의 미래 현금수입과 사업성을 담보로 주택 사업자가 대출받는 토지비 등 사업비에 대한 주택사업금융의 원리금 상환을 책임지는 보증을 말한다. 후분양 PF 보증이란 주택 사업자가 주택의 일부를 후분양하는 조건으로 주택 건설 자금 대출금을 조달하는 경우에 원리금 상환을 책임지는 보증이다. HUG는 2014년 제도를 시행한 이후 표준 PF 보증 약 12조6000억원 후분양 표준 PF 보증 약 8600억원을 지원했다. 이번에 새로 선정된 주관금융사는 표준 PF의 경우 국민은행 부산은행 수협은행 우리은행 하나은행이다. 후분양 표준 PF는 부산은행 우리은행 수협은행이다. 권형택 HUG 사장은 “최근 원자재가격 급등 대출금리 인상 등 비용증가로 어려움을 겪는 주택사업자에게 저금리 금융지원을 통하여 비용을 경감시킬 수 있게 된 점을 뜻깊게 생각한다”며 “표준PF 후분양 표준PF 제도운영을 통하여 주택공급 확대를 통한 부동산 시장 안정 주거안정 지원에 큰 도움이 될 것으로 전망한다”고 밝혔다.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "157493fd-bab0-4d18-a2d0-e85eab42e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['국민은행', '우리은행', '부산은행', '수협은행', '하나은행'], 'positive_keywords': ['HUG', '표준 프로젝트 파이낸싱', '주택사업금융', '후분양'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '이 뉴스는 HUG가 주택사업자에게 자금조달을 지원하는 표준 PF 및 후분양 표준 PF 보증의 주관금융사로 선정되었다는 내용으로, 이는 관련 은행들에게 주택건설 관련 금융 지원 기회를 제공함으로써 긍정적인 영향을 미칠 수 있다.', 'summary': 'HUG가 주택사업자 자금조달 지원을 위한 표준 프로젝트 파이낸싱의 주관금융사로 국민은행, 우리은행, 부산은행, 수협은행, 하나은행을 선정하여 저금리 금융지원을 통해 주택 사업자의 비용 증가 부담 완화에 기여할 예정이다.'}\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a825b-89c3-4288-9c97-2c3c7f01ad16",
   "metadata": {},
   "source": [
    "## 9. 파인튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c34fb6-6b3a-4d67-b9b7-4a28167f5181",
   "metadata": {},
   "source": [
    "`AutoPeftModelForCausalLM()`의 입력으로 LoRA Adapter가 저장된 체크포인트의 주소를 넣으면 LoRA Adapter가 기존의 LLM과 부착되어 로드됩니다. 이 과정은 LoRA Adapter의 가중치를 사전 학습된 언어 모델(LLM)에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n",
    "\n",
    "`peft_model_id` 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. `\"llama3-8b-summarizer-ko/checkpoint-372\"`는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서 해당 가중치를 불러옵니다.\n",
    "\n",
    "`fine_tuned_model`은 `AutoPeftModelForCausalLM.from_pretrained` 메서드를 통해 체크포인트를 로드하여 생성됩니다. 이 메서드는 LLM과 LoRA Adapter를 결합하고, 최적화된 설정으로 모델을 메모리에 로드합니다. `device_map=\"auto\"` 옵션은 모델을 자동으로 GPU에 배치합니다.\n",
    "\n",
    "`pipeline`은 Hugging Face의 고수준 유틸리티로, NLP 작업(예: 텍스트 생성, 번역, 요약 등)을 간단히 수행할 수 있게 해줍니다. 이 코드에서 사용된 `pipeline(\"text-generation\")`은 텍스트 생성 작업을 수행하기 위한 파이프라인 객체를 생성합니다. 파이프라인은 내부적으로 모델과 토크나이저를 관리하여, 입력 텍스트를 토큰화하고, 모델을 통해 생성된 결과를 다시 디코딩하여 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "\n",
    "이 코드는 미세 조정된 LLM을 로드한 뒤, 이를 이용해 텍스트 생성 작업을 간단히 수행할 수 있도록 준비하는 데 목적이 있습니다. `pipeline`을 통해 텍스트 생성 작업을 실행하면, 입력 텍스트에 기반하여 모델이 다음 토큰을 예측하고 이를 반복적으로 생성합니다. 이 과정은 사용자에게 자연스러운 텍스트를 출력하는 데 사용됩니다.데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "043328ae-7ad0-41c3-b6c6-aedab0b8ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7dac6d4d-2012-4603-9a62-da33a253e29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4b18883b2f41a39a55f5c682d93aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"llama3-8b-summarizer-ko/checkpoint-372\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1190ab71-0f39-44c3-9f89-55c0ab4b23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0e846a9-3b68-4ba3-846e-e8f54b4667c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cd6da67-6672-4dc4-af0e-88052b7144ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC'], 'negative_keywords': ['스마트폰 판매 감소', '애플', '엔비디아', 'TSMC'], 'positive_impact_stocks': [], 'positive_keywords': [],'reason_for_negative_impact': '가트너가 전세계 스마트폰 판매량이 7% 감소할 것으로 전망하면서 애플과 같은 스마트폰 제조사 및 엔비디아와 TSMC와 같은 반도체 업체에 부정적인 영향을 미칠 것으로 예상됩니다.','reason_for_positive_impact': '','summary': '가트너는 전세계 스마트폰 판매량이 7% 감소할 것으로 전망하며, 이는 애플과 같은 스마트폰 제조사 및 엔비디아와 TSMC와 같은 반도체 업체에 부정적인 영향을 미칠 것으로 예상됩니다. 또한, 유럽연합은 가상자산을 이용한 돈세탁을 막기 위해 관련 기업을 규제하는 방안에 잠정 합의했으며, 스피릿 항공은 프론티어 항공과의 합병안에 대한 주주투표를 연기했습니다. 텐센트와 바이트댄스는 하반기 대규모 구조조정을 준비하고 있습니다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자', '포커스미디어'], 'positive_keywords': ['야놀자', '포커스미디어', '동네가게 오래함께 캠페인', '지역 내 우수 소상공인', '맞춤형 광고', '지역경제 활성화'],'reason_for_negative_impact': '','reason_for_positive_impact': '야놀자와 포커스미디어가 협력하여 지역 내 우수 소상공인을 발굴하고 홍보하는 캠페인을 진행함으로써 지역경제 활성화에 기여할 수 있을 것으로 예상되며, 이는 양사의 브랜드 이미지와 매출 증대에 긍정적인 영향을 미칠 수 있다.','summary': '야놀자와 포커스미디어가 지역 내 우수 소상공인을 발굴하고 홍보하는 캠페인을 진행한다. 양사는 14억 원 규모의 광고 제작 및 송출 비용을 부담하며, 지역경제 활성화에 기여할 것으로 기대된다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['삼성바이오로직스', 'MSD', '위탁생산 공급계약', '의약품'],'reason_for_negative_impact': '','reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증가 가능성이 높아졌기 때문입니다.','summary': '삼성바이오로직스는 미국 제약기업 MSD와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증가 가능성을 높였다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['LG전자', 'SM엔터테인먼트', '피트니스 캔디', '디지털 피트니스', 'K POP'],'reason_for_negative_impact': '','reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 '피트니스 캔디'라는 디지털 피트니스 콘텐츠 브랜드를 발표하여, 디지털 피트니스 시장에서의 경쟁력을 강화할 수 있을 것으로 보입니다. 이는 두 회사의 브랜드 가치와 시장 점유율에 긍정적인 영향을 미칠 수 있습니다.\",'summary': \"LG전자와 SM엔터테인먼트가 '피트니스 캔디'라는 디지털 피트니스 콘텐츠 브랜드를 발표하여, 디지털 피트니스 시장에서의 경쟁력을 강화하고자 합니다. 이 브랜드는 MZ세대를 겨냥한 모바일 기반의 운동 데이터 플랫폼으로, K POP의 댄스와 리듬을 리마스터링한 콘텐츠를 제공할 예정입니다.\"}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', '공기질 관리', '스마트폰 앱'],'reason_for_negative_impact': '','reason_for_positive_impact': '경동나비엔이 장마철에도 실내 미세먼지 관리를 위한 청정환기시스템을 출시하여 시장에 새로운 제품을 제공하고 있으며, 이는 고객들의 건강과 쾌적한 생활을 위한 요구에 부응하고 있어 긍정적인 영향을 줄 것으로 예상된다.','summary': '경동나비엔은 장마철에도 실내 미세먼지를 관리할 수 있는 청정환기시스템을 출시하여 건강한 생활 환경을 제공하고 있다. 이 시스템은 공기청정과 청정환기를 동시에 구현하여 창문을 열지 않고도 실내 공기질을 개선할 수 있다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84388175-c0da-432a-b4a0-5333ae1c7feb",
   "metadata": {},
   "source": [
    "## 10. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49751b0-4dc2-43a7-a771-314e012b65c1",
   "metadata": {},
   "source": [
    "이번에는 LoRA Adapter를 merge하지 않은 기본 모델로 테스트 데이터에 대해서 인퍼런스해보겠습니다.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29c4c78d-50d0-4aeb-911b-989d691a1101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91a3e2306ec49788d0488750eabd468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0adeb550-b6c2-4783-b174-344e695a3641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "답변:\n",
      "{\n",
      "\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"애플\", \"엔비디아\", \"TSMC\"],\n",
      "\"reason_for_positive_impact\": \"가트너는 스마트폰 수요 감소로 인해 애플과 같은 스마트폰 제조사, 그리고 반도체 공급 업체인 엔비디아와 TSMC에 압력이 가해질 것이라고 전망했습니다.\",\n",
      "\"positive_keywords\": [\"애플\", \"엔비디아\", \"TSMC\", \"스마트폰 제조사\", \"반도체 공급 업체\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"이번 뉴스에서는 부정적인 영향을 받을 특정 종목에 대한 언급이 없습니다.\",\n",
      "\"negative_keywords\": [],\n",
      "\"summary\": \"시장조사업체 가트너는 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망했습니다. 이는 경제 전반의 침체, 중국의 봉쇄 조치, 그리고 인플레이션으로 인한 소비자 지출 감소 때문이라고 설명했습니다. 특히 중국 시장에서의 판매 감소가 크게 예상되며, 이는 애플과 같은 스마트폰 제조사 및 엔비디아, TSMC와 같은 반도체 공급 업체에 부담을 줄 것으로 보입니다.\"\n",
      "}\n",
      "\n",
      "이 뉴스는 스마트폰 판매량 감소에 대한 가트너의 전망을 다루고 있습니다. 이는 애플과 같은 스마트폰 제조사 및 엔비디아와 TSMC와 같은 반도체 공급 업체에 긍정적 영향을 미칠 수 있음을 시사합니다. \n",
      "\n",
      "'positive_impact_stocks'에는 애플, 엔비디아, TSMC의 이름이 포함되어 있으며,'reason_for_positive_impact'에서는 가트너가 스마트폰 수요 감소로 인해 이들 회사에 압력이 가해질 것이라고 전망했다는 내용을 설명하고 있습니다.\n",
      "\n",
      "'positive_keywords'에는 애플, 엔비디아, TSMC와 관련된 키워드들이 포함되어 있습니다.\n",
      "\n",
      "반면 'negative_impact_stocks'에는 언급된 종목이 없으며,'reason_for_negative_impact'에서는 부정적인 영향을 받을 특정 종목에 대한 언급이 없다고 명시하고 있습니다.\n",
      "\n",
      "'negative_keywords' 역시 빈 리스트입니다.\n",
      "\n",
      "'summary'에서는 가트너의 전망과 그 원인, 그리고 이에 따른 잠재적 영향에 대해 요약하고 있습니다. \n",
      "\n",
      "따라서 이 뉴스는 주식 시장에 긍정적인 영향을 미칠 수 있는 내용을 담고 있기 때문에 'is_stock_related'의 값은 True로 설정하였습니다.\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "답변:\n",
      "{\n",
      "\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"야놀자\", \"포커스미디어\"],\n",
      "\"reason_for_positive_impact\": \"야놀자와 포커스미디어가 동네가게 오래함께 캠페인을 진행함으로써, 이 두 회사가 상호 협력과 마케팅 확대를 통해 각각의 브랜드 인지도를 높이고 매출을 증가시킬 수 있을 것으로 보입니다.\",\n",
      "\"positive_keywords\": [\"야놀자\", \"포커스미디어\", \"동네가게 오래함께 캠페인\", \"지역 내 우수 소상공인\", \"맞춤형 광고\", \"홍보 지원\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"본문에 해당 주식에 부정적인 영향을 미칠 내용은 언급되지 않았습니다.\",\n",
      "\"negative_keywords\": []\n",
      "}\n",
      "\n",
      "이 뉴스는 야놀자와 포커스미디어 두 회사가 '동네가게 오래함께' 캠페인을 진행한다는 내용을 다루고 있습니다. 이 캠페인은 두 회사가 지역 내 우수 소상공인 발굴과 홍보를 통해 상생과 지역경제 활성화에 기여하려는 의도를 보여줍니다. \n",
      "\n",
      "따라서 야놀자와 포커스미디어 두 회사가 이 캠페인으로 인해 브랜드 인지도 상승과 매출 증대를 기대할 수 있다고 판단하였습니다. \n",
      "\n",
      "그러나 캠페인의 구체적인 성과를 예측하기는 어렵기 때문에, 단기적으로는 두 회사의 주가에 큰 변동을 가져오지 않을 것으로 보입니다. \n",
      "\n",
      "따라서 'positive_impact_stocks'에는 야놀자와 포커스미디어를 포함하고, 'negative_impact_stocks'에는 아무런 종목을 포함하지 않았습니다.'reason_for_positive_impact'와 'positive_keywords'도 캠페인 내용에 기반하여 작성하였습니다.'reason_for_negative_impact'과 'negative_keywords'는 본문에 부정적인 영향을 주는 내용이 없기 때문에 빈 리스트로 작성하였습니다. \n",
      "\n",
      "마지막으로'summary'에는 뉴스의 주요 내용을 간략하게 요약하였습니다.\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\n",
      "\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"삼성바이오로직스\"],\n",
      "\"reason_for_positive_impact\": \"삼성바이오로직스는 미국 제약기업 MSD와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결하였습니다. 이는 최근 매출액 대비 17.65% 규모로, 2022년 7월부터 2028년 12월까지 지속될 예정이며, 고객사의 수요증가에 따라 계약금액이 증가할 수 있습니다.\",\n",
      "\"positive_keywords\": [\"삼성바이오로직스\", \"MSD\", \"의약품 위탁생산 공급계약\", \"2768억2938만원\", \"2022년 7월\", \"2028년 12월\", \"수요증가\"],\n",
      "\"negative_impact_stocks\": [],\n",
      "\"reason_for_negative_impact\": \"이 뉴스에서는 삼성바이오로직스에만 긍정적인 영향을 미치는 내용이 언급되었습니다.\",\n",
      "\"negative_keywords\": [],\n",
      "\"summary\": \"삼성바이오로직스는 4일 공시를 통해 미국 제약기업 MSD와 2768억2938만원 규모의 의약품 위탁생산 공급계약을 체결했다고 밝혔습니다. 이번 계약은 2022년 7월부터 2028년 12월까지 지속되며, 고객사의 수요증가에 따라 계약금액이 증가할 수 있습니다.\"\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "답변:\n",
      " \n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표했습니다. 이 브랜드는 MZ세대를 겨냥한 모바일 기반의 차세대 운동 데이터 플랫폼으로, LG전자의 디지털 기술력과 고객 데이터 경영 경험을 바탕으로 개인의 즐거운 피트니스 참여를 위한 맞춤형 커뮤니티 서비스를 제공할 예정입니다. '피트니스 캔디'는 K POP의 댄스와 리듬을 리마스터링한 디지털 라이프스타일 무브먼트를 주력 콘텐츠로 삼고, 단순한 다이어트 운동을 넘어 건전하고 건강한 피트니스 습관을 재발견하는 디지털 피트니스 콘텐츠 프로젝트입니다.\"}\n",
      "\n",
      " \n",
      "이 뉴스는 LG전자와 SM엔터테인먼트의 합작 브랜드인 '피트니스 캔디'의 출시에 관한 내용을 다루고 있습니다. 특정 종목(회사)에 대한 언급이 없으므로 'is_stock_related'를 False로 설정했습니다. \n",
      "\n",
      "'피트니스 캔디' 브랜드의 주요 특징과 서비스 내용을 요약하여'summary' 항목에 작성했습니다. \n",
      "\n",
      "LG전자의 디지털 기술력과 고객 데이터 경영 경험을 바탕으로 개인 맞춤형 피트니스 서비스를 제공할 것이라는 점, K POP의 댄스와 리듬을 활용한 디지털 라이프스타일 무브먼트를 주력 콘텐츠로 삼는다는 점, 건전하고 건강한 피트니스 습관을 재발견하는 목표 등을 포함하였습니다. \n",
      "\n",
      "이 뉴스는 주로 LG전자와 SM엔터테인먼트의 사업 확장에 초점을 맞추고 있으므로, 금융 시장에 직접적인 영향을 주지 않을 것으로 보입니다. 따라서 'is_stock_related'를 False로 설정한 것입니다.\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\n",
      "\"is_stock_related\": False,\n",
      "\"summary\": \"미세먼지와 코로나19로 인해 창문을 닫고 있는 상황에서 실내 공기질 관리가 중요해지고 있다. 경동나비엔은 장마철에도 청정환기시스템을 통해 실내 미세먼지를 관리할 수 있는 솔루션을 제안하고 있다. 이 시스템은 공기청정과 청정환기를 동시에 수행하며, UV LED 모듈을 포함해 다양한 필터를 통해 미세먼지와 세균, 바이러스를 제거한다. 주방에서는 '특화 덕트'와 '3D 에어후드'를 적용한 '키친플러스' 제품이 사용되며, 이는 요리 중 발생하는 미세먼지를 효과적으로 제거한다. 연구 결과에 따르면 '키친플러스' 사용 시 주방과 거실의 초미세먼지 농도가 각각 평균 54%, 70% 감소했다.\"\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
