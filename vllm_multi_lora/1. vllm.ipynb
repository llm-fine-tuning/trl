{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qIqe_4r0TUc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "540f58772ebc4067b95aff0869daf87e",
            "78b4760434ac4e0cafd2d6f0b2db5e7c",
            "65c9c6465fc44f84b50c62bd57fe7cd1",
            "d981fdbb6e8747c99b6925eb2e17eff5",
            "a10b8764ba454edf8d15253c9f85885c",
            "ca25f4df7e724be9bd594664c9282336",
            "8bed8991af4847ac835871b4e9c0502a",
            "fb6c3ad5518f4d47882f79b223e5aac9",
            "84c877893220462f8e6635517c8cee05",
            "9a629bba7ff444f78eee559bff2580a7",
            "c2f3111132d94e0aa2d7cf32398bfb8f"
          ]
        },
        "id": "YRu3jRkuAlHo",
        "outputId": "95467528-8903-4725-b975-08d720209b70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING 08-15 08:23:20 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
            "INFO 08-15 08:23:20 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
            "INFO 08-15 08:23:20 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='daje/meta-llama3.1-8B-qna-koalpaca-v1.1', speculative_config=None, tokenizer='daje/meta-llama3.1-8B-qna-koalpaca-v1.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=daje/meta-llama3.1-8B-qna-koalpaca-v1.1, use_v2_block_manager=False, enable_prefix_caching=False)\n",
            "INFO 08-15 08:23:21 model_runner.py:720] Starting to load model daje/meta-llama3.1-8B-qna-koalpaca-v1.1...\n",
            "INFO 08-15 08:23:21 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "540f58772ebc4067b95aff0869daf87e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-15 08:23:28 model_runner.py:732] Loading model weights took 14.9888 GB\n",
            "INFO 08-15 08:23:29 gpu_executor.py:102] # GPU blocks: 9997, # CPU blocks: 2048\n",
            "INFO 08-15 08:23:31 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 08-15 08:23:31 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 08-15 08:23:47 model_runner.py:1225] Graph capturing finished in 16 secs.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "from vllm import LLM, SamplingParams\n",
        "import huggingface_hub\n",
        "\n",
        "huggingface_hub.login(token=\"Your_Huggingface_Token\")\n",
        "\n",
        "llm = LLM(model=\"daje/meta-llama3.1-8B-qna-koalpaca-v1.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpGAeG0KGWsK",
        "outputId": "9bbd761d-0751-46bf-d895-c9d7c2de77d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 10.73it/s, est. speed input: 67.07 toks/s, output: 203.90 toks/s]\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"\uc548\ub155 \ub0b4 \uc774\ub984\uc740\",\n",
        "    \"\ud55c\uad6d\uc758 \ub300\ud1b5\ub839\uc740 \",\n",
        "    \"\ub300\ud55c\ubbfc\uad6d\uc758 \uc218\ub3c4\ub294 \ud604\uc7ac\",\n",
        "    \"AI\uc758 \ubbf8\ub798\ub294\",\n",
        "]\n",
        "sampling_params = SamplingParams(temperature=0.9, top_p=0.95, max_tokens=20)\n",
        "outputs = llm.generate(prompts, sampling_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjwO-Li4CHLk",
        "outputId": "c5528968-2476-4732-b769-cabae55e4336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: '\uc548\ub155 \ub0b4 \uc774\ub984\uc740', Generated text: ' \uc774\ud61c\uc601\uc785\ub2c8\ub2e4. \uc81c\uac00 \uc624\ub298 \uccab \ube14\ub85c\uadf8\ub97c \uc791\uc131\ud558\ub294\ub370,\uc608\uc804 TV\uc5d0\uc11c'\n",
            "Prompt: '\ud55c\uad6d\uc758 \ub300\ud1b5\ub839\uc740 ', Generated text: '5\ub144\ub9c8\ub2e4 \uc120\uac70\ub97c \ud558\uc9c0\ub9cc, \uc784\uae30\ub97c \uc911\ub3c4\uc5d0\uc11c \uc870\uae30 \ud3d0\uc9c0\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.'\n",
            "Prompt: '\ub300\ud55c\ubbfc\uad6d\uc758 \uc218\ub3c4\ub294 \ud604\uc7ac', Generated text: ' \uc11c\uc6b8\uc778\ub370, \uc774\uc804\uc5d0 \ub2e4\ub978 \uc9c0\uc5ed\uc5d0\uc11c \uc218\ub3c4\uac00 \ubc30\uc815\ub418\uc5c8\ub358 \uc801\uc774 \uc788\ub098\uc694? \uadf8\ub807'\n",
            "Prompt: 'AI\uc758 \ubbf8\ub798\ub294', Generated text: ' \uc5b4\ub5bb\uac8c \ub420\uae4c\uc694? \uc778\ub958\uc5d0\uac8c \ubbf8\uce58\ub294 \uc601\ud5a5\uc740 \ubb34\uc5c7\uc778\uac00\uc694?'\n"
          ]
        }
      ],
      "source": [
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DL3eBlxDKi96",
        "outputId": "af0e1c57-ce61-4a9b-b812-9fd95b260803"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.21it/s, est. speed input: 81.33 toks/s, output: 62.55 toks/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' 10\uac1c\uc758 \uc694\ub9ac\ub97c \ucd94\ucc9c\ud574\uc8fc\uc138\uc694.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = llm.generate(\"\ud55c\uad6d\uc5d0\uc11c \uc720\uba85\ud55c \uc74c\uc2dd\uc740 \ubb34\uc5c7\uc778\uac00\uc694?\")\n",
        "outputs[0].outputs[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mDH13-D55PIm",
        "outputId": "7f0fc84b-5818-4845-d610-60839224881f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2021\ub144 \ud604\uc7ac \ub300\ud55c\ubbfc\uad6d\uc758 \uc218\ub3c4\ub294 \uc11c\uc6b8\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc218\ub3c4 \uacb0\uc815\uc740 \uad6d\ud68c\uc5d0\uc11c \uc774\ub8e8\uc5b4\uc9c0\uba70, \uad6d\ud68c\uc5d0\uc11c \uc11c\uc6b8\uc744 \uc218\ub3c4\ub85c \uc120\uc815\ud55c \uac83\uc740 \ud589\uc815\ud3b8\uc758\uc0c1 \uc120\uc815\ub41c \uac83\uc774\uc5c8\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uc218\ub3c4\ub294 \uc0ac\ub78c\ub4e4\uc758 \ud3b8\uc758\ub97c \uace0\ub824\ud558\uc5ec \uacb0\uc815\ub41c \uac83\uc774\uae30 \ub54c\ubb38\uc5d0, \uad6d\uac00\uc758 \uc911\uc2ec\uc9c0\uc640 \uac19\uc740 \uc758\ubbf8\ub97c \ub2f4\uace0 \uc788\ub294 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. \uc989, \uc218\ub3c4\ub294 \ub2e8\uc21c\ud55c \ud589\uc815\ud3b8\uc758\ub97c \uace0\ub824\ud55c \uacb0\uc815\uc774\uc5c8\ub358 \uac83\uc785\ub2c8\ub2e4.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ[\"RUNPOD_API_KEY\"] = \"your_runpod_api_key\"\n",
        "runpod_url = \"runpod_url_key\"\n",
        "openai_api_base = f\"https://api.runpod.ai/v2/{runpod_url}/openai/v1\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"RUNPOD_API_KEY\"],\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model=\"daje/meta-llama3.1-8B-qna-koalpaca-v1.1\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"\ub300\ud55c\ubbfc\uad6d\uc758 \uc218\ub3c4\ub294 \ud604\uc7ac\"},\n",
        "    ]\n",
        ")\n",
        "chat_response.choices[0].message.content"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}