{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7956796",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5431f-5b05-433c-9d6b-33517c8bf434",
   "metadata": {},
   "source": [
    "해당 데이터를 전처리해서 허깅페이스에 데이터셋을 업로드하기까지의 과정은 아래의 Colab 주소에서 확인 가능합니다.  \n",
    "https://colab.research.google.com/drive/1ZVwJ24AX92XnosIpS5-sbDqcKz_SloE9?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc338ac8-931c-4b20-9057-e987e8227b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.4.0)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m186.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m168.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0 typing-extensions-4.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.45.1\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==3.0.1\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl==0.11.1\n",
      "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft==0.13.0\n",
      "  Downloading peft-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.45.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.1) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.45.1)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.1)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.45.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0 (from datasets==3.0.1)\n",
      "  Downloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==3.0.1)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers==4.45.1)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets==3.0.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1) (2023.4.0)\n",
      "Collecting aiohttp (from datasets==3.0.1)\n",
      "  Downloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2) (2.4.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.1)\n",
      "  Downloading tyro-0.9.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.0.1)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.45.1)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets==3.0.1)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.1) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2) (12.6.85)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==3.0.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==3.0.1)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==3.0.1)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.0.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.1) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m176.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m160.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m175.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.11-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typeguard, tqdm, shtab, safetensors, requests, regex, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, accelerate, peft, datasets, trl\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.34.2 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.5.0 fsspec-2024.6.1 huggingface-hub-0.27.1 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 peft-0.13.0 propcache-0.2.1 pyarrow-19.0.0 pytz-2024.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.5.2 shtab-1.7.1 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.45.1 trl-0.11.1 typeguard-4.4.1 tyro-0.9.11 tzdata-2024.2 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"torch==2.4.0\"\n",
    "%pip install \"transformers==4.45.1\" \"datasets==3.0.1\" \"accelerate==0.34.2\" \"trl==0.11.1\" \"peft==0.13.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab0492f-1159-4562-bc34-9c0967b950a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5183067f-0a44-4c1d-b5a0-b574c1238771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 991\n",
      "\n",
      "전체 데이터 분할 결과: Train 496개, Test 495개\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/finance_news_summarizer\", split=\"train\")\n",
    "\n",
    "# 2. system_message 정의\n",
    "# 데이터셋에 이미 포함된 system_prompt 열을 사용할 것이므로 따로 정의하지 않음\n",
    "\n",
    "# 3. 원본 데이터의 type별 분포 출력\n",
    "# 데이터셋에 type 열이 없으므로 전체 데이터 크기만 출력\n",
    "print(\"전체 데이터 크기:\", len(dataset))\n",
    "\n",
    "# 4. train/test 분할 비율 설정 (0.5면 5:5로 분할)\n",
    "test_ratio = 0.5\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 5. 전체 데이터의 인덱스를 train/test로 분할\n",
    "data_indices = list(range(len(dataset)))\n",
    "test_size = int(len(data_indices) * test_ratio)\n",
    "\n",
    "test_data = data_indices[:test_size]\n",
    "train_data = data_indices[test_size:]\n",
    "\n",
    "# 6. OpenAI format으로 데이터 변환을 위한 함수\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sample[\"system_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": sample[\"user_prompt\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": str(sample[\"assistant\"])\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "# 7. 분할된 데이터를 OpenAI format으로 변환\n",
    "train_dataset = [format_data(dataset[i]) for i in train_data]\n",
    "test_dataset = [format_data(dataset[i]) for i in test_data]\n",
    "\n",
    "# 8. 최종 데이터셋 크기 출력\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e17067-01c0-45e9-9aa0-b352e82a4901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}'},\n",
       " {'role': 'user',\n",
       "  'content': '중소기업 하반기 경기전망 작년보다 악화…원자잿값 상승 우려\\n서울 연합뉴스 신선미 기자 중소기업의 올해 하반기 경기전망 지수가 지난해 같은 기간보다 하락한 것으로 나타났다. 5일 중소기업중앙회에 따르면 지난달 15∼24일 중소기업 500곳을 대상으로 실시한 중소기업 경영애로 및 2022년 하반기 경기전망조사 결과 하반기 경기전망지수 SBHI 는 87.6으로 지난해 하반기 91.6 보다 4.0포인트 p 하락했다. 이 지수가 100 이상이면 경기가 개선될 것으로 보는 응답자가 더 많고 100 미만이면 그 반대라는 의미다. 중기중앙회 중기중앙회 제공 하반기 SBHI를 업종별로 보면 제조업의 경우 펄프·종이 및 종이제품업 54.2 섬유제품업 54.2 전기장비업 68.2 은 경기가 악화될 것으로 내다봤고 기타 운송장비업 127.3 가죽·가방 및 신발업 104.6 은 경기가 호전될 것으로 전망했다. 서비스업에서는 부동산업 및 임대업 60.0 도매 및 소매업 84.0 은 경기가 악화될 것으로 봤지만 예술·스포츠 및 여가 관련 서비스업 112.0 은 업황 개선을 전망했다. 하반기 예상되는 애로 요인 복수응답 은 원자재 가격 상승 58.8% 내수 부진 31.2% 인력 수급난 29.8% 금리상승 28.4% 최저임금 상승 19.4% 등의 순이었다. 또 상반기 겪은 애로 요인으로는 원자재가격 상승 62.6% 내수부진 35.2% 인력 수급난 29.8% 금리상승 25.2% 최저임금 상승 22.8% 등의 순으로 응답률이 높았다. 소상공인·중소기업의 경기 개선을 위해 필요한 정부 정책 복수응답 으로는 세금 및 각종 부담금 인하 61.4% 금융지원 45.0% 인력난 해소 34.6% 원자재 수급 안정화 28.6% 근로시간 유연화 20.0% 순으로 꼽혔다. 코로나19 이전 수준의 경영실적 회복 예상 시기에 대해서는 응답자의 27.0%가 2024년 이후 라고 답했고 이어 2023년 상반기 와 2023년 하반기 각 23.0% 2022년 하반기 14.8% 등이었다.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'negative_keywords': ['펄프·종이 및 종이제품업', '섬유제품업', '전기장비업', '부동산업 및 임대업', '도매 및 소매업'], 'positive_impact_stocks': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'positive_keywords': ['기타 운송장비업', '가죽·가방 및 신발업', '예술·스포츠 및 여가 관련 서비스업'], 'reason_for_negative_impact': '펄프·종이 및 종이제품업, 섬유제품업, 전기장비업, 부동산업 및 임대업, 도매 및 소매업은 하반기 경기 전망에서 경기가 악화될 것으로 예측되고 있다.', 'reason_for_positive_impact': '기타 운송장비업, 가죽·가방 및 신발업, 예술·스포츠 및 여가 관련 서비스업은 하반기 경기 전망에서 경기가 호전될 것으로 예측되고 있다.', 'summary': '중소기업중앙회 조사 결과, 올해 하반기 중소기업 경기 전망 지수가 작년보다 하락했다. 제조업에서는 펄프, 종이, 섬유, 전기장비업이 경기 악화를 예상했고, 운송장비와 가죽, 신발업이 호전을 전망했다. 원자재 가격 상승과 내수 부진 등이 주요 애로 요인으로, 경기 개선을 위해 세금 및 부담금 인하와 금융지원 등이 필요하다고 응답됐다.'}\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[345][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e561d8f-7a5c-4d50-a030-a31ecbe7853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# 리스트 형태에서 다시 Dataset 객체로 변경\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "print(type(train_dataset))\n",
    "print(type(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205c38e8-e863-446d-b1f6-e9c587138276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': '당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\\n두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\\n큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\\n아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\\n해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 \\'없음\\' 등을 적어서는 안 됩니다.\\n\\n만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": False,\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\\n\\n만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\\n\\n답변:\\n{\"is_stock_related\": True,\\n\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\\n\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\\n\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\\n\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}',\n",
       "   'role': 'system'},\n",
       "  {'content': '이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\\n금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.',\n",
       "   'role': 'user'},\n",
       "  {'content': \"{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33536c0",
   "metadata": {},
   "source": [
    "## 2. 모델 로드 및 템플릿 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5f299a-e0ca-407f-ae21-0f0e5fadeac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d6d4e27ee84d4cadca2f6c41da2ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0d35914f3248bc86baab13e0dc4c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd31dbbe7c9d4a3fa5d5fa7ea2a08031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c994e0a7f4ec4313827738afd41e755e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b755325e621490481f1721423c372f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db290645dbeb48c28cdd266594d08ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4f84e9396e4fd3afd4f9acacac7a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277c48d3cb014e7b8255ecbd386afcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e924bafc1ef400e8a63291bf2959cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a117c5eaef46f9806a0ad2c6e395d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0772ff4beb84f01bb5eef9952d85a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada3403576c443e1af1a21697643ce74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d508f433ea4bce9c702724fe5bab09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-7B-Instruct\" \n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8df8462-72af-4ad4-b1ed-06a84dd6086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|im_end|>\n",
      "<|im_start|>user\n",
      "이복현 카드사에 경고장…무리한 영업 자제 리볼빙 관리해야\n",
      "금감원장 여전사 CEO 간담회 유동성 리스크 관리…취약 요인별 대비해야 취약차주 이용 고금리 多…리스크 관리 필요 리볼빙 불완전 판매 우려…개선방안 마련 이복현 금융감독원장. 사진 허문찬기자 이복현 금융감독원장은 유동성 관리 취지에서 단기 수익성 확보를 위한 무리한 영업 확장을 자제해줄 것을 5일 당부했다. 이달부터 개인별 총부채원리금상환비율 DSR 3단계 조치가 시행되는 데 따라 결제성 리볼빙 등 DSR 적용 제외 상품에 대한 수요가 증가할 수 있는 만큼 리스크 관리에 각별히 신경 써달라고도 주문했다. 이 원장은 이날 서울 중구 다동 여신금융협회에서 열린 여신전문금융회사 최고경영자 CEO 와의 간담회에서 유동성 리스크에 각별한 관심을 가져 주기 바란다. 여전사는 수신 기능이 없기 때문에 유동성 리스크가 가장 기본적이고 핵심적인 리스크이며 업계 스스로 관리할 필요가 있다 며 충분한 규모의 유동성을 확보하는 한편 단기 수익성 확보를 위한 무리한 영업 확장이나 고위험 자산 확대는 자제하여 주기 바란다 고 말했다. 이어 이 원장은 여전사는 여전채 발행 등 시장성 차입을 통해 대부분의 자금을 조달하고 있어 시중금리 추가 상승 시 조달에 어려움이 발생할 수 있다. 또 자금 운용 측면에서 가계대출은 상대적으로 취약한 계층이 이용하고 기업대출은 프로젝트파이낸싱 PF 대출 등 부동산 업종에 집중돼 경제 상황에 민감하게 영향을 받는다 며 여전사의 자금조달·운용상 특수성으로 취약 요인별로 철저한 대비가 필요하다 고 했다. 이 원장은 2020년 신종 코로나바이러스 감염증 코로나19 발생 당시 여전채 스프레드가 확대되면서 여전채 신규 발행이 사실상 중단되어 일부 중소형 여전사는 수 개월간 유동성 애로에 직면한 바 있다 며 지난 6월 이후 여전채 스프레드가 2020년 유동성 위기 당시 최고점 92bp 을 상회하면서 자금조달 여건이 더욱 악화되고 있다 고 했다. 그러면서 이 원장은 자체적으로 보수적인 상황을 가정해 유동성 스트레스 테스트를 실시하고 비상 자금 조달 계획도 다시 한번 점검해 주기 바란다 며 추가적인 대출처 확충이나 대주주 지원방안 유상증자 자금지원 등 확보 등을 통해 만기도래 부채를 자체적으로 상환할 수 있도록 충분한 규모의 유동성 확보가 필요하다 고 강조했다. 아울러 이 원장은 가계대출을 안정적으로 관리하고 손실 흡수 능력을 확충하는 데도 집중해 달라고 당부했다. 그는 여전사의 가계대출은 취약차주가 이용하는 고금리 상품이 대부분을 차지하고 있어 금리 상승 시 건전성이 저하될 우려가 있다 며 취약차주에 대한 고금리 대출 취급 시 차주의 상환 능력에 맞는 대출 취급 관행이 정착될 수 있도록 관심을 가져 주시기 바란다 고 했다. 이 원장은 이달부터 시행된 DSR 3단계 조치 이후 현금서비스 결제성 리볼빙 등 DSR 적용 대상에서 제외되는 상품에 대한 수요가 증가할 수 있으므로 리스크 관리에 보다 신경 써주길 바란다 며 특히 손실 흡수 능력 확충을 위해 미래 전망을 보수적으로 설정해 대손충당금을 충분히 적립할 필요가 있다 고 덧붙였다. 이 원장은 기업대출이 특정 업종에 편중되지 않도록 여신심사 및 사후관리를 강화해 줄 것도 피력했다. 그는 여전사는 과거 10년간 저금리 기조 및 경쟁 심화로 PF 대출 등 부동산 업종을 중심으로 기업대출을 확대해 최근에는 고유업무 자산을 초과하게 됐다 면서 그러나 부동산 가격하락에 대한 우려가 높은 점을 고려해 대출 취급 시 담보물이 아닌 채무 상환 능력 위주로 여신심사를 하고 대출 취급 이후에는 차주의 신용위험 변화 여부를 주기적으로 점검할 필요가 있다 고 말했다. 이어 이 원장은 여전사 스스로 기업여신 심사 및 사후관리를 강화하고 시장 상황 악화에 대비해 대손충당금 추가 적립에도 힘써 주시기 바란다 며 금감원은 모든 PF 대출에 대한 사업성 평가를 실시하는 등 기업대출 실태를 점검하고 그 결과를 바탕으로 업계와 기업여신 심사 및 사후관리 모범규준 을 마련할 계획 이라고 했다. 이 원장은 코로나19 지원 프로그램 종료 등에 대비한 취약차주 지원에도 관심을 당부했다. 그는 여전사가 자체 운영 중인 프리워크아웃 등 채무조정 지원 프로그램을 활용해 일시적으로 재무적 곤경에 처한 차주가 조기에 생업에 복귀할 수 있도록 적극적인 지원을 부탁드린다 며 올해 8월부터 회사별 금리인하요구권 운영실적 공시가 시행되므로 고객 안내 강화 등을 통해 신용도가 개선된 고객의 금리부담이 경감될 수 있도록 많은 관심을 가져 주시기 바란다 고 강조했다. 그러면서 이 원장은 최근 이용금액이 증가하는 결제성 리볼빙은 취약차주의 상환 부담을 일시적으로 줄여줄 수 있는 장점이 있지만 금소법상 금융상품에 해당하지 않아 불완전 판매에 대한 우려가 있는 것도 사실 이라며 금감원은 금융위 협회와 함께 금융소비자 권익 제고를 위해 리볼빙 설명서 신설 취약차주 가입 시 해피콜 실시 금리 산정 내역 안내 금리 공시 주기 단축 등의 개선방안을 마련 중에 있다. 각 카드사 CEO께서도 개선방안 마련 전까지 고객에 대한 설명 미흡 등으로 인해 불완전 판매가 발생하지 않도록 자체적으로 관리를 강화해 주시기를 당부드린다 고 했다. 이 원장은 여전업계 경쟁력 강화를 위한 규제 완화 등 정책적 지원을 아끼지 않겠다는 뜻도 밝혔다. 그는 디지털 전환 시대를 맞이해 금융업과 비금융업의 경계가 허물어지고 있습니다. 특히 여전사는 빅테크와의 경쟁 심화로 여타 업종보다 어려움에 처해 있으므로 새로운 성장동력을 발굴할 수 있도록 지원하겠다 며 디지털 전환 추세를 고려해 겸영 및 부수업무의 범위 여전업별 취급 가능 업무의 경우 금융업과 연관된 사업에 대해서는 금융위에 확대를 건의하겠다. 또 해외 진출 시에도 금감원의 해외 네트워크를 활용하여 여전사의 애로사항을 해소할 수 있도록 힘쓰겠다 고 말했다. 끝으로 이 원장은 금융시장 상황이 단기간에 개선되지 않을 것으로 예상되므로 긴 호흡을 가지고 리스크 관리와 금융소비자 보호에 집중해 주시기를 당부드린다 며 금감원도 여전업계와 긴밀히 소통하면서 본업부문의 경쟁력 강화를 위해 관련 규제를 개선하고 실효성 제고를 위한 노력도 지속할 것 이라고 했다.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['여신전문금융회사', '카드사'], 'negative_keywords': ['유동성 리스크', '리볼빙', '고위험 자산', '여신전문금융회사'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '금융감독원장이 유동성 리스크 관리를 강조하며 무리한 영업 자제와 리볼빙 관리 강화를 지시한 것은 여신전문금융회사와 카드사들에게 부정적인 영향을 미칠 수 있습니다. 특히, 고위험 자산 확대 및 무리한 영업 확장 자제가 요구되면서 수익성에 부정적인 영향을 줄 수 있습니다.', 'reason_for_positive_impact': '', 'summary': '금융감독원장이 카드사와 여신전문금융회사를 대상으로 무리한 영업 자제와 리볼빙 관리를 당부하며, 유동성 리스크와 취약차주 대출에 대한 주의를 강조했다. 이는 해당 금융사들의 수익성에 부정적인 영향을 미칠 수 있다.'}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템플릿 적용\n",
    "text = tokenizer.apply_chat_template(\n",
    "    train_dataset[0][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72437ff3",
   "metadata": {},
   "source": [
    "## 3. LoRA와 SFTConfig 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb39bf90-042a-470f-8ded-63dba005466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880250c",
   "metadata": {},
   "source": [
    "`lora_alpha`: LoRA(Low-Rank Adaptation)에서 사용하는 스케일링 계수를 설정합니다. LoRA의 가중치 업데이트가 모델에 미치는 영향을 조정하는 역할을 하며, 일반적으로 학습 안정성과 관련이 있습니다.\n",
    "\n",
    "`lora_dropout`: LoRA 적용 시 드롭아웃 확률을 설정합니다. 드롭아웃은 과적합(overfitting)을 방지하기 위해 일부 뉴런을 랜덤하게 비활성화하는 정규화 기법입니다. `0.1`로 설정하면 학습 중 10%의 뉴런이 비활성화됩니다.\n",
    "\n",
    "`r`: LoRA의 랭크(rank)를 설정합니다. 이는 LoRA가 학습할 저차원 공간의 크기를 결정합니다. 작은 값일수록 계산 및 메모리 효율이 높아지지만 모델의 학습 능력이 제한될 수 있습니다.\n",
    "\n",
    "`bias`: LoRA 적용 시 편향(bias) 처리 방식을 지정합니다. `\"none\"`으로 설정하면 편향이 LoRA에 의해 조정되지 않습니다. `\"all\"` 또는 `\"lora_only\"`와 같은 값으로 변경하여 편향을 조정할 수도 있습니다.\n",
    "\n",
    "`target_modules`: LoRA를 적용할 특정 모듈(레이어)의 이름을 리스트로 지정합니다. 예제에서는 `\"q_proj\"`와 `\"v_proj\"`를 지정하여, 주로 Self-Attention 메커니즘의 쿼리와 값 프로젝션 부분에 LoRA를 적용합니다.\n",
    "\n",
    "`task_type`: LoRA가 적용되는 작업 유형을 지정합니다. `\"CAUSAL_LM\"`은 Causal Language Modeling, 즉 시퀀스 생성 작업에 해당합니다. 다른 예로는 `\"SEQ2SEQ_LM\"`(시퀀스-투-시퀀스 언어 모델링) 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d57675fd-4374-4cd6-b8bb-950bf216cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-rag-ko\",           # 저장될 디렉토리와 저장소 ID\n",
    "    num_train_epochs=3,                      # 학습할 총 에포크 수 \n",
    "    per_device_train_batch_size=2,           # GPU당 배치 크기\n",
    "    gradient_accumulation_steps=2,           # 그래디언트 누적 스텝 수\n",
    "    gradient_checkpointing=True,             # 메모리 절약을 위한 체크포인팅\n",
    "    optim=\"adamw_torch_fused\",               # 최적화기\n",
    "    logging_steps=10,                        # 로그 기록 주기\n",
    "    save_strategy=\"steps\",                   # 저장 전략\n",
    "    save_steps=50,                           # 저장 주기\n",
    "    bf16=True,                              # bfloat16 사용\n",
    "    learning_rate=1e-4,                     # 학습률\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",           # 고정 학습률\n",
    "    push_to_hub=False,                      # 허브 업로드 안 함\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b457fc",
   "metadata": {},
   "source": [
    "`output_dir`: 학습 결과가 저장될 디렉토리 또는 모델 저장소의 이름을 지정합니다. 이 디렉토리에 학습된 모델 가중치, 설정 파일, 로그 파일 등이 저장됩니다.\n",
    "\n",
    "`num_train_epochs`: 모델을 학습시키는 총 에포크(epoch) 수를 지정합니다. 에포크는 학습 데이터 전체를 한 번 순회한 주기를 의미합니다. 예를 들어, `3`으로 설정하면 데이터셋을 3번 학습합니다.\n",
    "\n",
    "`per_device_train_batch_size`: GPU 한 대당 사용되는 배치(batch)의 크기를 설정합니다. 배치 크기는 모델이 한 번에 처리하는 데이터 샘플의 수를 의미합니다. 작은 크기는 메모리 사용량이 적지만 학습 시간이 증가할 수 있습니다.\n",
    "\n",
    "`gradient_accumulation_steps`: 그래디언트를 누적할 스텝(step) 수를 지정합니다. 이 값이 2로 설정된 경우, 두 스텝마다 그래디언트를 업데이트합니다. 배치 크기를 가상으로 늘리는 효과가 있으며, GPU 메모리 부족 문제를 해결할 때 유용합니다.\n",
    "\n",
    "`gradient_checkpointing`: 그래디언트 체크포인팅을 활성화하여 메모리를 절약합니다. 이 옵션은 계산 그래프를 일부 저장하지 않고 다시 계산하여 메모리를 절약하지만, 속도가 약간 느려질 수 있습니다.\n",
    "\n",
    "`optim`: 학습 시 사용할 최적화 알고리즘을 설정합니다. `adamw_torch_fused`는 PyTorch의 효율적인 AdamW 최적화기를 사용합니다.\n",
    "\n",
    "`logging_steps`: 로그를 기록하는 주기를 스텝 단위로 지정합니다. 예를 들어, `10`으로 설정하면 매 10 스텝마다 로그를 기록합니다.\n",
    "\n",
    "`save_strategy`: 모델을 저장하는 전략을 설정합니다. `\"steps\"`로 설정된 경우, 지정된 스텝마다 모델이 저장됩니다.\n",
    "\n",
    "`save_steps`: 모델을 저장하는 주기를 스텝 단위로 설정합니다. 예를 들어, `50`으로 설정하면 매 50 스텝마다 모델을 저장합니다.\n",
    "\n",
    "`bf16`: bfloat16 정밀도를 사용하도록 설정합니다. bfloat16은 FP32와 유사한 범위를 제공하면서 메모리와 계산 효율성을 높입니다.\n",
    "\n",
    "`learning_rate`: 학습률을 지정합니다. 학습률은 모델의 가중치가 한 번의 업데이트에서 얼마나 크게 변할지를 결정합니다. 일반적으로 작은 값을 사용하여 안정적인 학습을 유도합니다.\n",
    "\n",
    "`max_grad_norm`: 그래디언트 클리핑의 임계값을 설정합니다. 이 값보다 큰 그래디언트가 발생하면, 임계값으로 조정하여 폭발적 그래디언트를 방지합니다.\n",
    "\n",
    "`warmup_ratio`: 학습 초기 단계에서 학습률을 선형으로 증가시키는 워밍업 비율을 지정합니다. 학습의 안정성을 높이기 위해 사용됩니다.\n",
    "\n",
    "`lr_scheduler_type`: 학습률 스케줄러의 유형을 설정합니다. `\"constant\"`는 학습률을 일정하게 유지합니다.\n",
    "\n",
    "`push_to_hub`: 학습된 모델을 허브에 업로드할지 여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
    "\n",
    "`remove_unused_columns`: 사용되지 않는 열을 제거할지 여부를 설정합니다. True로 설정하면 메모리를 절약할 수 있습니다.\n",
    "\n",
    "`dataset_kwargs`: 데이터셋 로딩 시 추가적인 설정을 전달합니다. 예제에서는 `skip_prepare_dataset: True`로 설정하여 데이터셋 준비 단계를 건너뜹니다.\n",
    "\n",
    "`report_to`: 학습 로그를 보고할 대상을 지정합니다. `None`으로 설정되면 로그가 기록되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11e4e1",
   "metadata": {},
   "source": [
    "## 4. 학습 중 전처리 함수: collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adbcf4b1-ab29-451d-b628-21466d16519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for example in batch:\n",
    "        # messages의 각 내용에서 개행문자 제거\n",
    "        clean_messages = []\n",
    "        for message in example[\"messages\"]:\n",
    "            clean_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "            clean_messages.append(clean_message)\n",
    "        \n",
    "        # 깨끗해진 메시지로 템플릿 적용\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            clean_messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).strip()\n",
    "        \n",
    "        # 텍스트를 토큰화\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        \n",
    "        # 레이블 초기화\n",
    "        labels = [-100] * len(input_ids)\n",
    "        \n",
    "        # assistant 응답 부분 찾기\n",
    "        im_start = \"<|im_start|>\"\n",
    "        im_end = \"<|im_end|>\"\n",
    "        assistant = \"assistant\"\n",
    "        \n",
    "        # 토큰 ID 가져오기\n",
    "        im_start_tokens = tokenizer.encode(im_start, add_special_tokens=False)\n",
    "        im_end_tokens = tokenizer.encode(im_end, add_special_tokens=False)\n",
    "        assistant_tokens = tokenizer.encode(assistant, add_special_tokens=False)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # <|im_start|>assistant 찾기\n",
    "            if (i + len(im_start_tokens) <= len(input_ids) and \n",
    "                input_ids[i:i+len(im_start_tokens)] == im_start_tokens):\n",
    "                \n",
    "                # assistant 토큰 찾기\n",
    "                assistant_pos = i + len(im_start_tokens)\n",
    "                if (assistant_pos + len(assistant_tokens) <= len(input_ids) and \n",
    "                    input_ids[assistant_pos:assistant_pos+len(assistant_tokens)] == assistant_tokens):\n",
    "                    \n",
    "                    # assistant 응답의 시작 위치로 이동\n",
    "                    current_pos = assistant_pos + len(assistant_tokens)\n",
    "                    \n",
    "                    # <|im_end|>를 찾을 때까지 레이블 설정\n",
    "                    while current_pos < len(input_ids):\n",
    "                        if (current_pos + len(im_end_tokens) <= len(input_ids) and \n",
    "                            input_ids[current_pos:current_pos+len(im_end_tokens)] == im_end_tokens):\n",
    "                            # <|im_end|> 토큰도 레이블에 포함\n",
    "                            for j in range(len(im_end_tokens)):\n",
    "                                labels[current_pos + j] = input_ids[current_pos + j]\n",
    "                            break\n",
    "                        labels[current_pos] = input_ids[current_pos]\n",
    "                        current_pos += 1\n",
    "                    \n",
    "                    i = current_pos\n",
    "                \n",
    "            i += 1\n",
    "        \n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "    # 패딩 적용\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n",
    "    \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n",
    "        \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n",
    "        new_batch[\"labels\"][i].extend([-100] * padding_length)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    for k, v in new_batch.items():\n",
    "        new_batch[k] = torch.tensor(v)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eccfa92",
   "metadata": {},
   "source": [
    "`collate_fn(batch)` 함수는 자연어 처리 모델 학습을 위해 데이터를 전처리하는 역할을 수행합니다. 이 함수는 배치 내의 데이터를 처리하여 모델이 사용할 수 있는 입력 형식으로 변환합니다.\n",
    "\n",
    "먼저, 각 샘플의 메시지에서 개행 문자를 제거하고 필요한 정보만 남깁니다. 정리된 메시지로 텍스트를 구성하고 이를 토큰화하여 input_ids와 attention_mask를 생성합니다. 이후 레이블 데이터를 초기화한 다음, 특정 토큰 패턴(<|im_start|>assistant 이후부터 <|im_end|>까지)을 찾아 해당 범위에 레이블을 설정합니다. 이 범위를 제외한 나머지 위치는 -100으로 설정하여 손실 계산에서 제외되도록 합니다.\n",
    "\n",
    "최종적으로, 배치 내 모든 샘플의 길이를 동일하게 맞추기 위해 패딩 작업을 수행합니다. 이 과정에서 입력 데이터에는 패딩 토큰 ID를 추가하고, 어텐션 마스크에는 0을 추가하며, 레이블에는 -100을 추가합니다. 모든 데이터는 PyTorch 텐서로 변환되어 반환됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82c8810f-2613-4d6b-8dcb-985ba50991f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 3196])\n",
      "어텐션 마스크 형태: torch.Size([1, 3196])\n",
      "레이블 형태: torch.Size([1, 3196])\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이\n",
    "max_seq_length=8192\n",
    "\n",
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "example = train_dataset[0]\n",
    "batch = collate_fn([example])\n",
    "\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50fbb8d-1265-4c9a-b56e-2803f09bd810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "[151644, 8948, 198, 64795, 82528, 33704, 55673, 31079, 85251, 5140, 231, 112, 24897, 138020, 98358, 87608, 19391, 126440, 129321, 17877, 55673, 16560, 5140, 231, 112, 24897, 134039, 140568, 126591, 42905, 40771, 230, 128024, 5140, 231, 112, 24897, 140568, 126591, 20487, 78952, 624, 126923, 127154, 143604, 3315, 120, 222, 12802, 24897, 19969, 134015, 130705, 125149, 136585, 54969, 12802, 144773, 20401, 10997, 141965, 76337, 42039, 140174, 16186, 139713, 624, 144544, 125686, 144707, 126414, 136541, 19391, 128772, 125686, 144707, 126414, 129125, 135968, 33509, 125476, 34395, 44518, 47985, 87425, 95577, 139713, 13, 23084, 16560, 10997, 54969, 144108, 17877, 60985, 98642, 128555, 128956, 129093, 135227, 72344, 48458, 13, 134084, 55673, 20401, 16186, 139713, 624, 52959, 53442, 10997, 56475, 126804, 897, 16560, 66790, 29326, 131193, 19391, 94613, 60838, 13, 66790, 55054, 131193, 17877, 126629, 135968, 21329, 125544, 139713, 13, 94613, 66790, 29326, 131193, 19391, 126629, 135968, 126550, 23573, 897, 18411, 3315, 109, 226, 130109, 143570, 33509, 139713, 624, 33883, 64795, 131193, 12802, 130671, 32290, 5140, 117, 230, 79921, 53955, 129549, 5140, 117, 230, 83634, 17380, 135968, 137571, 130650, 13, 16235, 226, 20401, 17380, 364, 127173, 48431, 6, 134454, 135968, 136108, 16560, 95170, 72344, 48458, 382, 72553, 125535, 94613, 5140, 231, 112, 24897, 19969, 142976, 98358, 87608, 7, 131110, 8, 12802, 139957, 128911, 132553, 50696, 127451, 11, 142976, 98358, 87608, 7, 131110, 8, 80573, 134006, 125120, 77353, 124780, 12802, 130768, 5140, 231, 112, 24897, 32077, 134832, 136646, 80573, 131050, 140174, 60838, 382, 132760, 126667, 510, 4913, 285, 30541, 53256, 788, 3557, 345, 1, 1708, 788, 330, 57026, 20487, 126377, 94613, 5140, 231, 112, 24897, 18411, 85997, 125535, 96137, 85997, 125535, 51588, 17877, 140174, 16186, 139713, 63159, 72553, 125535, 94613, 5140, 231, 112, 24897, 19969, 142976, 98358, 87608, 7, 131110, 8, 134771, 77353, 124780, 133245, 127451, 11, 142976, 98358, 87608, 7, 131110, 8, 53680, 134006, 125120, 77353, 124780, 12802, 130768, 5140, 231, 112, 24897, 32077, 134832, 136646, 80573, 131050, 140174, 60838, 382, 132760, 126667, 510, 4913, 285, 30541, 53256, 788, 3007, 345, 1, 30487, 36788, 531, 1261, 25183, 788, 4383, 126793, 12802, 144773, 79921, 53955, 83634, 20401, 141966, 17380, 23084, 5140, 231, 112, 24897, 19969, 40771, 235, 29281, 128533, 126440, 129321, 17877, 53989, 226, 124708, 42039, 57835, 29281, 128841, 98358, 87608, 129360, 86034, 17877, 140174, 16186, 139713, 13, 23872, 121, 25715, 17380, 135968, 127451, 30520, 226, 79632, 42039, 135968, 21329, 125544, 139713, 13, 98358, 87608, 79632, 42039, 57835, 29281, 128841, 61298, 83291, 79632, 17877, 135968, 33509, 139713, 13, 5140, 231, 112, 24897, 138020, 57835, 29281, 47836, 28733, 64521, 36055, 133085, 23573, 10764, 240, 222, 126730, 93701, 42039, 135968, 33509, 139713, 13, 139327, 11, 134015, 87425, 127728, 133099, 5140, 117, 230, 83634, 17380, 140174, 16186, 139713, 1189, 1259, 1, 19895, 5478, 54160, 36788, 531, 788, 330, 80901, 20401, 98358, 87608, 126253, 94613, 5140, 231, 112, 24897, 138020, 40771, 235, 29281, 128533, 126440, 129321, 17877, 83596, 17877, 132091, 57835, 29281, 23573, 132819, 18411, 131180, 19391, 135448, 140174, 16186, 139713, 756, 1, 30487, 51354, 788, 4383, 144203, 29281, 128533, 126440, 129321, 17877, 53989, 226, 132091, 57835, 29281, 128841, 98358, 87608, 126253, 134015, 128836, 32290, 131180, 19391, 40771, 235, 29281, 128533, 126440, 129321, 17877, 55673, 126551, 134312, 120, 92192, 19969, 97143, 139325, 55673, 35711, 23573, 130345, 55054, 10764, 92120, 130109, 29346, 129125, 54969, 12802, 144773, 79921, 53955, 83634, 141966, 17380, 140174, 16186, 139713, 13, 54116, 125880, 79632, 11, 98005, 55054, 79632, 134454, 130593, 133970, 60838, 13, 30520, 113, 128747, 130345, 55054, 130005, 10764, 245, 230, 26699, 60838, 13, 130671, 32290, 5140, 117, 230, 83634, 17380, 140174, 128747, 29326, 57268, 1189, 1259, 1, 42224, 36788, 531, 1261, 25183, 788, 4383, 126793, 12802, 144773, 79921, 53955, 83634, 20401, 141966, 17380, 23084, 5140, 231, 112, 24897, 19969, 40771, 235, 29281, 128533, 126440, 129321, 17877, 53989, 226, 124708, 42039, 57835, 29281, 128841, 98358, 87608, 129125, 140174, 16186, 139713, 13, 23872, 121, 25715, 17380, 135968, 127451, 30520, 226, 79632, 42039, 135968, 21329, 125544, 139713, 13, 98358, 87608, 79632, 42039, 57835, 29281, 128841, 61298, 83291, 79632, 17877, 135968, 33509, 139713, 13, 5140, 231, 112, 24897, 138020, 57835, 29281, 47836, 28733, 64521, 36055, 133085, 23573, 10764, 240, 222, 126730, 93701, 42039, 135968, 33509, 139713, 13, 139327, 11, 134015, 87425, 127728, 133099, 5140, 117, 230, 83634, 17380, 140174, 16186, 139713, 1189, 1259, 1, 19895, 5478, 53865, 36788, 531, 788, 330, 80901, 20401, 98358, 87608, 126253, 94613, 5140, 231, 112, 24897, 138020, 40771, 235, 29281, 128533, 126440, 129321, 17877, 83596, 17877, 132091, 57835, 29281, 23573, 132819, 18411, 131180, 19391, 135448, 140174, 16186, 139713, 756, 1, 42224, 51354, 788, 4383, 63089, 29281, 128533, 126440, 129321, 17877, 53989, 226, 132091, 57835, 29281, 128841, 98358, 87608, 126253, 134015, 128836, 32290, 131180, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 55673, 126551, 134312, 120, 92192, 19969, 97143, 139325, 55673, 35711, 23573, 130345, 55054, 10764, 92120, 130109, 29346, 129125, 54969, 12802, 144773, 79921, 53955, 83634, 141966, 17380, 140174, 16186, 139713, 13, 54116, 125880, 79632, 11, 98005, 55054, 79632, 134454, 130593, 133970, 60838, 13, 30520, 113, 128747, 130345, 55054, 130005, 10764, 245, 230, 26699, 60838, 13, 130671, 32290, 5140, 117, 230, 83634, 17380, 140174, 128747, 29326, 57268, 1189, 1259, 1, 1708, 788, 330, 57026, 20487, 126377, 94613, 5140, 231, 112, 24897, 18411, 85997, 125535, 96137, 85997, 125535, 51588, 17877, 140174, 16186, 139713, 9207, 151645, 198, 151644, 872, 198, 12802, 97834, 126407, 90711, 112, 29346, 55054, 19391, 43115, 34395, 40853, 1940, 125054, 28002, 23573, 126440, 124517, 64577, 37087, 56983, 130454, 144269, 92751, 28002, 129264, 198, 125052, 129567, 54321, 40853, 83518, 65865, 55054, 12156, 16778, 226, 125786, 61741, 126310, 57089, 32831, 56983, 140532, 92751, 28002, 1940, 137237, 125535, 85997, 31328, 126591, 60960, 70582, 129264, 131565, 125535, 125625, 54330, 126563, 126429, 125052, 28002, 40666, 248, 1940, 28002, 140532, 92751, 28002, 126871, 56983, 130454, 144269, 126488, 130973, 65865, 140568, 129865, 124657, 125476, 1940, 59761, 125519, 126321, 126246, 140175, 23084, 97834, 126407, 40771, 230, 128024, 129567, 129502, 54321, 40853, 13, 139764, 10764, 245, 230, 51588, 138143, 20487, 25715, 23084, 97834, 126407, 40771, 230, 128024, 129567, 129502, 54321, 40853, 33704, 126310, 57089, 32831, 92751, 28002, 131565, 21329, 56475, 129400, 20487, 28733, 131870, 32831, 130729, 41671, 18411, 130679, 125149, 28002, 23573, 126440, 124517, 130729, 137471, 64577, 37087, 33883, 131303, 129337, 220, 20, 32077, 125834, 63089, 128836, 13, 23084, 129062, 126558, 126799, 126591, 3315, 112, 251, 63089, 130752, 54321, 28002, 125052, 55902, 65238, 70582, 132841, 422, 14557, 220, 18, 125068, 124781, 65510, 59698, 19969, 138767, 128841, 5140, 41902, 126629, 82619, 37087, 32831, 56983, 130454, 144269, 77002, 422, 14557, 135969, 62071, 128792, 58034, 125678, 19391, 128605, 28733, 35711, 19969, 132376, 251, 19969, 47836, 28733, 64521, 62107, 127908, 56983, 140532, 92751, 28002, 19391, 126804, 126591, 125511, 128753, 65306, 3315, 235, 101, 129062, 129254, 47985, 55673, 51588, 128836, 13, 23084, 129093, 40853, 33704, 23084, 129378, 136905, 70943, 88259, 49367, 57089, 83518, 82528, 138870, 129640, 61741, 56475, 130847, 129807, 83518, 82528, 133081, 138870, 131110, 81173, 34395, 142408, 25715, 12156, 74884, 222, 20401, 16778, 226, 125786, 61741, 56475, 126310, 57089, 32831, 56983, 140532, 19391, 126804, 126591, 23573, 138449, 17877, 88846, 55673, 20487, 81718, 129804, 13146, 13, 83518, 65865, 136294, 28733, 82528, 54116, 66019, 12802, 46682, 20487, 129147, 126310, 57089, 32831, 56983, 140532, 19969, 130887, 136089, 80968, 133215, 20136, 113, 125512, 128533, 56983, 140532, 136262, 24485, 227, 124781, 141769, 92751, 28002, 47836, 126871, 19969, 90686, 48108, 108, 36330, 102, 79716, 23573, 134313, 129439, 20401, 126310, 57089, 132818, 130729, 41671, 42905, 61298, 129027, 129400, 20487, 28733, 131870, 32831, 130729, 41671, 18411, 130679, 125149, 28002, 23573, 126440, 124517, 130729, 40853, 129835, 126429, 80901, 125341, 64577, 85057, 130729, 66845, 16560, 64577, 37087, 82190, 55673, 20487, 81718, 129804, 13146, 126429, 126254, 128836, 13, 23084, 31079, 23084, 129093, 40853, 33704, 83518, 65865, 136294, 83518, 65865, 130752, 95996, 124528, 77002, 44518, 40853, 32831, 129882, 43866, 17877, 131582, 140094, 20401, 64577, 125052, 17877, 65510, 129062, 126204, 127353, 44518, 126402, 125052, 28002, 68408, 58034, 130766, 44518, 65510, 129062, 19391, 124685, 125476, 133914, 12802, 133396, 47836, 28733, 90686, 13, 125640, 64577, 125052, 132028, 26699, 3315, 116, 94, 32290, 56475, 35509, 124781, 134239, 33704, 58034, 66845, 128552, 131565, 125535, 23573, 94203, 137812, 12802, 126563, 126204, 54116, 124517, 134239, 33704, 84255, 17380, 144644, 28626, 126793, 12802, 142907, 144108, 28415, 60960, 69923, 77002, 85403, 57089, 85057, 24485, 227, 126337, 19391, 130263, 126402, 136398, 43115, 37087, 58034, 130803, 19391, 125713, 120, 129567, 128555, 126440, 129321, 17877, 83596, 16560, 13146, 48108, 108, 83518, 65865, 55054, 20401, 64577, 125052, 92817, 129062, 13935, 93672, 26699, 55902, 127820, 23259, 32831, 42039, 131565, 125535, 85997, 31328, 126591, 17380, 48364, 254, 126781, 23573, 60960, 70582, 19969, 126871, 129330, 126429, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 220, 17, 15, 17, 15, 126216, 128753, 126337, 3315, 65291, 17380, 60315, 130271, 12802, 60294, 24897, 129423, 137284, 128844, 3315, 65291, 17380, 60315, 16, 24, 133396, 140730, 83518, 65865, 130752, 79207, 44680, 62544, 126673, 29346, 19969, 130729, 66845, 64119, 131611, 83518, 65865, 130752, 128753, 131005, 95996, 124528, 12802, 130037, 55902, 70943, 125068, 128993, 136605, 70943, 43590, 128909, 83518, 65865, 136294, 28733, 73523, 128514, 62275, 126310, 57089, 32831, 23872, 254, 17380, 19391, 137351, 32290, 23573, 81718, 90686, 48108, 108, 133146, 220, 21, 128514, 136331, 83518, 65865, 130752, 79207, 44680, 62544, 126673, 29346, 19969, 220, 17, 15, 17, 15, 126216, 126310, 57089, 32831, 45710, 20487, 140730, 81173, 34395, 126333, 220, 24, 17, 17808, 16751, 226, 58034, 61741, 132537, 64577, 125052, 92817, 129062, 83518, 124873, 12802, 140084, 23872, 227, 56290, 130357, 90686, 126429, 10764, 244, 19946, 13, 125261, 131611, 23084, 129093, 40853, 33704, 136751, 128552, 63332, 23259, 128533, 58034, 130803, 17877, 35509, 29281, 33883, 126310, 57089, 32831, 141767, 28626, 126673, 24897, 10764, 72509, 53189, 18411, 126423, 29326, 126204, 73986, 55902, 64577, 125052, 65510, 129062, 94203, 127324, 47985, 131170, 138903, 18585, 238, 129845, 33883, 55673, 20487, 81718, 129804, 13146, 48108, 108, 68408, 128533, 60960, 69923, 125746, 130729, 131347, 129835, 60960, 54330, 54330, 132185, 126321, 126246, 126310, 55902, 128844, 25715, 64577, 125052, 137709, 77002, 130729, 41671, 134454, 131582, 62107, 130898, 53442, 85403, 130752, 18411, 136751, 128552, 58034, 65238, 47836, 28733, 136303, 36330, 102, 79716, 23573, 134313, 129439, 20401, 126310, 57089, 32831, 130729, 41671, 19969, 126871, 129330, 126429, 129413, 92817, 128836, 13, 48408, 126893, 60294, 23084, 129093, 40853, 33704, 35509, 124781, 134239, 17877, 95170, 29281, 128552, 92751, 28002, 126204, 77596, 238, 125086, 10764, 251, 94, 23259, 143861, 98, 135818, 130729, 131347, 42905, 5140, 41902, 47985, 130263, 126402, 33883, 34143, 105, 129254, 125834, 63089, 128836, 13, 54825, 16560, 83518, 65865, 55054, 20401, 35509, 124781, 134239, 33704, 131565, 125535, 125625, 54330, 19969, 126563, 42905, 126429, 125052, 28002, 58034, 125678, 12802, 140094, 17877, 129882, 21329, 126204, 127353, 40771, 230, 28002, 58034, 130766, 44518, 130270, 65865, 137032, 125569, 16186, 131396, 124657, 125476, 19969, 90686, 48108, 108, 131565, 125535, 125625, 54330, 19391, 128605, 126429, 125052, 28002, 60960, 69923, 131565, 128911, 44518, 129882, 130788, 58034, 65238, 143861, 98, 28754, 19391, 131417, 16560, 60960, 69923, 131565, 128911, 92751, 124528, 12802, 36055, 135375, 131396, 28733, 136303, 138449, 17877, 88846, 55673, 29326, 20487, 81718, 129804, 13146, 126429, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 23084, 129062, 126558, 138767, 52300, 422, 14557, 220, 18, 125068, 124781, 65510, 59698, 136331, 141526, 125052, 141556, 82619, 37087, 32831, 56983, 130454, 144269, 77002, 422, 14557, 135969, 60960, 55902, 56475, 62071, 128792, 128841, 58034, 125678, 19391, 128605, 28733, 35711, 19969, 132376, 251, 19969, 47836, 28733, 135354, 137621, 56983, 140532, 92751, 28002, 19391, 63332, 13146, 128753, 65306, 3315, 235, 101, 54330, 130105, 81718, 129804, 13146, 48108, 108, 136115, 77596, 238, 125086, 10764, 251, 94, 23259, 143861, 98, 28754, 130729, 131347, 17877, 130039, 143005, 56419, 130472, 17877, 63332, 23259, 128552, 65880, 33883, 60960, 130640, 131347, 64795, 125052, 17877, 36330, 102, 79716, 125511, 135968, 126702, 47836, 126871, 19969, 90686, 126429, 126365, 100, 141142, 139836, 13, 23084, 129093, 40853, 33704, 54116, 124517, 134239, 12802, 142976, 24485, 227, 126337, 19391, 10764, 236, 116, 126402, 132553, 50696, 126054, 83518, 82528, 125512, 55054, 128355, 32129, 127033, 129576, 18411, 129413, 56290, 33883, 53989, 226, 137638, 142510, 28754, 128836, 13, 54825, 16560, 83518, 65865, 136294, 45130, 120, 92192, 220, 16, 15, 126216, 62275, 125569, 125052, 28002, 54116, 92817, 128355, 43115, 131833, 28927, 105, 56290, 17380, 28415, 60960, 69923, 77002, 85403, 57089, 85057, 24485, 227, 126337, 17877, 141482, 42039, 54116, 124517, 134239, 17877, 130729, 66845, 33883, 139465, 126377, 126429, 125522, 124517, 125054, 64577, 85057, 17877, 83315, 53680, 128555, 72344, 238, 13146, 48108, 112, 26698, 130549, 85403, 57089, 85057, 35509, 126614, 16186, 132772, 19391, 128605, 124657, 125476, 19969, 5140, 228, 240, 33704, 18585, 238, 17877, 126429, 125476, 33883, 60960, 69923, 131565, 128911, 44518, 34143, 112, 41671, 126251, 12802, 134808, 3315, 109, 226, 125054, 58034, 65238, 143861, 98, 28754, 45710, 54330, 17380, 83518, 82528, 125512, 134445, 130127, 60960, 69923, 131565, 128911, 136331, 126377, 129882, 130788, 128753, 26699, 80901, 125341, 137525, 83518, 63089, 18411, 55673, 20487, 128552, 18585, 238, 129845, 47836, 126871, 19969, 90686, 126429, 126254, 128836, 13, 23084, 31079, 23084, 129093, 40853, 33704, 83518, 65865, 55054, 141769, 54116, 124517, 57026, 82528, 28927, 105, 55054, 128355, 32129, 127033, 129576, 18411, 129413, 56290, 126204, 44518, 40853, 58034, 130803, 23872, 227, 56290, 19391, 60960, 70582, 33883, 60960, 130640, 131347, 64795, 125052, 68408, 135968, 126702, 130612, 10764, 252, 246, 135660, 55673, 29326, 20487, 81718, 129804, 13146, 48108, 108, 40771, 230, 129567, 54321, 33704, 129304, 28415, 60960, 69923, 19391, 128605, 131608, 32831, 69441, 231, 19969, 18411, 126423, 29326, 42905, 77002, 54116, 124517, 134239, 60985, 225, 250, 18411, 18585, 238, 129845, 126204, 54825, 98801, 18411, 81718, 144059, 42039, 24485, 227, 124781, 80573, 54116, 124517, 57026, 82528, 28927, 105, 55054, 128355, 32129, 127033, 129576, 54070, 130765, 131005, 129044, 16751, 226, 140175, 47836, 94203, 127324, 23084, 129254, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 3315, 65291, 17380, 60315, 16, 24, 132185, 84255, 81650, 127483, 98358, 63256, 77002, 19391, 60960, 70582, 23573, 131565, 125535, 125625, 54330, 132185, 130612, 138449, 17877, 125834, 63089, 128836, 13, 54825, 16560, 83518, 65865, 55054, 19969, 136751, 132029, 70943, 31328, 84255, 28002, 130109, 81133, 52959, 138873, 77002, 3315, 109, 226, 125054, 92817, 29281, 132185, 84255, 81650, 127483, 17877, 140969, 33883, 83556, 29326, 128552, 129242, 125054, 80968, 45130, 97, 65306, 19391, 71647, 23573, 129882, 54330, 19969, 65510, 132264, 47818, 124517, 19391, 30520, 113, 133032, 47836, 28733, 136303, 135968, 131529, 128533, 132185, 17877, 85403, 133376, 29346, 129807, 13146, 48108, 108, 38523, 105, 33883, 220, 23, 128514, 126558, 98005, 55054, 126591, 40771, 230, 28002, 31328, 16186, 35711, 88259, 128739, 132029, 125086, 80968, 125466, 29326, 19969, 138767, 64119, 137621, 138501, 95170, 95218, 129413, 56290, 134454, 131582, 128753, 26699, 47985, 19969, 73523, 125519, 52300, 138501, 20401, 40771, 230, 28002, 63089, 125786, 12802, 43115, 129567, 131396, 28733, 136303, 129875, 138449, 17877, 88846, 55673, 29326, 20487, 81718, 129804, 13146, 126429, 129413, 92817, 128836, 13, 125261, 131611, 23084, 129093, 40853, 33704, 139465, 126563, 125052, 132652, 12802, 132376, 251, 19969, 42905, 82619, 37087, 32831, 56983, 130454, 144269, 33704, 131565, 125535, 125625, 130788, 58034, 65238, 85403, 125786, 17877, 83556, 29326, 128552, 53989, 226, 57026, 131303, 28733, 64521, 129359, 126333, 12802, 141258, 40771, 230, 43590, 126270, 55902, 40771, 230, 128024, 55902, 125678, 19391, 94613, 87425, 50696, 52959, 126488, 130973, 65865, 140568, 129865, 19391, 128605, 124657, 125476, 19969, 64521, 137638, 130037, 23084, 50340, 124905, 40771, 230, 129567, 54321, 33704, 40771, 230, 128024, 80901, 47455, 239, 61741, 80573, 129676, 40771, 230, 128024, 43590, 70582, 25715, 142452, 131870, 62071, 34395, 18411, 130039, 56983, 130454, 144269, 133828, 26698, 128753, 125624, 131565, 125535, 125625, 54330, 35509, 43866, 44518, 60716, 129262, 144063, 126423, 29326, 40771, 230, 28002, 127165, 108, 29281, 66136, 126346, 95170, 95218, 40771, 230, 28002, 125466, 29326, 55673, 20487, 129400, 126970, 136357, 73523, 125519, 126321, 126246, 17877, 140175, 70943, 19391, 90686, 13, 126804, 90711, 112, 29346, 55054, 12156, 131493, 47985, 73523, 125519, 126321, 126246, 140175, 56419, 128878, 138501, 19391, 128605, 133828, 125714, 144147, 77002, 42039, 58677, 33883, 126488, 130973, 65865, 140568, 129865, 19969, 133396, 87425, 50696, 126054, 136751, 128552, 92751, 133886, 129413, 56290, 33883, 55673, 29326, 131777, 125834, 63089, 29346, 129807, 13146, 126429, 10764, 244, 19946, 13, 23084, 129093, 40853, 33704, 83518, 65865, 124517, 124781, 43115, 131833, 28754, 129413, 56290, 18411, 130679, 134313, 37087, 74884, 226, 56290, 77002, 36055, 126712, 80968, 132185, 17877, 48408, 142063, 21329, 50696, 127816, 16560, 5140, 250, 119, 47985, 22042, 251, 144472, 13146, 13, 54825, 16560, 76497, 242, 21329, 144046, 56419, 65238, 44518, 66845, 18411, 131417, 12802, 33883, 40771, 230, 128024, 124517, 53680, 73986, 138870, 124517, 20401, 43115, 124781, 19969, 10764, 245, 230, 126251, 31079, 134497, 128472, 13, 136115, 83518, 65865, 136294, 5140, 117, 227, 130229, 81133, 80573, 20401, 43115, 131833, 28927, 105, 56290, 17380, 83518, 125166, 24485, 227, 126337, 129885, 124685, 125476, 133914, 19391, 71647, 33883, 135354, 137621, 134585, 128677, 40853, 57089, 135818, 95996, 144071, 47836, 28733, 136303, 132185, 143353, 48108, 108, 76497, 242, 21329, 144046, 56419, 65238, 57835, 41429, 18411, 126429, 125476, 33883, 23894, 116, 125144, 128355, 85403, 23259, 124517, 125054, 20401, 47665, 242, 80901, 83518, 65865, 124517, 126591, 131565, 128911, 95351, 24485, 227, 125054, 20401, 49052, 40771, 230, 128024, 124517, 53680, 77353, 124780, 52300, 131608, 19391, 130869, 16560, 40771, 230, 128024, 80901, 19391, 130729, 66845, 18411, 130270, 20401, 143353, 13, 125640, 60716, 128792, 126616, 69923, 44518, 130612, 40771, 230, 129567, 54321, 20401, 60716, 128792, 65553, 97, 28626, 130109, 81133, 18411, 140969, 82190, 83518, 65865, 55054, 20401, 23872, 254, 17380, 131193, 17877, 60716, 43590, 47836, 28733, 136303, 10764, 252, 246, 131767, 127816, 126429, 126254, 128836, 13, 5140, 223, 251, 42039, 23084, 129093, 40853, 33704, 40771, 230, 128024, 133627, 58034, 130803, 12802, 129400, 133633, 19391, 73523, 125519, 132553, 136982, 132091, 95617, 55902, 64119, 137621, 40771, 112, 91043, 144147, 17877, 131670, 56983, 140532, 92751, 28002, 80573, 40771, 230, 128024, 43590, 70582, 25715, 63332, 47324, 19391, 130263, 126402, 33883, 55673, 29326, 131777, 125834, 63089, 29346, 129807, 13146, 48108, 108, 40771, 230, 129567, 54321, 47985, 83518, 65865, 124517, 124781, 80573, 40771, 112, 134372, 125511, 126291, 125160, 132537, 129238, 124517, 63089, 51588, 20401, 43115, 131833, 28754, 129413, 56290, 18411, 130039, 129985, 134313, 37087, 18411, 73523, 125519, 126204, 60985, 248, 101, 32831, 62071, 34395, 18411, 130679, 136111, 47985, 66790, 126299, 47836, 71108, 23084, 129254, 10764, 244, 19946, 13, 151645, 198, 151644, 77091, 198, 13608, 285, 30541, 53256, 1210, 3007, 11, 364, 42224, 36788, 531, 1261, 25183, 1210, 2509, 57026, 82528, 133081, 138870, 131110, 516, 364, 129616, 29346, 55054, 4089, 364, 42224, 51354, 1210, 2509, 125522, 57089, 32831, 56983, 140532, 516, 364, 28002, 130454, 144269, 516, 364, 34395, 80901, 125341, 64577, 85057, 516, 364, 57026, 82528, 133081, 138870, 131110, 4089, 364, 30487, 36788, 531, 1261, 25183, 1210, 10071, 364, 30487, 51354, 1210, 10071, 364, 19895, 5478, 53865, 36788, 531, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 126310, 57089, 32831, 56983, 140532, 92751, 133886, 129413, 92817, 130705, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 28002, 129413, 56290, 18411, 66790, 29326, 23573, 129274, 83518, 82528, 133081, 138870, 131110, 80573, 90711, 112, 29346, 55054, 132812, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 136115, 11, 126429, 80901, 125341, 64577, 85057, 130729, 66845, 128355, 125149, 28002, 23573, 126440, 124517, 130729, 40853, 64577, 37087, 19969, 135797, 64119, 131611, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 53989, 226, 28733, 128472, 15670, 364, 19895, 5478, 54160, 36788, 531, 1210, 8981, 364, 1708, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 90711, 112, 29346, 55054, 80573, 83518, 82528, 133081, 138870, 131110, 18411, 60960, 55902, 42039, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 133886, 125834, 63089, 130705, 11, 126310, 57089, 32831, 56983, 140532, 80573, 131565, 125535, 125625, 54330, 60960, 69923, 19391, 128605, 55673, 20401, 18411, 129413, 92817, 128836, 13, 23084, 16560, 94613, 40771, 230, 128024, 55054, 129360, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 90686, 3159, 92, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c10d8b0-3604-45b8-95c1-c91c02599db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 13608, 285, 30541, 53256, 1210, 3007, 11, 364, 42224, 36788, 531, 1261, 25183, 1210, 2509, 57026, 82528, 133081, 138870, 131110, 516, 364, 129616, 29346, 55054, 4089, 364, 42224, 51354, 1210, 2509, 125522, 57089, 32831, 56983, 140532, 516, 364, 28002, 130454, 144269, 516, 364, 34395, 80901, 125341, 64577, 85057, 516, 364, 57026, 82528, 133081, 138870, 131110, 4089, 364, 30487, 36788, 531, 1261, 25183, 1210, 10071, 364, 30487, 51354, 1210, 10071, 364, 19895, 5478, 53865, 36788, 531, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 126310, 57089, 32831, 56983, 140532, 92751, 133886, 129413, 92817, 130705, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 28002, 129413, 56290, 18411, 66790, 29326, 23573, 129274, 83518, 82528, 133081, 138870, 131110, 80573, 90711, 112, 29346, 55054, 132812, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 128472, 13, 136115, 11, 126429, 80901, 125341, 64577, 85057, 130729, 66845, 128355, 125149, 28002, 23573, 126440, 124517, 130729, 40853, 64577, 37087, 19969, 135797, 64119, 131611, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 53989, 226, 28733, 128472, 15670, 364, 19895, 5478, 54160, 36788, 531, 1210, 8981, 364, 1708, 1210, 364, 138870, 129567, 129502, 54321, 40853, 12802, 90711, 112, 29346, 55054, 80573, 83518, 82528, 133081, 138870, 131110, 18411, 60960, 55902, 42039, 125149, 28002, 23573, 126440, 124517, 64577, 37087, 80573, 56983, 130454, 144269, 92751, 133886, 125834, 63089, 130705, 11, 126310, 57089, 32831, 56983, 140532, 80573, 131565, 125535, 125625, 54330, 60960, 69923, 19391, 128605, 55673, 20401, 18411, 129413, 92817, 128836, 13, 23084, 16560, 94613, 40771, 230, 128024, 55054, 129360, 28733, 131870, 32831, 19391, 85403, 29281, 128533, 126440, 129321, 17877, 125714, 142588, 28733, 90686, 3159, 92, 151645]\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf6d01",
   "metadata": {},
   "source": [
    "LLM 학습에서 `input_ids`와 `labels`는 모델의 학습 목표에 따라 생성됩니다. 이를 예시 문장과 정수 인코딩을 통해 상세히 설명하겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "예를 들어, 다음과 같은 대화 데이터를 모델이 학습해야 한다고 가정합니다.  \n",
    "사용자가 **\"안녕하세요, 오늘 날씨는 어떤가요?\"**라고 물었고,  \n",
    "모델은 **\"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"**라고 응답해야 한다고 합시다.\n",
    "\n",
    "이 데이터를 학습하기 위해 먼저 전체 대화 데이터를 정수로 인코딩합니다.  \n",
    "토크나이저는 대화의 구조를 구분하기 위해 `<|im_start|>`와 `<|im_end|>`을 사용하여 정수 시퀀스를 생성한다고 가정해봅시다.  \n",
    "(실제로는 LLM 템플릿이 이보다는 복잡함을 기억하고 혼동하지 맙시다.)\n",
    "정수 시퀀스는 다음과 같이 구성될 수 있습니다.  \n",
    "\n",
    "---\n",
    "<|im_start|>user 안녕하세요, 오늘 날씨는 어떤가요?<|im_end|>  \n",
    "<|im_start|>assistant 안녕하세요! 오늘 날씨는 맑고 화창합니다.<|im_end|>\n",
    "\n",
    "---\n",
    "\n",
    "이를 정수로 변환하면 다음과 같습니다.  \n",
    "`input_ids = [1001, 2001, 3001, 4001, 5001, 6001, 7001, 1002, 1001, 8001, 9001, 1003, 2002]`  \n",
    "모델이 예측해야 하는 부분은 `assistant`의 응답인 \"안녕하세요! 오늘 날씨는 맑고 화창합니다.\"입니다. 따라서, `labels`는 다음과 같이 설정됩니다.\n",
    "\n",
    "`labels = [-100, -100, -100, -100, -100, -100, -100, -100, -100, 8001, 9001, 1003, 2002]`  \n",
    "\n",
    "이처럼 `labels`는 모델의 출력이 필요한 영역만을 포함하고, 나머지 부분은 `-100`으로 채워져 모델이 실제로 예측하고 오차를 계산해야 하는 대상(학습 대상)에서 제외됩니다. 이를 통해 모델은 불필요한 영역을 학습하지 않고, 필요한 응답 영역에만 집중할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff0f9e",
   "metadata": {},
   "source": [
    "## 5. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a511a4b7-d2f8-4942-a501-7d7a4e4cfb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    max_seq_length=max_seq_length,  # 최대 시퀀스 길이 설정\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edb9fbff-19ef-4b4b-8381-11df04aa96a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='372' max='372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [372/372 16:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.435700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.436300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.416100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()   # 모델이 자동으로 허브와 output_dir에 저장됨\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model()   # 최종 모델을 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc989b25",
   "metadata": {},
   "source": [
    "## 6. 테스트 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a7972",
   "metadata": {},
   "source": [
    "실제 모델에 입력을 넣을 때에는 입력의 뒤에 '<|im_start|>assistant'가 부착되어서 넣는 것이 좋습니다. 그래야만 모델이 바로 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b58447a2-f506-4529-ac01-a8c280cb76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for prompt in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    input = text.split('<|im_start|>assistant')[0] + '<|im_start|>assistant'\n",
    "    label = text.split('<|im_start|>assistant')[1]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed095c91-c0ab-4b6c-b60c-cf8c8314db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 주어진 뉴스로부터 종목에 영향을 주는 뉴스인지 판별하는 금융 뉴스 판별기입니다.\n",
      "두 가지 답변 케이스가 존재하며 무조건 파이썬의 dictionary 형식으로 작성하십시오.\n",
      "큰 따옴표 사이에 다른 따옴표들을 적으려고 시도하지 마십시오. 이는 dictionary 파싱을 실패하게 하는 원인이 됩니다. 따라서 주의하십시오.\n",
      "아래 dictionary에서 각 value는 지시사항에 해당합니다. 지사사항을 따라 적지마십시오. 해당 지시사항에 따라 적절한 value를 채워넣으십시오.\n",
      "해당사항이 없다면 빈 문자열 또는 빈 리스트로 적어야 합니다. 임의로 '없음' 등을 적어서는 안 됩니다.\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)이 언급되지 않거나, 특정 종목(회사)와 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": False,\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}\n",
      "\n",
      "만약 해당 뉴스가 특정 종목(회사)들과 연관되었거나, 특정 종목(회사)과 아무런 연관이 없는 뉴스일 경우에는 아래와 같이 작성합니다.\n",
      "\n",
      "답변:\n",
      "{\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들의 이름을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_positive_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"positive_keywords\": [\"긍정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 긍정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"negative_impact_stocks\": [\"파이썬 문자열 리스트의 형태로 이 뉴스가 긍정적인 영향을 줄것으로 추정되는 종목들을 작성하십시오. 약자로 적거나 별명으로 적지마십시오. 종목명으로 추정되는 한글명을 적으십시오. 뉴스로부터 추정할 수 있는 정확한 풀네임으로 적으십시오. 만약, 존재하지 않는다면 빈 리스트로 작성하십시오.\"],\n",
      "\"reason_for_negative_impact\": \"위의 종목들이 해당 뉴스로부터 긍정적인 영향을 받을 것으로 추정한 이유를 여기에다가 작성하십시오\",\n",
      "\"negative_keywords\": [\"부정적인 영향을 줄 것으로 추정되는 종목들이 존재했다면 여기에 부정적인 영향을 주는데 근거가 되었던 주요한 명사 키워드들을 파이썬 문자열 리스트 형태로 작성하십시오. 기술명, 회사명 등을 모두 포함합니다. 복합 명사 또한 허용합니다. 없다면 빈 리스트로 작성합시오.\"],\n",
      "\"summary\": \"여기에는 해당 뉴스를 요약해서 요약문을 작성하십시오\"}<|im_end|>\n",
      "<|im_start|>user\n",
      "尹대통령 김주현 금융위원장 후보자 청문보고서 송부요청\n",
      "윤석열 대통령이 4일 국회에 김주현 금융위원장 후보자의 인사청문 경과보고서 송부를 요청했다. 기한은 오는 8일까지 5일간이다. 윤 대통령은 지난달 7일 김주현 여신금융협회장을 차기 금융위원장으로 지명했지만 인사청문회가 하염없이 미뤄지면서 아직까지 임명하지 못했다. 인사청문회 1차 개최 기간도 지난달 30일로 끝났다. 김 후보자에 대한 국회 인사청문회가 불발된 것은 여야가 하반기 원 구성 문제를 두고 대립하면서 인사청문회를 맡을 상임위도 구성되지 못한 탓이다. 이에 따라 윤 대통령이 인사청문경과보고서 송부를 다시 요청하게 됐고 기일 안에 청문회가 열리지 않으면 직권으로 김 후보자를 임명할 수 있다. 특히 10일 이내 범위에서 정할 수 있는 인사청문 경과보고서 송부 기한을 비교적 짧은 5일로 정했다는 점에서 직권 임명 가능성이 높다는 관측이다. 다만 이날 여야가 후반기 원 구성 협상에서 극적으로 타결 국회가 정상화 수순을 밟게 된 만큼 인사청문회가 열릴 가능성도 남아 있다.<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bc52e99-eb23-46d9-a85f-85ef08d3e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'is_stock_related': False, 'negative_impact_stocks': None, 'negative_keywords': None, 'positive_impact_stocks': None, 'positive_keywords': None, 'reason_for_negative_impact': None, 'reason_for_positive_impact': None, 'summary': '윤석열 대통령이 국회에 김주현 금융위원장 후보자의 인사청문 경과보고서 송부를 요청했다. 여야의 대립으로 인사청문회가 지연되고 있으며, 만약 기한 내에 청문회가 열리지 않으면 대통령이 직권으로 후보자를 임명할 수 있다.'}<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3522a-b7c9-4406-8df4-9d989044b873",
   "metadata": {},
   "source": [
    "## 7. 파인 튜닝 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfe38b",
   "metadata": {},
   "source": [
    "`AutoPeftModelForCausalLM()`의 입력으로 LoRA Adapter가 저장된 체크포인트의 주소를 넣으면 LoRA Adapter가 기존의 LLM과 부착되어 로드됩니다. 이 과정은 LoRA Adapter의 가중치를 사전 학습된 언어 모델(LLM)에 통합하여 미세 조정된 모델을 완성하는 것을 의미합니다.\n",
    "\n",
    "`peft_model_id` 변수는 미세 조정된 가중치가 저장된 체크포인트의 경로를 나타냅니다. `\"qwen2-7b-rag-ko/checkpoint-285\"`는 LoRA Adapter 가중치가 저장된 위치로, 이 경로에서 해당 가중치를 불러옵니다.\n",
    "\n",
    "`fine_tuned_model`은 `AutoPeftModelForCausalLM.from_pretrained` 메서드를 통해 체크포인트를 로드하여 생성됩니다. 이 메서드는 LLM과 LoRA Adapter를 결합하고, 최적화된 설정으로 모델을 메모리에 로드합니다. `device_map=\"auto\"` 옵션은 모델을 자동으로 GPU에 배치합니다.\n",
    "\n",
    "`pipeline`은 Hugging Face의 고수준 유틸리티로, NLP 작업(예: 텍스트 생성, 번역, 요약 등)을 간단히 수행할 수 있게 해줍니다. 이 코드에서 사용된 `pipeline(\"text-generation\")`은 텍스트 생성 작업을 수행하기 위한 파이프라인 객체를 생성합니다. 파이프라인은 내부적으로 모델과 토크나이저를 관리하여, 입력 텍스트를 토큰화하고, 모델을 통해 생성된 결과를 다시 디코딩하여 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "\n",
    "이 코드는 미세 조정된 LLM을 로드한 뒤, 이를 이용해 텍스트 생성 작업을 간단히 수행할 수 있도록 준비하는 데 목적이 있습니다. `pipeline`을 통해 텍스트 생성 작업을 실행하면, 입력 텍스트에 기반하여 모델이 다음 토큰을 예측하고 이를 반복적으로 생성합니다. 이 과정은 사용자에게 자연스러운 텍스트를 출력하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15a4262b-186b-4abf-b6cf-65d22f3c10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import  AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27d9d395-a3c2-47b7-b135-34f4f78f2ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3dd505bfcc47ddb0468a7b7509bb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"qwen2-7b-rag-ko/checkpoint-372\"\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(peft_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b6fbdd3-a8db-48dc-92fa-fb51b20f2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|im_end|>\",add_special_tokens=False)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fad932e6-1c1a-4bd8-b1b5-c877a575cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c41ebf47-6905-45ad-b685-a1751613caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC'], 'negative_keywords': ['스마트폰 판매 감소', '가트너', '중국 봉쇄', '인플레이션'], 'positive_impact_stocks': [], 'positive_keywords': [],'reason_for_negative_impact': '글로벌 비즈니스 조사 기관 가트너의 전세계 스마트폰 판매 감소 전망이 애플, 엔비디아, TSMC 등의 스마트폰 제조사 및 반도체 업체들에게 부정적인 영향을 미칠 것으로 예상됩니다.','reason_for_positive_impact': '','summary': '글로벌 비즈니스 조사 기관 가트너는 전세계 스마트폰 판매가 7% 감소할 것으로 전망하며, 이는 애플, 엔비디아, TSMC 등의 스마트폰 제조사 및 반도체 업체들에게 부정적인 영향을 미칠 것으로 예상됩니다. 유럽연합은 가상자산 돈세탁을 막기 위한 규제를 강화하고 있으며, 미국 저비용 항공사 스피릿 항공의 인수전은 프론티어와 제트블루 항공 사이에서 뜨겁게 벌어지고 있습니다. 중국 대표 기술기업들인 텐센트와 바이트댄스는 경기 둔화와 규제 철퇴로 인해 대규모 감원을 계획하고 있습니다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자', '포커스미디어'], 'positive_keywords': ['야놀자', '포커스미디어', '동네가게 오래함께 캠페인', '광고 제작 및 송출 비용', '소상공인 지원'],'reason_for_negative_impact': '','reason_for_positive_impact': '야놀자와 포커스미디어가 공동으로 지역 우수 소상공인을 지원하는 캠페인을 진행하여 광고 제작 및 송출 비용을 전액 부담하며 지역 소상공인의 인지도와 매출 증대에 기여할 것으로 예상됨.','summary': '야놀자와 포커스미디어가 지역 우수 소상공인을 지원하는 캠페인을 진행하며, 지역 내 홍보를 지원하기 위해 14억 원 규모의 광고 제작 및 송출 비용을 전액 부담한다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['MSD', '위탁생산', '계약 체결', '매출 증가'],'reason_for_negative_impact': '','reason_for_positive_impact': '삼성바이오로직스가 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결함으로써 매출 증가 및 사업 확대가 예상되며, 이는 긍정적인 영향을 미칠 것으로 보인다.','summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 공급계약을 체결하여 매출 증가와 사업 확대가 예상된다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['LG전자', 'SM엔터테인먼트', '피트니스 캔디', '디지털 피트니스', 'K POP'],'reason_for_negative_impact': '','reason_for_positive_impact': 'LG전자와 SM엔터테인먼트가 협력하여 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표함으로써 새로운 사업 분야 진출과 혁신적인 디지털 피트니스 콘텐츠 제공으로 긍정적인 영향을 미칠 것으로 예상됩니다.','summary': 'LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표하여 협력으로 새로운 사업 분야 진출과 혁신적인 디지털 피트니스 콘텐츠 제공으로 긍정적인 영향을 미칠 것으로 예상됩니다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '초미세먼지', '공기청정', '공기질 관리'],'reason_for_negative_impact': '','reason_for_positive_impact': '경동나비엔의 청정환기시스템이 장마철에도 실내 미세먼지를 효과적으로 관리할 수 있다는 점이 언급되어, 이는 회사의 제품 판매 및 시장 성장에 긍정적인 영향을 미칠 수 있습니다.','summary': '경동나비엔의 청정환기시스템은 창문을 열지 않고도 실내 공기를 청정하고 환기할 수 있는 시스템으로, 미세먼지와 코로나19 등의 위험을 줄이는 데 도움을 줄 수 있습니다. 특히 주방에서 발생하는 미세먼지와 오염물질을 방지하는 '키친플러스' 제품이 주목받고 있습니다.'}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77bc0b-135e-44bc-aa6f-6e105c9dcbc1",
   "metadata": {},
   "source": [
    "## 8. 기본 모델 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7539771-6a09-4669-b828-697ff508d389",
   "metadata": {},
   "source": [
    "이번에는 LoRA Adapter를 merge하지 않은 기본 모델로 테스트 데이터에 대해서 인퍼런스해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "918de7bf-b690-460e-95af-46d74c189cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096122f69f3549cabbb71281a8fe2d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa95b03c-71e8-4453-8d97-31255c38b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    response:\n",
      "{\n",
      "\"is_stock_related\": True,\n",
      "\"positive_impact_stocks\": [\"애플\", \"엔비디아\", \"TSMC\"],\n",
      "\"reason_for_positive_impact\": \"애플을 비롯한 스마트폰 제조사 및 엔비디아, TSMC 같은 반도체 업체들이 스마트폰 판매 감소로 인한 압력이 가해질 것으로 전망됨에 따라, 이들 회사의 주가는 이에 따른 반응으로 상승할 수 있습니다.\",\n",
      "\"positive_keywords\": [\"애플\", \"엔비디아\", \"TSMC\", \"스마트폰 제조사\", \"반도체 업체\"],\n",
      "\"negative_impact_stocks\": [\"삼성전자\", \"LG전자\"],\n",
      "\"reason_for_negative_impact\": \"삼성전자와 LG전자 등의 스마트폰 제조사들이 전세계 스마트폰 판매 감소로 인해 수익 감소를 경험하게 될 것으로 예상되므로, 이들 회사의 주가는 이에 따른 반응으로 하락할 수 있습니다.\",\n",
      "\"negative_keywords\": [\"삼성전자\", \"LG전자\", \"스마트폰 제조사\"],\n",
      "\"summary\": \"올해 전세계 스마트폰 판매량이 7% 감소할 것으로 전망되며, 이로 인해 애플을 비롯한 스마트폰 제조사 및 엔비디아, TSMC 같은 반도체 업체들이 압력이 가해질 것으로 전망됩니다. 반면에 삼성전자와 LG전자 등의 스마트폰 제조사들이 수익 감소를 경험하게 될 것으로 예상됩니다.\"\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': ['애플', '엔비디아', 'TSMC', '텐센트', '바이트댄스'], 'negative_keywords': ['스마트폰 판매 감소', '인력 감축', '비용 절감', '경제 둔화'], 'positive_impact_stocks': [], 'positive_keywords': [], 'reason_for_negative_impact': '가트너의 보고서에 따르면 스마트폰 판매량의 감소는 애플과 같은 제조업체뿐만 아니라 엔비디아와 TSMC 같은 반도체 업체에게 부정적인 영향을 미칠 것으로 보입니다. 또한, 텐센트와 바이트댄스가 인력을 추가 감원할 계획을 발표함에 따라 이들의 주가에 부정적인 영향이 예상됩니다.', 'reason_for_positive_impact': '', 'summary': '가트너는 올해 전세계 스마트폰 판매량이 7% 감소할 것이라고 전망하며, 애플과 반도체 업체들이 영향을 받을 것으로 보입니다. 한편, 텐센트와 바이트댄스는 하반기 대규모 감원을 계획하고 있어, 글로벌 빅테크 기업들이 경제 둔화와 비용 절감 압박에 직면하고 있습니다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\n",
      "    \"is_stock_related\": True,\n",
      "    \"positive_impact_stocks\": [\"야놀자\", \"포커스미디어\"],\n",
      "    \"reason_for_positive_impact\": \"이 캠페인은 지역 내 우수 소상공인을 지원함으로써 해당 지역의 경제 활성화에 기여하며, 이를 통해 관련된 회사들의 매출 증대를 유도할 수 있습니다.\",\n",
      "    \"positive_keywords\": [\"야놀자\", \"포커스미디어\", \"소상공인\", \"광고 제작\", \"홍보 지원\", \"지역 경제 활성화\"],\n",
      "    \"negative_impact_stocks\": [],\n",
      "    \"reason_for_negative_impact\": \"\",\n",
      "    \"negative_keywords\": [],\n",
      "    \"summary\": \"야놀자와 포커스미디어가 공동으로 '동네가게 오래함께'라는 캠페인을 진행하며, 지역 내 우수 소상공인을 지원하고 지역 경제 활성화에 기여하는 방안을 마련하였습니다.\"\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['야놀자'], 'positive_keywords': ['야놀자', '포커스미디어', '소상공인 지원', '광고 캠페인', '지역경제 활성화'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '야놀자가 포커스미디어와 협력하여 지역 소상공인을 지원하는 캠페인을 진행함으로써 지역사회와의 상생 및 경제 활성화에 기여하며, 이는 브랜드 이미지와 매출 성장에 긍정적인 영향을 미칠 수 있다.', 'summary': \"야놀자가 포커스미디어와 함께 지역 소상공인을 지원하기 위한 '동네가게 오래함께' 캠페인을 진행하여 소상공인들의 인지도와 매출 증가에 기여할 예정이다. 캠페인은 서울시 노원구 동작구를 시작으로 점차 대상 범위를 확대할 계획이다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\n",
      "    \"is_stock_related\": True,\n",
      "    \"positive_impact_stocks\": [\"삼성바이오로직스\"],\n",
      "    \"reason_for_positive_impact\": \"삼성바이오로직스가 MSD와의 위탁생산 계약 체결로 인해 매출 증대 및 사업 확대에 따른 긍정적인 영향을 받을 것으로 예상됩니다.\",\n",
      "    \"positive_keywords\": [\"삼성바이오로직스\", \"MSD\", \"위탁생산\", \"계약 체결\", \"매출 증대\", \"사업 확대\"],\n",
      "    \"negative_impact_stocks\": [],\n",
      "    \"reason_for_negative_impact\": \"\",\n",
      "    \"negative_keywords\": []\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['삼성바이오로직스'], 'positive_keywords': ['위탁생산계약', 'MSD', '매출 증대'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '삼성바이오로직스가 미국 제약기업인 MSD와 2768억원 규모의 위탁생산계약을 체결하였으므로, 이는 회사의 매출 증대에 긍정적 영향을 미칠 수 있다.', 'summary': '삼성바이오로직스가 미국 제약기업 MSD와 2768억원 규모의 의약품 위탁생산 계약을 체결하였으며, 이는 회사의 매출 대비 상당한 규모로, 향후 매출 증가가 기대된다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\n",
      "    \"is_stock_related\": True,\n",
      "    \"positive_impact_stocks\": [\"LG전자\", \"SM엔터테인먼트\"],\n",
      "    \"reason_for_positive_impact\": \"LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표함으로써, 두 회사의 디지털 기술력과 피트니스 콘텐츠 경쟁력을 활용하여 새로운 사업 분야를 개척함으로써 긍정적인 영향을 줄 것으로 예상됩니다.\",\n",
      "    \"positive_keywords\": [\"LG전자\", \"SM엔터테인먼트\", \"디지털 피트니스 콘텐츠\", \"피트니스 캔디\", \"디지털 라이프스타일\", \"굿 디지털 라이프\"],\n",
      "    \"negative_impact_stocks\": [],\n",
      "    \"reason_for_negative_impact\": \"\",\n",
      "    \"negative_keywords\": [],\n",
      "    \"summary\": \"LG전자와 SM엔터테인먼트가 디지털 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 발표하였으며, 이로 인해 두 회사의 디지털 기술력과 피트니스 콘텐츠 경쟁력을 활용하여 새로운 사업 분야를 개척함으로써 긍정적인 영향을 줄 것으로 예상됩니다.\"\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['LG전자', 'SM엔터테인먼트'], 'positive_keywords': ['피트니스 캔디', '디지털 피트니스', 'LG전자', 'SM엔터테인먼트', 'K POP', '메타버스'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': \"LG전자와 SM엔터테인먼트가 공동으로 피트니스 콘텐츠 브랜드 '피트니스 캔디'를 출시하며 디지털 피트니스 시장에 진출하여, 양사의 디지털 기술력과 K POP 콘텐츠를 바탕으로 새로운 수익 창출과 시장 기회를 얻게 될 가능성이 높기 때문입니다.\", 'summary': \"LG전자와 SM엔터테인먼트는 디지털 피트니스 콘텐츠 합작 브랜드 '피트니스 캔디'를 발표하며, 피트니스 시장의 확장을 기대하고 있습니다. 이 플랫폼은 개인 맞춤형 커뮤니티 서비스와 K POP 콘텐츠를 결합한 디지털 피트니스 트렌드를 지향합니다.\"}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n",
      "    response:\n",
      "{\n",
      "    \"is_stock_related\": True,\n",
      "    \"positive_impact_stocks\": [\"나비엔\"],\n",
      "    \"reason_for_positive_impact\": \"나비엔의 청정환기시스템이 장마철 미세먼지 문제 해결에 도움이 될 수 있어 해당 회사의 주가가 상승할 가능성이 있습니다.\",\n",
      "    \"positive_keywords\": [\"나비엔\", \"청정환기시스템\", \"장마철\", \"미세먼지\", \"환기\"],\n",
      "    \"negative_impact_stocks\": [],\n",
      "    \"reason_for_negative_impact\": \"\",\n",
      "    \"negative_keywords\": [],\n",
      "    \"summary\": \"장마철 미세먼지 문제를 해결하는 나비엔의 청정환기시스템에 대한 기사입니다.\"\n",
      "}\n",
      "    label:\n",
      "\n",
      "{'is_stock_related': True, 'negative_impact_stocks': [], 'negative_keywords': [], 'positive_impact_stocks': ['경동나비엔'], 'positive_keywords': ['경동나비엔', '청정환기시스템', '미세먼지', 'UV LED 모듈', '공기청정'], 'reason_for_negative_impact': '', 'reason_for_positive_impact': '경동나비엔의 청정환기시스템이 미세먼지 문제와 장마철 환기 문제를 동시에 해결하는 혁신적인 솔루션으로 소개되면서, 해당 시스템에 대한 수요 증가가 예상된다.', 'summary': '경동나비엔의 청정환기시스템은 실내 공기질 개선 솔루션으로, 미세먼지 문제와 장마철 환기 어려움을 극복할 수 있는 제품으로 소개되었다. 이 시스템은 실내외의 공기를 깨끗하게 교환하며, 주방에서 발생하는 오염물질 확산도 방지한다.'}<|im_end|>\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for prompt, label in zip(prompt_lst[10:15], label_lst[10:15]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
