{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb551bd-dbc8-4b3d-8f4b-c31e11062296",
   "metadata": {},
   "source": [
    "## 1. 테스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d38e69-7641-4df0-a471-1646536dd382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 06:25:26 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from datasets import load_dataset, Dataset\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cda7a02-9573-4ee3-8bb6-c80502f051e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 허깅페이스 허브에서 데이터셋 로드\n",
    "dataset = load_dataset(\"iamjoon/ecommerce-function-calling-datasets-korean\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1743b9-9dc7-42cc-a581-461c89c09c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 비율 설정\n",
    "test_ratio = 0.2\n",
    "\n",
    "# 전체 길이와 테스트 데이터 크기 계산\n",
    "total_len = len(dataset)\n",
    "test_size = int(total_len * test_ratio)\n",
    "\n",
    "# 앞에서부터 테스트 데이터, 나머지는 학습 데이터\n",
    "test_indices = list(range(test_size))\n",
    "train_indices = list(range(test_size, total_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b067b6-9d12-4295-8773-9ad99e5c8b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "전체 데이터 분할 결과: Train 311개, Test 77개\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 포맷으로 변환 함수\n",
    "def format_conversations(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n",
    "            *sample[\"messages\"]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 분할 및 변환\n",
    "train_dataset = [format_conversations(dataset[i]) for i in train_indices]\n",
    "test_dataset = [format_conversations(dataset[i]) for i in test_indices]\n",
    "\n",
    "# 리스트를 다시 HuggingFace Dataset 객체로 변환\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "test_dataset = Dataset.from_list(test_dataset)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"\\n전체 데이터 분할 결과: Train {len(train_dataset)}개, Test {len(test_dataset)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884f3cd6-66ad-43bb-a7a7-675faa0b44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chatml(data):\n",
    "    \"\"\"\n",
    "    data: messages 리스트이거나 {\"messages\": [...]} 형태의 dict\n",
    "    반환값: ChatML 포맷의 문자열\n",
    "    \"\"\"\n",
    "    # data가 dict이고 'messages' 키가 있으면 messages 리스트를 꺼내고,\n",
    "    # 아니면 data 자체를 messages 리스트로 간주\n",
    "    messages = data.get(\"messages\") if isinstance(data, dict) and \"messages\" in data else data\n",
    "\n",
    "    parts = []\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2eca780-f5b3-468e-bc43-752e1bb1f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples(chatml: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    ChatML 문자열에서 각 assistant 응답을 분리하여\n",
    "    'input'과 'label' 쌍을 생성합니다.\n",
    "    'input'은 해당 assistant 응답 직전까지의 모든 대화 + '<|im_start|>assistant',\n",
    "    'label'은 해당 assistant의 응답 내용입니다.\n",
    "    \"\"\"\n",
    "    examples: List[Dict[str, str]] = []\n",
    "    pattern = re.compile(r'<\\|im_start\\|>assistant(.*?)(?=<\\|im_end\\|>)', re.DOTALL)\n",
    "\n",
    "    for match in pattern.finditer(chatml):\n",
    "        start_idx = match.start()\n",
    "        input_text = chatml[:start_idx].strip() + '\\n<|im_start|>assistant'\n",
    "        label_text = match.group(1).strip()\n",
    "        examples.append({\n",
    "            \"input\": input_text,\n",
    "            \"label\": label_text\n",
    "        })\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2116120-6322-4d67-ab0c-e7b815049091",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for item in test_dataset:\n",
    "    chatml = to_chatml(item)  # ChatML 문자열로 변환\n",
    "    examples = extract_examples(chatml)  # assistant 응답 단위로 분리\n",
    "\n",
    "    for ex in examples:\n",
    "        prompt_lst.append(ex['input'])\n",
    "        label_lst.append(ex['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02874cd-ea6a-4db8-9b27-86ec4964ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 상준몰의 AI 상담사입니다. 성심성의껏 상담하십시오.\n",
      "\n",
      "로그인한 사용자의 현재 ID: U006\n",
      "오늘 날짜: 2024-02-02\n",
      "\n",
      "# Tools\n",
      "\n",
      "You may call one or more functions to assist with the user query.\n",
      "\n",
      "You are provided with function signatures within <tools></tools> XML tags:\n",
      "<tools>\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"add_to_cart\", \"description\": \"사용자의 장바구니에 지정된 상품(product_id)과 수량(quantity)을 추가합니다. 동일 상품이 이미 있으면 수량을 증가시키고, 새 항목으로 추가합니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"장바구니에 상품을 추가할 사용자의 고유 식별자 (예: 'U001')\"}, \"product_id\": {\"type\": \"string\", \"description\": \"장바구니에 추가할 상품의 고유 식별자 (예: 'P003')\"}, \"quantity\": {\"type\": \"integer\", \"description\": \"추가할 상품 수량 (기본값: 1)\", \"default\": 1, \"minimum\": 1}}, \"required\": [\"user_id\", \"product_id\"], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"view_order_history\", \"description\": \"사용자의 전체 주문 내역을 반환합니다. 각 주문에 대해 주문 번호, 주문 일자, 총 결제 금액, 결제 상태, 배송 상태, 택배사, 운송장 번호, 배송 진행 단계, 주문에 포함된 상품명 목록을 제공합니다. 이 함수로도 최근 주문의 배송 상태를 확인할 수 있습니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"주문 내역을 조회할 사용자의 고유 식별자 (예: 'U001')\"}}, \"required\": [\"user_id\"], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"view_user_profile\", \"description\": \"사용자의 기본 프로필 정보(이름, 이메일, 전화번호, 주소, 포인트, 멤버십)와 보유 쿠폰 정보를 반환합니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"프로필을 조회할 사용자의 고유 식별자, 예: 'U001'\"}}, \"required\": [\"user_id\"], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"show_cart\", \"description\": \"지정된 사용자(user_id)의 현재 장바구니에 담긴 모든 상품을 반환합니다. 각 항목에 대해 상품 ID, 상품명, 가격, 수량, 장바구니에 추가된 일시를 포함합니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"장바구니를 조회할 사용자의 고유 식별자 (예: 'U001')\"}}, \"required\": [\"user_id\"], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"remove_from_cart\", \"description\": \"사용자의 장바구니에서 상품명을 포함하는 키워드(keyword) 또는 특정 상품 ID(product_id)를 사용해 항목을 제거합니다. keyword 또는 product_id 중 하나를 반드시 지정해야 합니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"장바구니에서 상품을 제거할 사용자의 고유 식별자 (예: 'U001')\"}, \"keyword\": {\"type\": \"string\", \"description\": \"상품명에 포함된 키워드로 제거 대상을 지정 (예: '우산')\"}, \"product_id\": {\"type\": \"string\", \"description\": \"제거할 정확한 상품 ID (예: 'P005')\"}}, \"required\": [\"user_id\"], \"anyOf\": [{\"required\": [\"keyword\"]}, {\"required\": [\"product_id\"]}], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"search_policy_info\", \"description\": \"지정된 키워드에 대한 상준몰 정책/약관 정보를 검색합니다. 예: '주문 취소', '반품', '환불', '배송', '재입고'에 관련된 약관만 질문하세요.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"keyword\": {\"type\": \"string\", \"description\": \"정책을 검색할 키워드 (예: '주문 취소', '반품', '환불', '배송', '재입고'만 넣을 수 있습니다.)\"}}, \"required\": [\"keyword\"], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"view_order_details\", \"description\": \"특정 주문(order_id)의 상세 내역을 조회합니다. 주문에 포함된 각 상품의 ID, 상품명, 수량, 단가, 할인 적용 가격을 반환합니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"주문 상세를 조회할 사용자의 고유 식별자 (예: 'U001')\"}, \"order_id\": {\"type\": \"string\", \"description\": \"상세 내역을 조회할 주문의 고유 식별자 (예: 'O001')\"}}, \"required\": [\"user_id\", \"order_id\"], \"additionalProperties\": false}}}\n",
      "{\"type\": \"function\", \"function\": {\"name\": \"search_product\", \"description\": \"상품명에 특정 키워드를 포함하는 제품을 검색하고, 평점 순으로 정렬된 결과를 반환합니다. 필요시 카테고리(category)로 추가 필터링이 가능합니다.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"keyword\": {\"type\": \"string\", \"description\": \"검색할 상품명 키워드 (예: '노트북')\"}, \"category\": {\"type\": \"string\", \"description\": \"선택적 카테고리 필터 (예: '전자기기')\"}}, \"required\": [\"keyword\"], \"additionalProperties\": false}}}\n",
      "</tools>\n",
      "\n",
      "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
      "<tool_call>\n",
      "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "안녕하세요. 최근에 주문한 내역 좀 확인하고 싶어요.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>\n",
      "{\"name\": \"view_order_history\", \"arguments\": {\"user_id\": \"U006\"}}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>user\n",
      "<tool_response>\n",
      "[{\"order_id\": \"O102\", \"order_date\": \"2023-12-15\", \"total\": 55000, \"payment_status\": \"결제완료\", \"delivery_status\": \"배송완료\", \"courier\": \"한진택배\", \"tracking_number\": \"123456789012\", \"status\": \"배송완료\", \"products\": \"무선 이어폰\"}, {\"order_id\": \"O100\", \"order_date\": \"2023-11-20\", \"total\": 78000, \"payment_status\": \"결제완료\", \"delivery_status\": \"배송완료\", \"courier\": \"CJ대한통운\", \"tracking_number\": \"987654321098\", \"status\": \"배송완료\", \"products\": \"블루투스 스피커\"}]\n",
      "</tool_response><|im_end|>\n",
      "<|im_start|>assistant\n",
      "고객님께서는 2023년 12월 15일에 '무선 이어폰', 2023년 11월 20일에 '블루투스 스피커'를 주문하셨으며, 두 상품 모두 배송 완료된 상태입니다. 더 궁금하신 사항이 있으신가요?<|im_end|>\n",
      "<|im_start|>user\n",
      "네, 그리고 요즘 인기 있는 노트북 상품 좀 추천해 주세요.<|im_end|>\n",
      "<|im_start|>assistant\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lst[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b046ff48-e4f1-4e90-8748-802bc049f410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_call>\n",
      "{\"name\": \"search_product\", \"arguments\": {\"keyword\": \"노트북\"}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "print(label_lst[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93200e3c-0d2a-4788-ad8d-efce1cfb9993",
   "metadata": {},
   "source": [
    "## 2. 모델 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3646a76-8c91-4dc2-b9ca-8312d00ccf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    stop=[\"<|im_end|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e290f721-0a21-454a-b3bf-4023a861d1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6792647c177f4b708bbf36bbf35615c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 06:26:45 [config.py:585] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-22 06:26:45 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c2be2f5d3a48cf9cca1acc1fb20c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65732a439264441f83b28ade28135216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c4d6dc7c294b3a9d46272aefb2e167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44a8960e51545fba67c74a086049329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e4c4e4129148988ea308fb07bb4175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 06:26:48 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-22 06:26:49 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x77672cda2d40>\n",
      "INFO 04-22 06:26:49 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-22 06:26:49 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-22 06:26:50 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "WARNING 04-22 06:26:50 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 04-22 06:26:50 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49c2f5c94684fc68d49e0a5ead6cae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0144b9fbcd48d889d4c9df86d74f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f7ace4ba61493f9b3a8820b0d9fbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8239a91e88c445ec8b5e1b0347479b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 06:27:29 [weight_utils.py:281] Time spent downloading weights for Qwen/Qwen2.5-7B-Instruct: 38.528593 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345967df10bc4f4dbab7ea52dd975e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbf72deea5b40108374f1695587d5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 06:27:31 [loader.py:447] Loading weights took 1.84 seconds\n",
      "INFO 04-22 06:27:31 [gpu_model_runner.py:1186] Model loading took 14.2487 GB and 40.861930 seconds\n",
      "INFO 04-22 06:27:37 [backends.py:415] Using cache directory: /root/.cache/vllm/torch_compile_cache/a207a5ab3c/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-22 06:27:37 [backends.py:425] Dynamo bytecode transform time: 5.93 s\n",
      "INFO 04-22 06:27:40 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 04-22 06:28:02 [backends.py:144] Compiling a graph for general shape takes 24.50 s\n",
      "INFO 04-22 06:28:16 [monitor.py:33] torch.compile takes 30.43 s in total\n",
      "INFO 04-22 06:28:16 [kv_cache_utils.py:566] GPU KV cache size: 953,616 tokens\n",
      "INFO 04-22 06:28:16 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 29.10x\n",
      "INFO 04-22 06:28:42 [gpu_model_runner.py:1534] Graph capturing finished in 25 secs, took 0.48 GiB\n",
      "INFO 04-22 06:28:42 [core.py:151] init engine (profile, create kv cache, warmup model) took 71.15 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f606489-3f7e-4c57-9476-e3e44438a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 655/655 [00:19<00:00, 32.84it/s, est. speed input: 65619.01 toks/s, output: 2315.51 toks/s] \n"
     ]
    }
   ],
   "source": [
    "base_model_outputs = llm.generate(prompt_lst, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f4b51a-902b-4d18-aa22-10b1811adc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_text_results = [sample.outputs[0].text.strip() for sample in base_model_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfbed2f-7efe-43c9-a4dc-65096556004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_call>\n",
      "{\"name\": \"search_product\", \"arguments\": {\"keyword\": \"노트북\"}}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "print(base_model_text_results[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca76ec-9836-47d1-b850-5430d49d9c03",
   "metadata": {},
   "source": [
    "## 3. 평가 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b81eb9a-3f68-4d91-a5c0-5b0044823261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"prompt\": prompt_lst,\n",
    "    \"label\": label_lst,\n",
    "    \"output\": base_model_text_results\n",
    "})\n",
    "\n",
    "# 저장\n",
    "df.to_csv(\"base_model_evaluation_results.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde683b6-109e-4ad1-93ac-0e8edb55c135",
   "metadata": {},
   "source": [
    "## 4. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e66f98fd-715f-4195-a3cc-4c8a0a52730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_function_calls(labels, predictions):\n",
    "    \"\"\"\n",
    "    펑션 콜링 성능을 평가하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : list\n",
    "        정답 레이블 목록\n",
    "    predictions : list\n",
    "        모델이 예측한 결과 목록\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        tool_selection: 함수 이름 일치율\n",
    "        params_selection: 파라미터 키 일치율 \n",
    "        params_value_accuracy: 파라미터 값 일치율\n",
    "        total_samples: 전체 tool_call 샘플 수\n",
    "    \"\"\"\n",
    "    # 결과 저장할 딕셔너리 초기화\n",
    "    results = {\n",
    "        'tool_selection': {'correct': 0, 'total': 0},\n",
    "        'params_selection': {'correct': 0, 'total': 0},\n",
    "        'params_value_accuracy': {'correct': 0, 'total': 0}\n",
    "    }\n",
    "    \n",
    "    # tool_call 형식만 필터링하기 위한 정규표현식\n",
    "    tool_call_pattern = re.compile(r'<tool_call>(.*?)</tool_call>', re.DOTALL)\n",
    "    \n",
    "    # 전체 샘플 중 tool_call 샘플 수\n",
    "    tool_call_count = 0\n",
    "    \n",
    "    for label, pred in zip(labels, predictions):\n",
    "        # tool_call 형식인지 확인\n",
    "        label_match = tool_call_pattern.search(label)\n",
    "        pred_match = tool_call_pattern.search(pred)\n",
    "        \n",
    "        # 레이블이 tool_call이 아니면 건너뛰기\n",
    "        if not label_match:\n",
    "            continue\n",
    "        \n",
    "        tool_call_count += 1\n",
    "        \n",
    "        # 예측이 tool_call 형식이 아니면 모든 지표가 틀린 것으로 처리\n",
    "        if not pred_match:\n",
    "            results['tool_selection']['total'] += 1\n",
    "            results['params_selection']['total'] += 1\n",
    "            results['params_value_accuracy']['total'] += 1\n",
    "            continue\n",
    "        \n",
    "        # JSON 파싱\n",
    "        try:\n",
    "            label_json = json.loads(label_match.group(1))\n",
    "            pred_json = json.loads(pred_match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            # JSON 파싱 오류 시 모든 지표가 틀린 것으로 처리\n",
    "            results['tool_selection']['total'] += 1\n",
    "            results['params_selection']['total'] += 1\n",
    "            results['params_value_accuracy']['total'] += 1\n",
    "            continue\n",
    "        \n",
    "        # 1. 함수 이름 일치 여부 (tool_selection)\n",
    "        results['tool_selection']['total'] += 1\n",
    "        if label_json.get('name') == pred_json.get('name'):\n",
    "            results['tool_selection']['correct'] += 1\n",
    "        \n",
    "        # 2. 파라미터 키 일치 여부 (params_selection)\n",
    "        # 개별 파라미터별로 맞고 틀림을 채점\n",
    "        label_params = set(label_json.get('arguments', {}).keys())\n",
    "        pred_params = set(pred_json.get('arguments', {}).keys())\n",
    "        \n",
    "        # 각 파라미터마다 평가를 위해 모든 파라미터 순회\n",
    "        for param in label_params:\n",
    "            results['params_selection']['total'] += 1\n",
    "            if param in pred_params:\n",
    "                results['params_selection']['correct'] += 1\n",
    "        \n",
    "        # 예측에만 있는 추가 파라미터도 틀린 것으로 평가\n",
    "        for param in pred_params:\n",
    "            if param not in label_params:\n",
    "                results['params_selection']['total'] += 1\n",
    "                # correct는 증가 안 함 (틀린 것이므로)\n",
    "        \n",
    "        # 3. 파라미터 값 일치 여부 (params_value_accuracy)\n",
    "        # 존재하는 공통 파라미터에 대해서만 값 일치 여부 평가\n",
    "        label_args = label_json.get('arguments', {})\n",
    "        pred_args = pred_json.get('arguments', {})\n",
    "        \n",
    "        # 공통으로 존재하는 파라미터 키 찾기\n",
    "        common_params = label_params.intersection(pred_params)\n",
    "        \n",
    "        if common_params:  # 공통 파라미터가 있는 경우에만 평가\n",
    "            results['params_value_accuracy']['total'] += 1\n",
    "            \n",
    "            # 공통 파라미터의 값이 모두 일치하는지 확인\n",
    "            values_match = True\n",
    "            for key in common_params:\n",
    "                if label_args.get(key) != pred_args.get(key):\n",
    "                    values_match = False\n",
    "                    break\n",
    "            \n",
    "            if values_match:\n",
    "                results['params_value_accuracy']['correct'] += 1\n",
    "    \n",
    "    # 최종 결과 계산\n",
    "    final_results = {}\n",
    "    for metric, counts in results.items():\n",
    "        if counts['total'] > 0:\n",
    "            final_results[metric] = counts['correct'] / counts['total']\n",
    "        else:\n",
    "            final_results[metric] = 0.0\n",
    "    \n",
    "    final_results['total_samples'] = tool_call_count\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a65bc97-a1cf-4cb8-87a4-23bf2f97c038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_selection: 78.06%\n",
      "params_selection: 78.93%\n",
      "params_value_accuracy: 77.84%\n",
      "total_samples: 196\n"
     ]
    }
   ],
   "source": [
    "labels = df['label'].to_list()\n",
    "preds = df['output'].to_list()\n",
    "\n",
    "results_with_errors = evaluate_function_calls(labels, preds)\n",
    "for metric, value in results_with_errors.items():\n",
    "    if metric != 'total_samples':\n",
    "        print(f\"{metric}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"{metric}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
