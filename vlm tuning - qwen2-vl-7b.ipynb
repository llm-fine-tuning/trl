{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f86e530-38e7-43ab-9b0a-6cdee41c2b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-24.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9627157b-ef0f-434a-9fdc-9f041e4ba8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install -q tensorboard wandb \n",
    " \n",
    "# Install Hugging Face libraries\n",
    "%pip install -q --upgrade \\\n",
    "  \"transformers==4.45.1\" \\\n",
    "  \"datasets==3.0.1\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.44.0\" \\\n",
    "  \"trl==0.11.1\" \\\n",
    "  \"peft==0.13.0\" \\\n",
    "  \"qwen-vl-utils\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c80169c-419f-4ef4-940d-c532b7aab5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow==9.4.0\n",
      "  Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
      "Downloading Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m184.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "Successfully installed pillow-9.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pillow==9.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b38728-a0b7-42ba-9b30-74790626909b",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045dace-8599-40ba-ad6c-2c8f48e12079",
   "metadata": {},
   "source": [
    "과제물 사진과 과제물 정보를 주고 실제 정답을 맞추는 모델을 개발합니다.\n",
    "\n",
    "이 모델은 선생님들이 학생들의 수행평가를 채점한다고 가정합니다. 본 프로젝트는 실제 교육 현장의 데이터와 차이가 있을 수 있으나, 자동 채점 시스템의 기본 원리와 가능성을 탐구하는 데 중점을 둡니다.\n",
    "\n",
    "이번 예시에서는 Ko-SciecneQA 데이터셋을 사용할 건이중에서 이미지를 가지고 있는 6,218개의 데이터 중 시간 절약을 위해서 모두 사용하지는 않고 여기서 20%(1,243)만 사용하겠습니다.\n",
    "\n",
    "이미지, 문제, 힌트을 기반으로 정답을 생성하도록 모델을 파인튜닝하려 합니다.\n",
    "따라서 이미지, 문제, 힌트를 포함한 입력을 만들고, 이를 이용하여 정답을 찾아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a617c07c-c7db-4a36-b897-85be4fe47901",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"\"\"질문: {korean_question}\n",
    "선택지: {korean_choices}\n",
    "힌트: {korean_hint}\"\"\"\n",
    "\n",
    "system_message = \"주어진 이미지와 질문을 바탕으로 답변하세요.\\n이때 정답은 선택지 중 1개를 선택해야하며 힌트가 주어질 수 있습니다. 가장 적절한 답을 1개 선택하세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db04425-f12f-4b14-841c-ab1981732b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "   return {\"messages\": [\n",
    "               {\n",
    "                   # 시스템 프롬프트\n",
    "                   \"role\": \"system\", \n",
    "                   \"content\": [{\"type\": \"text\", \"text\": system_message}], \n",
    "               },\n",
    "               {\n",
    "                   # 유저 프롬프트\n",
    "                   \"role\": \"user\",  \n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           \"text\": prompt.format(\n",
    "                              korean_question=sample[\"korean_question\"], \n",
    "                              korean_choices=sample[\"korean_choices\"], \n",
    "                              korean_hint=sample[\"korean_hint\"]\n",
    "                              ),\n",
    "                       },{\n",
    "                           \"type\": \"image\", \n",
    "                           \"image\": sample[\"image\"] \n",
    "                                if sample[\"image\"] is not None else \"\", \n",
    "                       }\n",
    "                   ],\n",
    "               },\n",
    "               {\n",
    "                   # AI 어시스턴트 답변\n",
    "                   \n",
    "                   \"role\": \"assistant\", \n",
    "                   \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": sample[\"answer_str\"]\n",
    "                        }\n",
    "                    ], \n",
    "               },\n",
    "           ],\n",
    "       }\n",
    "\n",
    "# 허브에서 데이터셋 로드 및 이미지가 존재하는 경우만 필터링\n",
    "dataset = load_dataset(\"daje/Ko-SciecneQA\", split=\"train\")\n",
    "dataset = dataset.filter(lambda example: example[\"image\"] is not None)\n",
    "dataset = [format_data(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26aa634d-c930-4cdf-956c-0532ec35b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수: 6218\n"
     ]
    }
   ],
   "source": [
    "print('데이터의 개수:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a4bb0fc-5f68-41e4-9da9-b90573df9f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 데이터 출력:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '주어진 이미지와 질문을 바탕으로 답변하세요.\\n이때 정답은 선택지 중 1개를 선택해야하며 힌트가 주어질 수 있습니다. 가장 적절한 답을 1개 선택하세요.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"질문: 다음 주 중에서 가장 북쪽에 있는 곳은 어디인가요?\\n선택지: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\\n힌트: \"},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429>}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': '웨스트버지니아'}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('첫번째 데이터 출력:')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53377741-3ed8-4615-9f14-37c49f3d745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "힌트가 존재하는 데이터 출력:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '주어진 이미지와 질문을 바탕으로 답변하세요.\\n이때 정답은 선택지 중 1개를 선택해야하며 힌트가 주어질 수 있습니다. 가장 적절한 답을 1개 선택하세요.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"질문: 각 쌍의 자석 사이의 자기력을 생각해보세요. 다음 중 어떤 진술이 맞나요?\\n선택지: ['두 쌍의 자기력 크기는 동일하다.', '쌍 1의 자기력 크기가 더 크다.', '쌍 2의 자기력 크기가 더 크다.']\\n힌트: 아래 이미지는 두 쌍의 자석을 보여줍니다. 다른 쌍의 자석들은 서로 영향을 미치지 않습니다. 보여진 모든 자석은 같은 재료로 만들어졌지만, 크기와 모양이 다를 수 있습니다.\"},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=448x171>}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text', 'text': '쌍 2의 자기력 크기가 더 크다.'}]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('힌트가 존재하는 데이터 출력:')\n",
    "dataset[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52324dce-d425-4659-9bbd-b5a2173ae586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:int(len(dataset) * 0.9)]\n",
    "test_dataset = dataset[int(len(dataset) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5fdfa8-81b6-47b2-9676-bdf2909070de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터의 개수: 5596\n",
      "테스트 데이터의 개수: 622\n"
     ]
    }
   ],
   "source": [
    "print('학습 데이터의 개수:', len(train_dataset))\n",
    "print('테스트 데이터의 개수:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f4457-5111-4cdd-8c3e-a554263df60e",
   "metadata": {},
   "source": [
    "## trl의 SFTTrainer를 이용한 파인 튜닝당하는 모듈입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284074ff-40af-4efe-8df7-9a565bd56069",
   "metadata": {},
   "source": [
    "trl의 SFTTrainer를 사용해 모델을 파인튜닝합니다. SFTTrainer는 오픈소스 LLM과 VLM의 지도 파인튜닝을 매우 간단하게 만들어줍니다.  \n",
    "SFTTrainer는 transformers 라이브러리의 Trainer를 상속받아서 로깅, 평가, 체크포인트 등 모든 기능을 지원하면서도 추가적인 편의 기능을 제공합니다.\n",
    "\n",
    "이번 예시에서는 PEFT 기능을 사용할 예정입니다.  \n",
    "PEFT 방법으로는 QLoRA를 사용할 건데, 이는 양자화와 LoRA 튜닝을 같이 사용하여 대규모 언어 모델의 메모리 사용량을 줄이는 기술입니다.\n",
    "\n",
    "참고: 멀티모달 입력에 패딩이 필요하기 때문에 Flash Attention은 사용할 수 없습니다.  \n",
    "Qwen 2 VL 72B 모델을 사용할 예정이지만, model_id 변수만 바꾸면 Meta AI의 Llama-3.2-11B-Vision, Mistral AI의 Pixtral-12B 등 다른 모델로도 쉽게 교체할 수 있습니다. bitsandbytes를 사용해 모델을 4비트로 양자화할 예정입니다.\n",
    "\n",
    "참고: 모델이 클수록 더 많은 메모리가 필요합니다.  \n",
    "VLM 학습을 위해 LLM, 토크나이저, 프로세서를 올바르게 준비하는 것이 매우 중요합니다. 프로세서는 특수 토큰과 이미지를 입력에 포함시키는 역할을 담당하는 모듈입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8244cee-fbab-4b38-8043-d6168aeed662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c038efa3db44263a281807b4560c4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    " \n",
    "# 허깅페이스에서 제공하는 Qwen 시리즈의 비전-언어 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    " \n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   # 가용한 GPU 메모리에 모델을 자동으로 최적 할당하는 설정\n",
    "   device_map=\"auto\",     \n",
    "   # 메모리 효율과 연산 속도를 위해 bfloat16 형식의 부동소수점 정밀도 사용             \n",
    "   torch_dtype=torch.bfloat16,                  \n",
    ")\n",
    " \n",
    "# 입력 텍스트와 이미지를 모델이 이해할 수 있는 형태로 변환하는 전처리기 로드\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd10516-d548-4e9a-b332-948a6136d1f0",
   "metadata": {},
   "source": [
    "기존 LLM 템플릿\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|im_end|>\n",
    "<|im_start|>assistant\n",
    "답변<|im_end|>\n",
    "```\n",
    "\n",
    "VLM 템플릿\n",
    "```python\n",
    "<|im_start|>system\n",
    "시스템 프롬프트<|im_end|>\n",
    "<|im_start|>user\n",
    "사용자의 질문<|vision_start|>이미지<|vision_end|><|im_end|>\n",
    "<|im_start|>assistant\n",
    "답변<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4829b3-928f-427b-b1fe-dfc5b5872c7c",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0231a1df-d5b0-47cb-a6d7-d35d1aa82112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "       # 모델 가중치에 LoRA 업데이트를 적용하는 정도를 조절하는 스케일링 계수\n",
    "       lora_alpha=128,\n",
    "       # 과적합을 방지하기 위한 드롭아웃 비율 설정\n",
    "       lora_dropout=0.05,\n",
    "       # LoRA의 순위(rank) - 저차원 행렬의 차원을 결정\n",
    "       r=256,\n",
    "       # 편향(bias) 업데이트 여부 - 'none'은 편향을 업데이트하지 않음\n",
    "       bias=\"none\",\n",
    "       # LoRA를 적용할 대상 모듈들 - 트랜스포머 모델의 주요 투영 레이어들\n",
    "       target_modules=[\n",
    "           \"q_proj\",    # Query 투영 레이어\n",
    "           \"up_proj\",   # FFN 상향 투영 레이어\n",
    "           \"o_proj\",    # Output 투영 레이어\n",
    "           \"k_proj\",    # Key 투영 레이어\n",
    "           \"down_proj\", # FFN 하향 투영 레이어\n",
    "           \"gate_proj\", # FFN 게이트 투영 레이어\n",
    "           \"v_proj\"     # Value 투영 레이어\n",
    "       ],\n",
    "       # 작업 유형 지정 - 인과적 언어 모델링(다음 토큰 예측)\n",
    "       task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d33300-5ef2-41bf-be95-a66ca0e5a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# SFTConfig를 통해 학습 설정을 정의\n",
    "args = SFTConfig(\n",
    "    # 학습된 모델과 체크포인트를 저장할 디렉터리 경로 및 리포지토리 ID\n",
    "    output_dir=\"output_dir\",\n",
    "    # 전체 학습 에포크 수 (데이터셋을 몇 번 반복할지 설정)\n",
    "    num_train_epochs=3,                                     \n",
    "    # 각 장비(GPU)당 사용될 배치 사이즈 (메모리와 연관됨)\n",
    "    per_device_train_batch_size=4,                          \n",
    "    # 경사 누적 스텝 수 (이 횟수만큼 기울기를 누적한 후 업데이트)\n",
    "    gradient_accumulation_steps=8,                          \n",
    "    # 메모리 절약을 위한 gradient checkpointing 활성화 (메모리 최적화)\n",
    "    gradient_checkpointing=True,                            \n",
    "    # AdamW 옵티마이저 (fused 버전 사용으로 학습 속도 향상)\n",
    "    optim=\"adamw_torch_fused\",                              \n",
    "    # 몇 스텝마다 로그를 출력할지 설정 (여기선 5 스텝마다 로그)\n",
    "    logging_steps=5,                                        \n",
    "    # 매 에포크마다 체크포인트 저장 설정\n",
    "    save_strategy=\"epoch\",                                  \n",
    "    # 학습의 정도. 실험을 통해 결정하는 것을 권장\n",
    "    learning_rate=1e-4,                                     \n",
    "    # bfloat16 정밀도 사용 (메모리 절약 및 속도 향상)\n",
    "    bf16=True,                                              \n",
    "    # tf32 정밀도 사용 (NVIDIA GPU에서 학습 속도 향상)\n",
    "    tf32=True,                                              \n",
    "    # 기울기 클리핑을 위한 최대 기울기 값 (QLoRA 논문에서 추천된 값)\n",
    "    max_grad_norm=0.3,                                      \n",
    "    # 학습 초기에 학습률을 점진적으로 올리는 warmup 비율 (QLoRA 논문에서 추천된 값)\n",
    "    warmup_ratio=0.03,                                      \n",
    "    # 일정한 학습률 스케줄러 사용 (학습률이 변하지 않음)\n",
    "    lr_scheduler_type=\"constant\",                                            \n",
    "    # TensorBoard를 통해 학습 상태를 모니터링\n",
    "    report_to=\"tensorboard\",                                \n",
    "    # reentrant gradient checkpointing 설정 (비재진입 방식 사용)\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, \n",
    "    # 데이터셋에서 텍스트 필드를 위한 더미 필드 (collator에서 필요)\n",
    "    dataset_text_field=\"\",                                  \n",
    "    # collator에서 데이터셋 전처리를 건너뛰기 위한 설정\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    # 불필요한 열 삭제하지 않도록 설정 (학습 중 사용되지 않는 열이라도 유지)\n",
    "    remove_unused_columns = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0728c022-c4cb-4c78-8342-8f3825aeb0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 이미지 쌍을 인코딩하기 위한 데이터 collator 함수 정의\n",
    "def collate_fn(examples):\n",
    "    # 각 예제에서 텍스트와 이미지를 추출하고, 텍스트는 채팅 템플릿을 적용\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    # 텍스트를 토크나이징하고 이미지를 처리하여 일괄 처리(batch) 형태로 변환\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # labels로 사용할 input_ids 복사본 생성 후, 패딩 토큰을 -100으로 설정하여 손실 계산 시 무시하도록 함\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # 패딩 토큰 손실 계산 제외\n",
    "\n",
    "    # 특정 이미지 토큰 인덱스는 손실 계산에서 무시 (모델에 따라 다름)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  \n",
    "        # Qwen2VL 모델의 이미지 토큰 인덱스\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "    else:\n",
    "        # 다른 모델에서 이미지 토큰 ID를 얻어 손실 계산에서 제외\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    \n",
    "    # 손실 계산 시 이미지 토큰 인덱스를 무시하도록 설정\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    \n",
    "    # 배치에 labels 추가 (손실 계산 시 사용)\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f5d2603-8588-4413-b796-399d8cb08f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': '주어진 이미지와 질문을 바탕으로 답변하세요.\\n이때 정답은 선택지 중 1개를 선택해야하며 힌트가 주어질 수 있습니다. 가장 적절한 답을 1개 선택하세요.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': \"질문: 다음 주 중에서 가장 북쪽에 있는 곳은 어디인가요?\\n선택지: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\\n힌트: \"}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429 at 0x7F90596326B0>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': '웨스트버지니아'}]}]}\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74eeae6-6bf2-4ef4-8abb-42eb18eb2dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 538])\n",
      "어텐션 마스크 형태: torch.Size([1, 538])\n",
      "이미지 픽셀 형태: torch.Size([1620, 1176])\n",
      "레이블 형태: torch.Size([1, 538])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79d91322-b0fc-4420-bb32-a50febfb41ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  54330,  31079,  85251,  90667,  21329,  80573,\n",
      "        138520,  17877,  81718, 144059,  42039, 143604,  91145,    624,  12802,\n",
      "        131866,  36055, 132760,  33704,  86038,  21329,  70943,    220,     16,\n",
      "         59761,  18411,  86038, 129264, 130705,  10764,    252,    234,  28626,\n",
      "         19969,  55673,  31079, 128732,  28733, 128472,     13, 130887, 135968,\n",
      "        126550,  23573, 143603,  17877,    220,     16,  59761,  86038,  91145,\n",
      "            13, 151645,    198, 151644,    872,    198, 128732,  51588,     25,\n",
      "        126844,  55673,  70943,  56475, 130887, 139963, 132064,  19391,  64521,\n",
      "         45130,    111,  33704, 139740,  31328,  19969,  35711,   5267,  14559,\n",
      "         75132,  21329,     25,   2509, 144025,  53189,  79004,  21329,  83036,\n",
      "         52959,    516,    364, 126746,  12802,  21329, 126898,  60315,    516,\n",
      "           364, 126898,  28002,  92817,  60315,    516,    364,  34992,  44680,\n",
      "           223,    112,  50340,  47324, 125544,   4432, 144190,  28626,     25,\n",
      "           220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151653, 151645,    198, 151644,  77091,    198, 144025,\n",
      "         53189,  79004,  21329,  83036,  52959, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c263a2e-dd94-4f7f-b0ad-8f8d21326bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  54330,  31079,  85251,  90667,  21329,  80573,\n",
      "        138520,  17877,  81718, 144059,  42039, 143604,  91145,    624,  12802,\n",
      "        131866,  36055, 132760,  33704,  86038,  21329,  70943,    220,     16,\n",
      "         59761,  18411,  86038, 129264, 130705,  10764,    252,    234,  28626,\n",
      "         19969,  55673,  31079, 128732,  28733, 128472,     13, 130887, 135968,\n",
      "        126550,  23573, 143603,  17877,    220,     16,  59761,  86038,  91145,\n",
      "            13, 151645,    198, 151644,    872,    198, 128732,  51588,     25,\n",
      "        126844,  55673,  70943,  56475, 130887, 139963, 132064,  19391,  64521,\n",
      "         45130,    111,  33704, 139740,  31328,  19969,  35711,   5267,  14559,\n",
      "         75132,  21329,     25,   2509, 144025,  53189,  79004,  21329,  83036,\n",
      "         52959,    516,    364, 126746,  12802,  21329, 126898,  60315,    516,\n",
      "           364, 126898,  28002,  92817,  60315,    516,    364,  34992,  44680,\n",
      "           223,    112,  50340,  47324, 125544,   4432, 144190,  28626,     25,\n",
      "           220,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100, 151645,    198, 151644,  77091,    198, 144025,\n",
      "         53189,  79004,  21329,  83036,  52959, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "466b2043-3c99-461b-ba50-b1a11fd29882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "주어진 이미지와 질문을 바탕으로 답변하세요.\n",
      "이때 정답은 선택지 중 1개를 선택해야하며 힌트가 주어질 수 있습니다. 가장 적절한 답을 1개 선택하세요.<|im_end|>\n",
      "<|im_start|>user\n",
      "질문: 다음 주 중에서 가장 북쪽에 있는 곳은 어디인가요?\n",
      "선택지: ['웨스트버지니아', '루이지애나', '애리조나', '오클라호마']\n",
      "힌트: <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "웨스트버지니아<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4095ff-cbeb-4172-bb38-07bb2cac1026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    # 앞서 로드한 Qwen2-VL 모델\n",
    "    model=model,                   \n",
    "    # SFTConfig를 통해 정의한 학습 설정\n",
    "    args=args,                      \n",
    "    # 학습에 사용할 데이터셋\n",
    "    train_dataset=train_dataset,    \n",
    "    # 데이터 배치 처리를 위한 collator 함수\n",
    "    data_collator=collate_fn,       \n",
    "    # 텍스트 필드 지정 (커스텀 collator 사용으로 빈 값)\n",
    "    dataset_text_field=\"\",          \n",
    "    # LoRA 파인튜닝 설정\n",
    "    peft_config=peft_config,        \n",
    "    # 텍스트 토크나이징을 위한 토크나이저\n",
    "    tokenizer=processor.tokenizer,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cccec5-66ca-4e93-9d95-cac83e8124f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='517' max='522' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [517/522 1:38:50 < 00:57, 0.09 it/s, Epoch 2.95/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.471400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.357800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.208100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.181700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작, 모델은 출력 디렉토리에 저장됨\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2965dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "adapter_path = \"./output_dir/checkpoint-522\"  # 학습된 어댑터 경로\n",
    "base_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"  # 기본 모델 ID\n",
    "merged_path = \"./merged\"  # 병합된 모델을 저장할 경로\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(base_model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# LoRA와 기본 모델을 병합하고 저장\n",
    "peft_model = PeftModel.from_pretrained(model, adapter_path)  # PEFT 모델 로드\n",
    "merged_model = peft_model.merge_and_unload()  # 모델 병합\n",
    "merged_model.save_pretrained(merged_path, safe_serialization=True, max_shard_size=\"2GB\")  # 병합된 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c5aff3-e0aa-4063-b2d2-97f82977bfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `test`\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 로그인 \n",
    "!huggingface-cli login --token 여러분들의KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f0baf2c-a28b-4546-b3ff-e0de41c78aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 커맨드로 모델 업로드\n",
    "# !huggingface-cli upload-large-folder --repo-type=model Qwen2-VL-7B-instruct-KoScienceQA ./merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
